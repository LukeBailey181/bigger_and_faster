Job starting ...
/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/
Running ptq @ 10ms
Namespace(arch_perfs_file='', candidate_file='/n/home00/lbailey/bigger_and_faster/cands/kd_ptq_10.cands', ckpt_path='/n/home00/lbailey/bigger_and_faster/conf_datasets/lat_predictor_quant.pt', feature_dim=4, feature_norm=[564, 5, 1024, 564], gen_size=10, head_num_space=[1, 12], hidden_dim=2000, hidden_layer_num=3, hidden_size_space=[144, 528], intermediate_size_space=[128, 1024], lat_norm=200, latency_constraint=10.0, layer_num_space=[1, 5], method='Candidate', model='KD', output_file='cands/archs.txt', qkv_size_space=[144, 528])
Size of candidates: 999
Initial cand gen done
Namespace(arch_perfs_file='', candidate_file='/n/home00/lbailey/bigger_and_faster/cands/kd_ptq_10.cands', ckpt_path='/n/home00/lbailey/bigger_and_faster/conf_datasets/lat_predictor_quant.pt', feature_dim=4, feature_norm=[564, 5, 1024, 564], gen_size=10, head_num_space=[1, 12], hidden_dim=2000, hidden_layer_num=3, hidden_size_space=[144, 528], intermediate_size_space=[128, 1024], lat_norm=200, latency_constraint=10.0, layer_num_space=[1, 5], method='Fast', model='KD', output_file='../cands/1st_generation_kd_ptq_10.cands', qkv_size_space=[144, 528])
Size of candidates: 999
Size of fast candidates: 98
Initial search done
05/28/2023 13:56:22 - INFO - __main__ -   device: cuda n_gpu: 1
05/28/2023 13:56:22 - INFO - __main__ -   task_lis: ['rte']
05/28/2023 13:56:22 - INFO - __main__ -   data_dir_lis: ['/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/']
05/28/2023 13:56:22 - INFO - transformer.tokenization -   loading vocabulary file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3/vocab.txt
05/28/2023 13:56:22 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:56:22 - INFO - __main__ -   subbert_configs: [{'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 480, 'sample_intermediate_sizes': [832, 832, 832], 'sample_qkv_sizes': [480, 480, 480]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 276, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [276, 276, 276, 276]}, {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 180, 'sample_intermediate_sizes': [320, 320, 320, 320, 320], 'sample_qkv_sizes': [180, 180, 180, 180, 180]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_qkv_sizes': [348, 348, 348, 348]}, {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [416, 416, 416, 416, 416], 'sample_qkv_sizes': [252, 252, 252, 252, 252]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [928, 928, 928], 'sample_qkv_sizes': [492, 492, 492]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [768, 768, 768], 'sample_qkv_sizes': [432, 432, 432]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [800, 800, 800], 'sample_qkv_sizes': [432, 432, 432]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [252, 252, 252, 252]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [832, 832, 832], 'sample_qkv_sizes': [468, 468, 468]}]
05/28/2023 13:56:22 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 480, 'sample_intermediate_sizes': [832, 832, 832], 'sample_qkv_sizes': [480, 480, 480]}
05/28/2023 13:56:22 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:56:22 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:56:22 - INFO - __main__ -   *** Example ***
05/28/2023 13:56:22 - INFO - __main__ -   guid: train-0
05/28/2023 13:56:22 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:56:22 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:22 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:22 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:22 - INFO - __main__ -   label: not_entailment
05/28/2023 13:56:22 - INFO - __main__ -   label_id: 1
05/28/2023 13:56:24 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:56:24 - INFO - __main__ -   *** Example ***
05/28/2023 13:56:24 - INFO - __main__ -   guid: dev-0
05/28/2023 13:56:24 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:56:24 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:24 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:24 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:24 - INFO - __main__ -   label: not_entailment
05/28/2023 13:56:24 - INFO - __main__ -   label_id: 1
05/28/2023 13:56:24 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:56:24 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:56:25 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:56:28 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:56:28 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:56:28 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:56:28 - INFO - __main__ -     Batch size = 32
05/28/2023 13:56:28 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A/n/home00/lbailey/bigger_and_faster/transformer/optimization.py:248: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  next_m.mul_(beta1).add_(1 - beta1, grad)

Iteration:   3%|2         | 2/78 [00:00<00:04, 15.78it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 19.52it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 20.98it/s][A05/28/2023 13:56:28 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:28 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:56:28 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:28 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.02it/s]
05/28/2023 13:56:28 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:28 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:56:28 - INFO - __main__ -    dev: eval_loss = 0.7068121963077121
05/28/2023 13:56:28 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:28 - INFO - __main__ -    dev: infer_time = 2.903222222222222
05/28/2023 13:56:28 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:28 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:56:28 - INFO - __main__ -     cls_loss = 0.7033637828297086
05/28/2023 13:56:28 - INFO - __main__ -     eval_loss = 0.7068121963077121
05/28/2023 13:56:28 - INFO - __main__ -     global_step = 9
05/28/2023 13:56:28 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:28 - INFO - __main__ -     infer_time = 2.903222222222222
05/28/2023 13:56:28 - INFO - __main__ -     loss = 0.7033637828297086
05/28/2023 13:56:28 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:02<00:16,  3.97it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:11,  5.73it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:07,  7.74it/s][A05/28/2023 13:56:30 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:30 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:56:30 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:30 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.76it/s]
05/28/2023 13:56:30 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:30 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:56:30 - INFO - __main__ -    dev: eval_loss = 0.7475692563586764
05/28/2023 13:56:30 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:30 - INFO - __main__ -    dev: infer_time = 2.890222222222222
05/28/2023 13:56:30 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:30 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:56:30 - INFO - __main__ -     cls_loss = 0.7127283435118826
05/28/2023 13:56:30 - INFO - __main__ -     eval_loss = 0.7475692563586764
05/28/2023 13:56:30 - INFO - __main__ -     global_step = 19
05/28/2023 13:56:30 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:30 - INFO - __main__ -     infer_time = 2.890222222222222
05/28/2023 13:56:30 - INFO - __main__ -     loss = 0.7127283435118826

Iteration:  24%|##4       | 19/78 [00:02<00:07,  8.42it/s][A
Iteration:  28%|##8       | 22/78 [00:02<00:05, 10.77it/s][A
Iteration:  32%|###2      | 25/78 [00:02<00:04, 13.01it/s][A
Iteration:  36%|###5      | 28/78 [00:02<00:03, 15.07it/s][A05/28/2023 13:56:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:31 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:56:31 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.85it/s]
05/28/2023 13:56:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:31 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:56:31 - INFO - __main__ -    dev: eval_loss = 0.6915416518847147
05/28/2023 13:56:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:31 - INFO - __main__ -    dev: infer_time = 2.8874444444444447
05/28/2023 13:56:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:31 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:56:31 - INFO - __main__ -     cls_loss = 0.7080030523497483
05/28/2023 13:56:31 - INFO - __main__ -     eval_loss = 0.6915416518847147
05/28/2023 13:56:31 - INFO - __main__ -     global_step = 29
05/28/2023 13:56:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:31 - INFO - __main__ -     infer_time = 2.8874444444444447
05/28/2023 13:56:31 - INFO - __main__ -     loss = 0.7080030523497483

Iteration:  40%|###9      | 31/78 [00:03<00:03, 14.66it/s][A
Iteration:  44%|####3     | 34/78 [00:03<00:02, 16.45it/s][A
Iteration:  47%|####7     | 37/78 [00:03<00:02, 18.03it/s][A05/28/2023 13:56:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:31 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:56:31 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.81it/s]
05/28/2023 13:56:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:31 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:56:31 - INFO - __main__ -    dev: eval_loss = 0.6956457628144158
05/28/2023 13:56:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:31 - INFO - __main__ -    dev: infer_time = 2.882444444444445
05/28/2023 13:56:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:31 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:56:31 - INFO - __main__ -     cls_loss = 0.7045987569368802
05/28/2023 13:56:31 - INFO - __main__ -     eval_loss = 0.6956457628144158
05/28/2023 13:56:31 - INFO - __main__ -     global_step = 39
05/28/2023 13:56:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:31 - INFO - __main__ -     infer_time = 2.882444444444445
05/28/2023 13:56:31 - INFO - __main__ -     loss = 0.7045987569368802

Iteration:  51%|#####1    | 40/78 [00:03<00:02, 16.47it/s][A
Iteration:  55%|#####5    | 43/78 [00:03<00:01, 17.97it/s][A
Iteration:  59%|#####8    | 46/78 [00:03<00:01, 19.17it/s][A05/28/2023 13:56:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:32 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:56:32 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.68it/s]
05/28/2023 13:56:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:32 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:56:32 - INFO - __main__ -    dev: eval_loss = 0.6941930055618286
05/28/2023 13:56:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:32 - INFO - __main__ -    dev: infer_time = 2.8844444444444446
05/28/2023 13:56:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:32 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:56:32 - INFO - __main__ -     cls_loss = 0.702687537183567
05/28/2023 13:56:32 - INFO - __main__ -     eval_loss = 0.6941930055618286
05/28/2023 13:56:32 - INFO - __main__ -     global_step = 49
05/28/2023 13:56:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:32 - INFO - __main__ -     infer_time = 2.8844444444444446
05/28/2023 13:56:32 - INFO - __main__ -     loss = 0.702687537183567

Iteration:  63%|######2   | 49/78 [00:04<00:01, 17.01it/s][A
Iteration:  67%|######6   | 52/78 [00:04<00:01, 18.27it/s][A
Iteration:  71%|#######   | 55/78 [00:04<00:01, 19.42it/s][A
Iteration:  74%|#######4  | 58/78 [00:04<00:00, 20.35it/s][A05/28/2023 13:56:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:32 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:56:32 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.98it/s]
05/28/2023 13:56:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:32 - INFO - __main__ -    dev: acc = 0.5487364620938628
05/28/2023 13:56:32 - INFO - __main__ -    dev: eval_loss = 0.6926902598804898
05/28/2023 13:56:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:32 - INFO - __main__ -    dev: infer_time = 2.879666666666667
05/28/2023 13:56:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:32 - INFO - __main__ -     acc = 0.5487364620938628
05/28/2023 13:56:32 - INFO - __main__ -     cls_loss = 0.701190747446933
05/28/2023 13:56:32 - INFO - __main__ -     eval_loss = 0.6926902598804898
05/28/2023 13:56:32 - INFO - __main__ -     global_step = 59
05/28/2023 13:56:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:32 - INFO - __main__ -     infer_time = 2.879666666666667
05/28/2023 13:56:32 - INFO - __main__ -     loss = 0.701190747446933
05/28/2023 13:56:32 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  78%|#######8  | 61/78 [00:06<00:03,  5.07it/s][A
Iteration:  82%|########2 | 64/78 [00:06<00:02,  6.61it/s][A
Iteration:  86%|########5 | 67/78 [00:06<00:01,  8.40it/s][A05/28/2023 13:56:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:34 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:56:34 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.80it/s]
05/28/2023 13:56:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:34 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:56:34 - INFO - __main__ -    dev: eval_loss = 0.691586971282959
05/28/2023 13:56:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:34 - INFO - __main__ -    dev: infer_time = 2.8764444444444446
05/28/2023 13:56:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:34 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:56:34 - INFO - __main__ -     cls_loss = 0.6997340986694115
05/28/2023 13:56:34 - INFO - __main__ -     eval_loss = 0.691586971282959
05/28/2023 13:56:34 - INFO - __main__ -     global_step = 69
05/28/2023 13:56:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:34 - INFO - __main__ -     infer_time = 2.8764444444444446
05/28/2023 13:56:34 - INFO - __main__ -     loss = 0.6997340986694115

Iteration:  88%|########8 | 69/78 [00:06<00:01,  8.93it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 11.06it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 13.20it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.37it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.86s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.86s/it]
05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   w_emb: 14650560

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   p_emb: 245760

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   t_emb: 960

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   ln_emb: 960

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   query_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   key_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   value_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   self_numel: 692640

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   output_numel: 231840

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 400192

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 399840

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   attention_numel: 924480

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   intermediate_numel: 400192

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   output_numel: 400800

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   query_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   key_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   value_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   self_numel: 692640

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   output_numel: 231840

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 400192

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 399840

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   attention_numel: 924480

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   intermediate_numel: 400192

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   output_numel: 400800

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   query_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   key_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   value_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   self_numel: 692640

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   output_numel: 231840

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 400192

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 399840

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   attention_numel: 924480

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   intermediate_numel: 400192

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   output_numel: 400800

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   layer_numel: 5176416
05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   dense_numel: 230880
05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   emb_numel: 14898240

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   encoder_numel: 5176416

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   pooler_numel: 230880

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   all parameters: 20305536

05/28/2023 13:56:35 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:56:35 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 480, 'sample_intermediate_sizes': [832, 832, 832], 'sample_qkv_sizes': [480, 480, 480]}
parameter size = 20305536
best_acc = 0.5487364620938628
time_per_batch_infer = 2.886 ms
infer_cnt = 63
**************E*************

05/28/2023 13:56:35 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 276, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [276, 276, 276, 276]}
05/28/2023 13:56:35 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:56:35 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:56:35 - INFO - __main__ -   *** Example ***
05/28/2023 13:56:35 - INFO - __main__ -   guid: train-0
05/28/2023 13:56:35 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:56:35 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:35 - INFO - __main__ -   label: not_entailment
05/28/2023 13:56:35 - INFO - __main__ -   label_id: 1
05/28/2023 13:56:36 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:56:36 - INFO - __main__ -   *** Example ***
05/28/2023 13:56:36 - INFO - __main__ -   guid: dev-0
05/28/2023 13:56:36 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:56:36 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:36 - INFO - __main__ -   label: not_entailment
05/28/2023 13:56:36 - INFO - __main__ -   label_id: 1
05/28/2023 13:56:37 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:56:37 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:56:37 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:56:37 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:56:37 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:56:37 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:56:37 - INFO - __main__ -     Batch size = 32
05/28/2023 13:56:37 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.67it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.99it/s][A05/28/2023 13:56:38 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:38 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:56:38 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:38 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 159.29it/s]
05/28/2023 13:56:38 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:38 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:56:38 - INFO - __main__ -    dev: eval_loss = 0.691624191072252
05/28/2023 13:56:38 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:38 - INFO - __main__ -    dev: infer_time = 3.478222222222222
05/28/2023 13:56:38 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:38 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:56:38 - INFO - __main__ -     cls_loss = 0.692793952094184
05/28/2023 13:56:38 - INFO - __main__ -     eval_loss = 0.691624191072252
05/28/2023 13:56:38 - INFO - __main__ -     global_step = 9
05/28/2023 13:56:38 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:38 - INFO - __main__ -     infer_time = 3.478222222222222
05/28/2023 13:56:38 - INFO - __main__ -     loss = 0.692793952094184
05/28/2023 13:56:38 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.90it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.80it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.03it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.32it/s][A05/28/2023 13:56:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:39 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:56:39 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 158.46it/s]
05/28/2023 13:56:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:39 - INFO - __main__ -    dev: acc = 0.48375451263537905
05/28/2023 13:56:39 - INFO - __main__ -    dev: eval_loss = 0.6955968803829617
05/28/2023 13:56:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:39 - INFO - __main__ -    dev: infer_time = 3.4768888888888885
05/28/2023 13:56:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:39 - INFO - __main__ -     acc = 0.48375451263537905
05/28/2023 13:56:39 - INFO - __main__ -     cls_loss = 0.6957681775093079
05/28/2023 13:56:39 - INFO - __main__ -     eval_loss = 0.6955968803829617
05/28/2023 13:56:39 - INFO - __main__ -     global_step = 19
05/28/2023 13:56:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:39 - INFO - __main__ -     infer_time = 3.4768888888888885
05/28/2023 13:56:39 - INFO - __main__ -     loss = 0.6957681775093079

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.60it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.97it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.17it/s][A05/28/2023 13:56:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:40 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:56:40 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 159.32it/s]
05/28/2023 13:56:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:40 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:56:40 - INFO - __main__ -    dev: eval_loss = 0.693066742685106
05/28/2023 13:56:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:40 - INFO - __main__ -    dev: infer_time = 3.457555555555556
05/28/2023 13:56:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:40 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:56:40 - INFO - __main__ -     cls_loss = 0.6927118712458117
05/28/2023 13:56:40 - INFO - __main__ -     eval_loss = 0.693066742685106
05/28/2023 13:56:40 - INFO - __main__ -     global_step = 29
05/28/2023 13:56:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:40 - INFO - __main__ -     infer_time = 3.457555555555556
05/28/2023 13:56:40 - INFO - __main__ -     loss = 0.6927118712458117

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.04it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 17.97it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 19.59it/s][A05/28/2023 13:56:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:40 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:56:40 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 159.76it/s]
05/28/2023 13:56:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:40 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:56:40 - INFO - __main__ -    dev: eval_loss = 0.700580464469062
05/28/2023 13:56:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:40 - INFO - __main__ -    dev: infer_time = 3.4593333333333334
05/28/2023 13:56:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:40 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:56:40 - INFO - __main__ -     cls_loss = 0.694260563605871
05/28/2023 13:56:40 - INFO - __main__ -     eval_loss = 0.700580464469062
05/28/2023 13:56:40 - INFO - __main__ -     global_step = 39
05/28/2023 13:56:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:40 - INFO - __main__ -     infer_time = 3.4593333333333334
05/28/2023 13:56:40 - INFO - __main__ -     loss = 0.694260563605871

Iteration:  50%|#####     | 39/78 [00:03<00:02, 18.42it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 19.67it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 20.61it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 21.72it/s][A05/28/2023 13:56:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:41 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:56:41 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 159.76it/s]
05/28/2023 13:56:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:41 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 13:56:41 - INFO - __main__ -    dev: eval_loss = 0.6928190655178494
05/28/2023 13:56:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:41 - INFO - __main__ -    dev: infer_time = 3.4653333333333336
05/28/2023 13:56:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:41 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 13:56:41 - INFO - __main__ -     cls_loss = 0.695114084652492
05/28/2023 13:56:41 - INFO - __main__ -     eval_loss = 0.6928190655178494
05/28/2023 13:56:41 - INFO - __main__ -     global_step = 49
05/28/2023 13:56:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:41 - INFO - __main__ -     infer_time = 3.4653333333333336
05/28/2023 13:56:41 - INFO - __main__ -     loss = 0.695114084652492

Iteration:  65%|######5   | 51/78 [00:03<00:01, 19.42it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 20.78it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:00, 21.83it/s][A05/28/2023 13:56:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:41 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:56:41 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 159.72it/s]
05/28/2023 13:56:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:41 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:56:41 - INFO - __main__ -    dev: eval_loss = 0.7052637272410922
05/28/2023 13:56:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:41 - INFO - __main__ -    dev: infer_time = 3.495444444444445
05/28/2023 13:56:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:41 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:56:41 - INFO - __main__ -     cls_loss = 0.6945771251694631
05/28/2023 13:56:41 - INFO - __main__ -     eval_loss = 0.7052637272410922
05/28/2023 13:56:41 - INFO - __main__ -     global_step = 59
05/28/2023 13:56:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:41 - INFO - __main__ -     infer_time = 3.495444444444445
05/28/2023 13:56:41 - INFO - __main__ -     loss = 0.6945771251694631

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 19.45it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 20.83it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 21.95it/s][A05/28/2023 13:56:42 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:42 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:56:42 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:42 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 159.86it/s]
05/28/2023 13:56:42 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:42 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:56:42 - INFO - __main__ -    dev: eval_loss = 0.700797332657708
05/28/2023 13:56:42 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:42 - INFO - __main__ -    dev: infer_time = 3.469555555555556
05/28/2023 13:56:42 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:42 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:56:42 - INFO - __main__ -     cls_loss = 0.6953931200331536
05/28/2023 13:56:42 - INFO - __main__ -     eval_loss = 0.700797332657708
05/28/2023 13:56:42 - INFO - __main__ -     global_step = 69
05/28/2023 13:56:42 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:42 - INFO - __main__ -     infer_time = 3.469555555555556
05/28/2023 13:56:42 - INFO - __main__ -     loss = 0.6953931200331536

Iteration:  88%|########8 | 69/78 [00:04<00:00, 19.86it/s][A
Iteration:  92%|#########2| 72/78 [00:04<00:00, 20.78it/s][A
Iteration:  96%|#########6| 75/78 [00:04<00:00, 21.87it/s][AIteration: 100%|##########| 78/78 [00:04<00:00, 15.66it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.98s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.98s/it]
05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   w_emb: 8424072

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   p_emb: 141312

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   t_emb: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ln_emb: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 124096

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 123924

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 124096

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   output_numel: 124476

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 124096

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 123924

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 124096

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   output_numel: 124476

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 124096

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 123924

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 124096

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   output_numel: 124476

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 124096

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 123924

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 124096

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   output_numel: 124476

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   layer_numel: 2219728
05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   dense_numel: 76452
05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   emb_numel: 8566488

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   encoder_numel: 2219728

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   pooler_numel: 76452

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   all parameters: 10862668

05/28/2023 13:56:42 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:56:42 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 276, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [276, 276, 276, 276]}
parameter size = 10862668
best_acc = 0.5270758122743683
time_per_batch_infer = 3.472 ms
infer_cnt = 63
**************E*************

05/28/2023 13:56:42 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 180, 'sample_intermediate_sizes': [320, 320, 320, 320, 320], 'sample_qkv_sizes': [180, 180, 180, 180, 180]}
05/28/2023 13:56:42 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:56:42 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:56:42 - INFO - __main__ -   *** Example ***
05/28/2023 13:56:42 - INFO - __main__ -   guid: train-0
05/28/2023 13:56:42 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:56:42 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:42 - INFO - __main__ -   label: not_entailment
05/28/2023 13:56:42 - INFO - __main__ -   label_id: 1
05/28/2023 13:56:44 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:56:44 - INFO - __main__ -   *** Example ***
05/28/2023 13:56:44 - INFO - __main__ -   guid: dev-0
05/28/2023 13:56:44 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:56:44 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:44 - INFO - __main__ -   label: not_entailment
05/28/2023 13:56:44 - INFO - __main__ -   label_id: 1
05/28/2023 13:56:44 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:56:44 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:56:45 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:56:45 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:56:45 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:56:45 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:56:45 - INFO - __main__ -     Batch size = 32
05/28/2023 13:56:45 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.86it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.99it/s][A05/28/2023 13:56:45 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:45 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:56:45 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:45 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.29it/s]
05/28/2023 13:56:45 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:45 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:56:45 - INFO - __main__ -    dev: eval_loss = 0.6977797746658325
05/28/2023 13:56:45 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:45 - INFO - __main__ -    dev: infer_time = 4.277444444444445
05/28/2023 13:56:45 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:45 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:56:45 - INFO - __main__ -     cls_loss = 0.6964630815717909
05/28/2023 13:56:45 - INFO - __main__ -     eval_loss = 0.6977797746658325
05/28/2023 13:56:45 - INFO - __main__ -     global_step = 9
05/28/2023 13:56:45 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:45 - INFO - __main__ -     infer_time = 4.277444444444445
05/28/2023 13:56:45 - INFO - __main__ -     loss = 0.6964630815717909
05/28/2023 13:56:45 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.01it/s][A
Iteration:  14%|#4        | 11/78 [00:01<00:12,  5.28it/s][A
Iteration:  18%|#7        | 14/78 [00:01<00:08,  7.60it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:06, 10.06it/s][A05/28/2023 13:56:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:47 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:56:47 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.90it/s]
05/28/2023 13:56:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:47 - INFO - __main__ -    dev: acc = 0.5090252707581228
05/28/2023 13:56:47 - INFO - __main__ -    dev: eval_loss = 0.6930995053715177
05/28/2023 13:56:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:47 - INFO - __main__ -    dev: infer_time = 4.248777777777778
05/28/2023 13:56:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:47 - INFO - __main__ -     acc = 0.5090252707581228
05/28/2023 13:56:47 - INFO - __main__ -     cls_loss = 0.6953658053749486
05/28/2023 13:56:47 - INFO - __main__ -     eval_loss = 0.6930995053715177
05/28/2023 13:56:47 - INFO - __main__ -     global_step = 19
05/28/2023 13:56:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:47 - INFO - __main__ -     infer_time = 4.248777777777778
05/28/2023 13:56:47 - INFO - __main__ -     loss = 0.6953658053749486
05/28/2023 13:56:47 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:13,  4.24it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:09,  5.80it/s][A
Iteration:  33%|###3      | 26/78 [00:03<00:06,  7.63it/s][A05/28/2023 13:56:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:49 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:56:49 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.75it/s]
05/28/2023 13:56:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:49 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 13:56:49 - INFO - __main__ -    dev: eval_loss = 0.6930134495099386
05/28/2023 13:56:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:49 - INFO - __main__ -    dev: infer_time = 4.259777777777779
05/28/2023 13:56:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:49 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 13:56:49 - INFO - __main__ -     cls_loss = 0.6943741029706495
05/28/2023 13:56:49 - INFO - __main__ -     eval_loss = 0.6930134495099386
05/28/2023 13:56:49 - INFO - __main__ -     global_step = 29
05/28/2023 13:56:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:49 - INFO - __main__ -     infer_time = 4.259777777777779
05/28/2023 13:56:49 - INFO - __main__ -     loss = 0.6943741029706495

Iteration:  37%|###7      | 29/78 [00:04<00:05,  9.10it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 11.12it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 13.30it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 15.36it/s][A05/28/2023 13:56:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:49 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:56:49 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.74it/s]
05/28/2023 13:56:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:49 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 13:56:49 - INFO - __main__ -    dev: eval_loss = 0.6917311085595025
05/28/2023 13:56:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:49 - INFO - __main__ -    dev: infer_time = 4.278
05/28/2023 13:56:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:49 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 13:56:49 - INFO - __main__ -     cls_loss = 0.6939650987967466
05/28/2023 13:56:49 - INFO - __main__ -     eval_loss = 0.6917311085595025
05/28/2023 13:56:49 - INFO - __main__ -     global_step = 39
05/28/2023 13:56:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:49 - INFO - __main__ -     infer_time = 4.278
05/28/2023 13:56:49 - INFO - __main__ -     loss = 0.6939650987967466
05/28/2023 13:56:49 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  53%|#####2    | 41/78 [00:05<00:07,  5.05it/s][A
Iteration:  56%|#####6    | 44/78 [00:06<00:05,  6.62it/s][A
Iteration:  60%|######    | 47/78 [00:06<00:03,  8.45it/s][A05/28/2023 13:56:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:51 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:56:51 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 168.89it/s]
05/28/2023 13:56:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:51 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 13:56:51 - INFO - __main__ -    dev: eval_loss = 0.6915638645490011
05/28/2023 13:56:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:51 - INFO - __main__ -    dev: infer_time = 4.260777777777778
05/28/2023 13:56:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:51 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 13:56:51 - INFO - __main__ -     cls_loss = 0.6938385343065068
05/28/2023 13:56:51 - INFO - __main__ -     eval_loss = 0.6915638645490011
05/28/2023 13:56:51 - INFO - __main__ -     global_step = 49
05/28/2023 13:56:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:51 - INFO - __main__ -     infer_time = 4.260777777777778
05/28/2023 13:56:51 - INFO - __main__ -     loss = 0.6938385343065068

Iteration:  63%|######2   | 49/78 [00:06<00:03,  9.21it/s][A
Iteration:  67%|######6   | 52/78 [00:06<00:02, 11.34it/s][A
Iteration:  71%|#######   | 55/78 [00:06<00:01, 13.59it/s][A
Iteration:  74%|#######4  | 58/78 [00:06<00:01, 15.64it/s][A05/28/2023 13:56:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:51 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:56:51 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.92it/s]
05/28/2023 13:56:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:51 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 13:56:51 - INFO - __main__ -    dev: eval_loss = 0.6920416090223525
05/28/2023 13:56:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:51 - INFO - __main__ -    dev: infer_time = 4.259444444444444
05/28/2023 13:56:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:51 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 13:56:51 - INFO - __main__ -     cls_loss = 0.6939051545272439
05/28/2023 13:56:51 - INFO - __main__ -     eval_loss = 0.6920416090223525
05/28/2023 13:56:51 - INFO - __main__ -     global_step = 59
05/28/2023 13:56:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:51 - INFO - __main__ -     infer_time = 4.259444444444444
05/28/2023 13:56:51 - INFO - __main__ -     loss = 0.6939051545272439
05/28/2023 13:56:51 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  78%|#######8  | 61/78 [00:08<00:03,  5.03it/s][A
Iteration:  82%|########2 | 64/78 [00:08<00:02,  6.61it/s][A
Iteration:  86%|########5 | 67/78 [00:08<00:01,  8.47it/s][A05/28/2023 13:56:53 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:53 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:56:53 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:53 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.37it/s]
05/28/2023 13:56:53 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:53 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 13:56:53 - INFO - __main__ -    dev: eval_loss = 0.6926853590541415
05/28/2023 13:56:53 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:53 - INFO - __main__ -    dev: infer_time = 4.277777777777778
05/28/2023 13:56:53 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:53 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 13:56:53 - INFO - __main__ -     cls_loss = 0.6936826334483381
05/28/2023 13:56:53 - INFO - __main__ -     eval_loss = 0.6926853590541415
05/28/2023 13:56:53 - INFO - __main__ -     global_step = 69
05/28/2023 13:56:53 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:53 - INFO - __main__ -     infer_time = 4.277777777777778
05/28/2023 13:56:53 - INFO - __main__ -     loss = 0.6936826334483381

Iteration:  88%|########8 | 69/78 [00:08<00:00,  9.28it/s][A
Iteration:  92%|#########2| 72/78 [00:08<00:00, 11.48it/s][A
Iteration:  96%|#########6| 75/78 [00:08<00:00, 13.59it/s][AIteration: 100%|##########| 78/78 [00:08<00:00,  8.67it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.00s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.00s/it]
05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   w_emb: 5493960

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   p_emb: 92160

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   t_emb: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_emb: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   query_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   key_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   value_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   self_numel: 97740

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 32940

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57780

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   attention_numel: 130680

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   intermediate_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 58140

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   query_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   key_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   value_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   self_numel: 97740

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 32940

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57780

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   attention_numel: 130680

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   intermediate_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 58140

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   query_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   key_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   value_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   self_numel: 97740

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 32940

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57780

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   attention_numel: 130680

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   intermediate_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 58140

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   query_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   key_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   value_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   self_numel: 97740

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 32940

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57780

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   attention_numel: 130680

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   intermediate_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 58140

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   query_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   key_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   value_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   self_numel: 97740

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 32940

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 57780

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ln_numel: 360

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   attention_numel: 130680

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   intermediate_numel: 57920

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   output_numel: 58140

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   layer_numel: 1233700
05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   dense_numel: 32580
05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   emb_numel: 5586840

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   encoder_numel: 1233700

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   pooler_numel: 32580

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   all parameters: 6853120

05/28/2023 13:56:54 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:56:54 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 180, 'sample_intermediate_sizes': [320, 320, 320, 320, 320], 'sample_qkv_sizes': [180, 180, 180, 180, 180]}
parameter size = 6853120
best_acc = 0.5379061371841155
time_per_batch_infer = 4.266 ms
infer_cnt = 63
**************E*************

05/28/2023 13:56:54 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_qkv_sizes': [348, 348, 348, 348]}
05/28/2023 13:56:54 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:56:54 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:56:54 - INFO - __main__ -   *** Example ***
05/28/2023 13:56:54 - INFO - __main__ -   guid: train-0
05/28/2023 13:56:54 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:56:54 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:54 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:54 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:54 - INFO - __main__ -   label: not_entailment
05/28/2023 13:56:54 - INFO - __main__ -   label_id: 1
05/28/2023 13:56:55 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:56:55 - INFO - __main__ -   *** Example ***
05/28/2023 13:56:55 - INFO - __main__ -   guid: dev-0
05/28/2023 13:56:55 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:56:55 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:55 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:55 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:56:55 - INFO - __main__ -   label: not_entailment
05/28/2023 13:56:55 - INFO - __main__ -   label_id: 1
05/28/2023 13:56:56 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:56:56 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:56:56 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:56:56 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:56:56 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:56:56 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:56:56 - INFO - __main__ -     Batch size = 32
05/28/2023 13:56:56 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 20.87it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.34it/s][A05/28/2023 13:56:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:57 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:56:57 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.39it/s]
05/28/2023 13:56:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:57 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:56:57 - INFO - __main__ -    dev: eval_loss = 0.7075179682837592
05/28/2023 13:56:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:57 - INFO - __main__ -    dev: infer_time = 3.6181111111111113
05/28/2023 13:56:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:57 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:56:57 - INFO - __main__ -     cls_loss = 0.6917528046502007
05/28/2023 13:56:57 - INFO - __main__ -     eval_loss = 0.7075179682837592
05/28/2023 13:56:57 - INFO - __main__ -     global_step = 9
05/28/2023 13:56:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:57 - INFO - __main__ -     infer_time = 3.6181111111111113
05/28/2023 13:56:57 - INFO - __main__ -     loss = 0.6917528046502007
05/28/2023 13:56:57 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.83it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.64it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.69it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.77it/s][A05/28/2023 13:56:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:56:58 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:56:58 - INFO - __main__ -     Num examples = 277
05/28/2023 13:56:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.88it/s]
05/28/2023 13:56:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:56:58 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 13:56:58 - INFO - __main__ -    dev: eval_loss = 0.7016301949818929
05/28/2023 13:56:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:56:58 - INFO - __main__ -    dev: infer_time = 3.637888888888889
05/28/2023 13:56:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:56:59 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 13:56:59 - INFO - __main__ -     cls_loss = 0.6921763012283727
05/28/2023 13:56:59 - INFO - __main__ -     eval_loss = 0.7016301949818929
05/28/2023 13:56:59 - INFO - __main__ -     global_step = 19
05/28/2023 13:56:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:56:59 - INFO - __main__ -     infer_time = 3.637888888888889
05/28/2023 13:56:59 - INFO - __main__ -     loss = 0.6921763012283727
05/28/2023 13:56:59 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:14,  3.92it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.45it/s][A
Iteration:  33%|###3      | 26/78 [00:03<00:07,  7.23it/s][A05/28/2023 13:57:00 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:00 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:57:00 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:00 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.31it/s]
05/28/2023 13:57:00 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:00 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:00 - INFO - __main__ -    dev: eval_loss = 0.6907738116052415
05/28/2023 13:57:00 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:00 - INFO - __main__ -    dev: infer_time = 3.645111111111111
05/28/2023 13:57:00 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:00 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:00 - INFO - __main__ -     cls_loss = 0.6923674509443086
05/28/2023 13:57:00 - INFO - __main__ -     eval_loss = 0.6907738116052415
05/28/2023 13:57:00 - INFO - __main__ -     global_step = 29
05/28/2023 13:57:00 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:00 - INFO - __main__ -     infer_time = 3.645111111111111
05/28/2023 13:57:00 - INFO - __main__ -     loss = 0.6923674509443086
05/28/2023 13:57:00 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  37%|###7      | 29/78 [00:05<00:13,  3.75it/s][A
Iteration:  40%|###9      | 31/78 [00:05<00:10,  4.62it/s][A
Iteration:  44%|####3     | 34/78 [00:05<00:07,  6.25it/s][A
Iteration:  47%|####7     | 37/78 [00:05<00:05,  8.10it/s][A05/28/2023 13:57:02 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:02 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:57:02 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:02 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.11it/s]
05/28/2023 13:57:02 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:02 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 13:57:02 - INFO - __main__ -    dev: eval_loss = 0.6896870401170518
05/28/2023 13:57:02 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:02 - INFO - __main__ -    dev: infer_time = 3.7108888888888893
05/28/2023 13:57:02 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:02 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 13:57:02 - INFO - __main__ -     cls_loss = 0.6953625235802088
05/28/2023 13:57:02 - INFO - __main__ -     eval_loss = 0.6896870401170518
05/28/2023 13:57:02 - INFO - __main__ -     global_step = 39
05/28/2023 13:57:02 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:02 - INFO - __main__ -     infer_time = 3.7108888888888893
05/28/2023 13:57:02 - INFO - __main__ -     loss = 0.6953625235802088
05/28/2023 13:57:02 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:07<00:10,  3.77it/s][A
Iteration:  53%|#####2    | 41/78 [00:07<00:07,  4.73it/s][A
Iteration:  56%|#####6    | 44/78 [00:07<00:05,  6.51it/s][A
Iteration:  60%|######    | 47/78 [00:07<00:03,  8.50it/s][A05/28/2023 13:57:04 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:04 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:57:04 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:04 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.91it/s]
05/28/2023 13:57:04 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:04 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 13:57:04 - INFO - __main__ -    dev: eval_loss = 0.694822092851003
05/28/2023 13:57:04 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:04 - INFO - __main__ -    dev: infer_time = 3.6639999999999997
05/28/2023 13:57:04 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:04 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 13:57:04 - INFO - __main__ -     cls_loss = 0.6958075883437176
05/28/2023 13:57:04 - INFO - __main__ -     eval_loss = 0.694822092851003
05/28/2023 13:57:04 - INFO - __main__ -     global_step = 49
05/28/2023 13:57:04 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:04 - INFO - __main__ -     infer_time = 3.6639999999999997
05/28/2023 13:57:04 - INFO - __main__ -     loss = 0.6958075883437176

Iteration:  63%|######2   | 49/78 [00:07<00:03,  9.11it/s][A
Iteration:  67%|######6   | 52/78 [00:08<00:02, 11.25it/s][A
Iteration:  71%|#######   | 55/78 [00:08<00:01, 13.37it/s][A
Iteration:  74%|#######4  | 58/78 [00:08<00:01, 15.23it/s][A05/28/2023 13:57:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:05 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:57:05 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.21it/s]
05/28/2023 13:57:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:05 - INFO - __main__ -    dev: acc = 0.5451263537906137
05/28/2023 13:57:05 - INFO - __main__ -    dev: eval_loss = 0.6905317107836405
05/28/2023 13:57:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:05 - INFO - __main__ -    dev: infer_time = 3.6582222222222223
05/28/2023 13:57:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:05 - INFO - __main__ -     acc = 0.5451263537906137
05/28/2023 13:57:05 - INFO - __main__ -     cls_loss = 0.6953154994269549
05/28/2023 13:57:05 - INFO - __main__ -     eval_loss = 0.6905317107836405
05/28/2023 13:57:05 - INFO - __main__ -     global_step = 59
05/28/2023 13:57:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:05 - INFO - __main__ -     infer_time = 3.6582222222222223
05/28/2023 13:57:05 - INFO - __main__ -     loss = 0.6953154994269549
05/28/2023 13:57:05 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  77%|#######6  | 60/78 [00:10<00:04,  4.18it/s][A
Iteration:  81%|########  | 63/78 [00:10<00:02,  5.70it/s][A
Iteration:  85%|########4 | 66/78 [00:10<00:01,  7.48it/s][A05/28/2023 13:57:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:07 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:57:07 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.21it/s]
05/28/2023 13:57:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:07 - INFO - __main__ -    dev: eval_loss = 0.6903254389762878
05/28/2023 13:57:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:07 - INFO - __main__ -    dev: infer_time = 3.672111111111111
05/28/2023 13:57:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:07 - INFO - __main__ -     cls_loss = 0.6947327714035476
05/28/2023 13:57:07 - INFO - __main__ -     eval_loss = 0.6903254389762878
05/28/2023 13:57:07 - INFO - __main__ -     global_step = 69
05/28/2023 13:57:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:07 - INFO - __main__ -     infer_time = 3.672111111111111
05/28/2023 13:57:07 - INFO - __main__ -     loss = 0.6947327714035476

Iteration:  88%|########8 | 69/78 [00:10<00:01,  8.73it/s][A
Iteration:  91%|#########1| 71/78 [00:10<00:00,  9.96it/s][A
Iteration:  95%|#########4| 74/78 [00:10<00:00, 12.13it/s][A
Iteration:  99%|#########8| 77/78 [00:10<00:00, 14.17it/s][AIteration: 100%|##########| 78/78 [00:10<00:00,  7.14it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.92s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.92s/it]
05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   w_emb: 10621656

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   p_emb: 178176

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   t_emb: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ln_emb: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   layer_numel: 3645296
05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   dense_numel: 121452
05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   emb_numel: 10801224

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   encoder_numel: 3645296

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   pooler_numel: 121452

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   all parameters: 14567972

05/28/2023 13:57:07 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:07 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_qkv_sizes': [348, 348, 348, 348]}
parameter size = 14567972
best_acc = 0.5451263537906137
time_per_batch_infer = 3.658 ms
infer_cnt = 63
**************E*************

05/28/2023 13:57:07 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [416, 416, 416, 416, 416], 'sample_qkv_sizes': [252, 252, 252, 252, 252]}
05/28/2023 13:57:07 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:57:07 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:57:07 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:07 - INFO - __main__ -   guid: train-0
05/28/2023 13:57:07 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:57:07 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:07 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:07 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:07 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:07 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:09 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:57:09 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:09 - INFO - __main__ -   guid: dev-0
05/28/2023 13:57:09 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:57:09 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:09 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:09 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:09 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:09 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:57:09 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:57:10 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:57:10 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:57:10 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:57:10 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:57:10 - INFO - __main__ -     Batch size = 32
05/28/2023 13:57:10 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   3%|2         | 2/78 [00:00<00:03, 19.38it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 20.68it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 21.01it/s][A05/28/2023 13:57:10 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:10 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:57:10 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:10 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 137.23it/s]
05/28/2023 13:57:10 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:10 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:10 - INFO - __main__ -    dev: eval_loss = 0.6914454764790006
05/28/2023 13:57:10 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:10 - INFO - __main__ -    dev: infer_time = 4.268777777777779
05/28/2023 13:57:10 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:10 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:10 - INFO - __main__ -     cls_loss = 0.6944290863143073
05/28/2023 13:57:10 - INFO - __main__ -     eval_loss = 0.6914454764790006
05/28/2023 13:57:10 - INFO - __main__ -     global_step = 9
05/28/2023 13:57:10 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:10 - INFO - __main__ -     infer_time = 4.268777777777779
05/28/2023 13:57:10 - INFO - __main__ -     loss = 0.6944290863143073
05/28/2023 13:57:10 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:02<00:17,  3.90it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:11,  5.60it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:08,  7.53it/s][A05/28/2023 13:57:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:12 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:57:12 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.61it/s]
05/28/2023 13:57:12 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:12 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:12 - INFO - __main__ -    dev: eval_loss = 0.6937137775950961
05/28/2023 13:57:12 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:12 - INFO - __main__ -    dev: infer_time = 4.254666666666667
05/28/2023 13:57:12 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:12 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:12 - INFO - __main__ -     cls_loss = 0.6950996643618533
05/28/2023 13:57:12 - INFO - __main__ -     eval_loss = 0.6937137775950961
05/28/2023 13:57:12 - INFO - __main__ -     global_step = 19
05/28/2023 13:57:12 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:12 - INFO - __main__ -     infer_time = 4.254666666666667
05/28/2023 13:57:12 - INFO - __main__ -     loss = 0.6950996643618533

Iteration:  24%|##4       | 19/78 [00:02<00:07,  8.29it/s][A
Iteration:  27%|##6       | 21/78 [00:02<00:05,  9.71it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 12.08it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 14.16it/s][A05/28/2023 13:57:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:13 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:57:13 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.85it/s]
05/28/2023 13:57:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:13 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:13 - INFO - __main__ -    dev: eval_loss = 0.7029165559344821
05/28/2023 13:57:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:13 - INFO - __main__ -    dev: infer_time = 4.270555555555556
05/28/2023 13:57:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:13 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:13 - INFO - __main__ -     cls_loss = 0.6960371588838512
05/28/2023 13:57:13 - INFO - __main__ -     eval_loss = 0.7029165559344821
05/28/2023 13:57:13 - INFO - __main__ -     global_step = 29
05/28/2023 13:57:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:13 - INFO - __main__ -     infer_time = 4.270555555555556
05/28/2023 13:57:13 - INFO - __main__ -     loss = 0.6960371588838512

Iteration:  37%|###7      | 29/78 [00:03<00:03, 13.46it/s][A
Iteration:  41%|####1     | 32/78 [00:03<00:03, 15.31it/s][A
Iteration:  45%|####4     | 35/78 [00:03<00:02, 16.92it/s][A
Iteration:  49%|####8     | 38/78 [00:03<00:02, 18.18it/s][A05/28/2023 13:57:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:13 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:57:13 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.89it/s]
05/28/2023 13:57:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:13 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:13 - INFO - __main__ -    dev: eval_loss = 0.6942685511377122
05/28/2023 13:57:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:13 - INFO - __main__ -    dev: infer_time = 4.254666666666667
05/28/2023 13:57:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:13 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:13 - INFO - __main__ -     cls_loss = 0.6970931566678561
05/28/2023 13:57:13 - INFO - __main__ -     eval_loss = 0.6942685511377122
05/28/2023 13:57:13 - INFO - __main__ -     global_step = 39
05/28/2023 13:57:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:13 - INFO - __main__ -     infer_time = 4.254666666666667
05/28/2023 13:57:13 - INFO - __main__ -     loss = 0.6970931566678561

Iteration:  51%|#####1    | 40/78 [00:03<00:02, 16.22it/s][A
Iteration:  55%|#####5    | 43/78 [00:03<00:01, 17.64it/s][A
Iteration:  59%|#####8    | 46/78 [00:03<00:01, 18.72it/s][A05/28/2023 13:57:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:14 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:57:14 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.62it/s]
05/28/2023 13:57:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:14 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:14 - INFO - __main__ -    dev: eval_loss = 0.6912408007515801
05/28/2023 13:57:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:14 - INFO - __main__ -    dev: infer_time = 4.2698888888888895
05/28/2023 13:57:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:14 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:14 - INFO - __main__ -     cls_loss = 0.6964082754388148
05/28/2023 13:57:14 - INFO - __main__ -     eval_loss = 0.6912408007515801
05/28/2023 13:57:14 - INFO - __main__ -     global_step = 49
05/28/2023 13:57:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:14 - INFO - __main__ -     infer_time = 4.2698888888888895
05/28/2023 13:57:14 - INFO - __main__ -     loss = 0.6964082754388148

Iteration:  63%|######2   | 49/78 [00:04<00:01, 16.94it/s][A
Iteration:  65%|######5   | 51/78 [00:04<00:01, 17.53it/s][A
Iteration:  69%|######9   | 54/78 [00:04<00:01, 18.63it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:01, 19.41it/s][A05/28/2023 13:57:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:14 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:57:14 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.70it/s]
05/28/2023 13:57:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:14 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:14 - INFO - __main__ -    dev: eval_loss = 0.6912886367903815
05/28/2023 13:57:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:14 - INFO - __main__ -    dev: infer_time = 4.264888888888889
05/28/2023 13:57:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:14 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:14 - INFO - __main__ -     cls_loss = 0.6960521148423017
05/28/2023 13:57:14 - INFO - __main__ -     eval_loss = 0.6912886367903815
05/28/2023 13:57:14 - INFO - __main__ -     global_step = 59
05/28/2023 13:57:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:14 - INFO - __main__ -     infer_time = 4.264888888888889
05/28/2023 13:57:14 - INFO - __main__ -     loss = 0.6960521148423017

Iteration:  76%|#######5  | 59/78 [00:04<00:01, 16.97it/s][A
Iteration:  79%|#######9  | 62/78 [00:04<00:00, 18.14it/s][A
Iteration:  83%|########3 | 65/78 [00:04<00:00, 19.05it/s][A
Iteration:  87%|########7 | 68/78 [00:05<00:00, 19.72it/s][A05/28/2023 13:57:15 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:15 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:57:15 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:15 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.30it/s]
05/28/2023 13:57:15 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:15 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:15 - INFO - __main__ -    dev: eval_loss = 0.6915627717971802
05/28/2023 13:57:15 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:15 - INFO - __main__ -    dev: infer_time = 4.259555555555555
05/28/2023 13:57:15 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:15 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:15 - INFO - __main__ -     cls_loss = 0.6958411007687666
05/28/2023 13:57:15 - INFO - __main__ -     eval_loss = 0.6915627717971802
05/28/2023 13:57:15 - INFO - __main__ -     global_step = 69
05/28/2023 13:57:15 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:15 - INFO - __main__ -     infer_time = 4.259555555555555
05/28/2023 13:57:15 - INFO - __main__ -     loss = 0.6958411007687666

Iteration:  90%|########9 | 70/78 [00:05<00:00, 16.67it/s][A
Iteration:  94%|#########3| 73/78 [00:05<00:00, 17.97it/s][A
Iteration:  97%|#########7| 76/78 [00:05<00:00, 18.94it/s][AIteration: 100%|##########| 78/78 [00:05<00:00, 13.90it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.61s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.61s/it]
05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   w_emb: 7691544

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   p_emb: 129024

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   t_emb: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_emb: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105084

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 105588

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105084

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 105588

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105084

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 105588

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105084

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 105588

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 105084

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 105248

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   output_numel: 105588

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   layer_numel: 2331820
05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   dense_numel: 63756
05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   emb_numel: 7821576

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   encoder_numel: 2331820

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   pooler_numel: 63756

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   all parameters: 10217152

05/28/2023 13:57:15 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:15 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [416, 416, 416, 416, 416], 'sample_qkv_sizes': [252, 252, 252, 252, 252]}
parameter size = 10217152
best_acc = 0.5270758122743683
time_per_batch_infer = 4.263 ms
infer_cnt = 63
**************E*************

05/28/2023 13:57:15 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [928, 928, 928], 'sample_qkv_sizes': [492, 492, 492]}
05/28/2023 13:57:15 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:57:15 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:57:15 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:15 - INFO - __main__ -   guid: train-0
05/28/2023 13:57:15 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:57:15 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:15 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:15 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:15 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:15 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:17 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:57:17 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:17 - INFO - __main__ -   guid: dev-0
05/28/2023 13:57:17 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:57:17 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:17 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:17 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:17 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:17 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:57:17 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:57:18 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:57:18 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:57:18 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:57:18 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:57:18 - INFO - __main__ -     Batch size = 32
05/28/2023 13:57:18 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.11it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.51it/s][A05/28/2023 13:57:18 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:18 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:57:18 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:18 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 108.73it/s]
05/28/2023 13:57:18 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:18 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 13:57:18 - INFO - __main__ -    dev: eval_loss = 0.6907695068253411
05/28/2023 13:57:18 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:18 - INFO - __main__ -    dev: infer_time = 2.8817777777777773
05/28/2023 13:57:18 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:18 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 13:57:18 - INFO - __main__ -     cls_loss = 0.6927304532792833
05/28/2023 13:57:18 - INFO - __main__ -     eval_loss = 0.6907695068253411
05/28/2023 13:57:18 - INFO - __main__ -     global_step = 9
05/28/2023 13:57:18 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:18 - INFO - __main__ -     infer_time = 2.8817777777777773
05/28/2023 13:57:18 - INFO - __main__ -     loss = 0.6927304532792833
05/28/2023 13:57:18 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.88it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.72it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.79it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.99it/s][A05/28/2023 13:57:20 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:20 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:57:20 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:20 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 108.67it/s]
05/28/2023 13:57:20 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:20 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:20 - INFO - __main__ -    dev: eval_loss = 0.6911408834987216
05/28/2023 13:57:20 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:20 - INFO - __main__ -    dev: infer_time = 2.9165555555555556
05/28/2023 13:57:20 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:20 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:20 - INFO - __main__ -     cls_loss = 0.6966585140479239
05/28/2023 13:57:20 - INFO - __main__ -     eval_loss = 0.6911408834987216
05/28/2023 13:57:20 - INFO - __main__ -     global_step = 19
05/28/2023 13:57:20 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:20 - INFO - __main__ -     infer_time = 2.9165555555555556
05/28/2023 13:57:20 - INFO - __main__ -     loss = 0.6966585140479239

Iteration:  26%|##5       | 20/78 [00:02<00:05, 10.16it/s][A
Iteration:  29%|##9       | 23/78 [00:02<00:04, 12.46it/s][A
Iteration:  33%|###3      | 26/78 [00:02<00:03, 14.53it/s][A05/28/2023 13:57:21 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:21 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:57:21 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:21 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 108.43it/s]
05/28/2023 13:57:21 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:21 - INFO - __main__ -    dev: acc = 0.49458483754512633
05/28/2023 13:57:21 - INFO - __main__ -    dev: eval_loss = 0.6937426990932889
05/28/2023 13:57:21 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:21 - INFO - __main__ -    dev: infer_time = 2.9104444444444444
05/28/2023 13:57:21 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:21 - INFO - __main__ -     acc = 0.49458483754512633
05/28/2023 13:57:21 - INFO - __main__ -     cls_loss = 0.6976258343663709
05/28/2023 13:57:21 - INFO - __main__ -     eval_loss = 0.6937426990932889
05/28/2023 13:57:21 - INFO - __main__ -     global_step = 29
05/28/2023 13:57:21 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:21 - INFO - __main__ -     infer_time = 2.9104444444444444
05/28/2023 13:57:21 - INFO - __main__ -     loss = 0.6976258343663709

Iteration:  37%|###7      | 29/78 [00:02<00:03, 14.10it/s][A
Iteration:  41%|####1     | 32/78 [00:03<00:02, 15.85it/s][A
Iteration:  45%|####4     | 35/78 [00:03<00:02, 17.28it/s][A
Iteration:  49%|####8     | 38/78 [00:03<00:02, 18.54it/s][A05/28/2023 13:57:21 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:21 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:57:21 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:21 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 108.61it/s]
05/28/2023 13:57:21 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:21 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:21 - INFO - __main__ -    dev: eval_loss = 0.692019349998898
05/28/2023 13:57:21 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:21 - INFO - __main__ -    dev: infer_time = 2.8998888888888885
05/28/2023 13:57:21 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:21 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:21 - INFO - __main__ -     cls_loss = 0.6990318374756055
05/28/2023 13:57:21 - INFO - __main__ -     eval_loss = 0.692019349998898
05/28/2023 13:57:21 - INFO - __main__ -     global_step = 39
05/28/2023 13:57:21 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:21 - INFO - __main__ -     infer_time = 2.8998888888888885
05/28/2023 13:57:21 - INFO - __main__ -     loss = 0.6990318374756055

Iteration:  53%|#####2    | 41/78 [00:03<00:02, 16.47it/s][A
Iteration:  56%|#####6    | 44/78 [00:03<00:01, 17.81it/s][A
Iteration:  60%|######    | 47/78 [00:03<00:01, 18.93it/s][A05/28/2023 13:57:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:22 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:57:22 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 108.59it/s]
05/28/2023 13:57:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:22 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:22 - INFO - __main__ -    dev: eval_loss = 0.7044597731696235
05/28/2023 13:57:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:22 - INFO - __main__ -    dev: infer_time = 2.919
05/28/2023 13:57:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:22 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:22 - INFO - __main__ -     cls_loss = 0.6994480210907605
05/28/2023 13:57:22 - INFO - __main__ -     eval_loss = 0.7044597731696235
05/28/2023 13:57:22 - INFO - __main__ -     global_step = 49
05/28/2023 13:57:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:22 - INFO - __main__ -     infer_time = 2.919
05/28/2023 13:57:22 - INFO - __main__ -     loss = 0.6994480210907605

Iteration:  64%|######4   | 50/78 [00:04<00:01, 16.55it/s][A
Iteration:  68%|######7   | 53/78 [00:04<00:01, 17.78it/s][A
Iteration:  72%|#######1  | 56/78 [00:04<00:01, 18.86it/s][A05/28/2023 13:57:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:22 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:57:22 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 108.63it/s]
05/28/2023 13:57:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:22 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:22 - INFO - __main__ -    dev: eval_loss = 0.7082985374662611
05/28/2023 13:57:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:22 - INFO - __main__ -    dev: infer_time = 2.9038888888888885
05/28/2023 13:57:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:22 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:22 - INFO - __main__ -     cls_loss = 0.6977059730028702
05/28/2023 13:57:22 - INFO - __main__ -     eval_loss = 0.7082985374662611
05/28/2023 13:57:22 - INFO - __main__ -     global_step = 59
05/28/2023 13:57:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:22 - INFO - __main__ -     infer_time = 2.9038888888888885
05/28/2023 13:57:22 - INFO - __main__ -     loss = 0.6977059730028702

Iteration:  76%|#######5  | 59/78 [00:04<00:01, 16.56it/s][A
Iteration:  78%|#######8  | 61/78 [00:04<00:00, 17.14it/s][A
Iteration:  82%|########2 | 64/78 [00:04<00:00, 18.45it/s][A
Iteration:  86%|########5 | 67/78 [00:04<00:00, 19.40it/s][A05/28/2023 13:57:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:23 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:57:23 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 108.78it/s]
05/28/2023 13:57:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:23 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:23 - INFO - __main__ -    dev: eval_loss = 0.696662081612481
05/28/2023 13:57:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:23 - INFO - __main__ -    dev: infer_time = 2.897777777777778
05/28/2023 13:57:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:23 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:23 - INFO - __main__ -     cls_loss = 0.698519948599995
05/28/2023 13:57:23 - INFO - __main__ -     eval_loss = 0.696662081612481
05/28/2023 13:57:23 - INFO - __main__ -     global_step = 69
05/28/2023 13:57:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:23 - INFO - __main__ -     infer_time = 2.897777777777778
05/28/2023 13:57:23 - INFO - __main__ -     loss = 0.698519948599995

Iteration:  90%|########9 | 70/78 [00:05<00:00, 16.81it/s][A
Iteration:  94%|#########3| 73/78 [00:05<00:00, 18.08it/s][A
Iteration:  97%|#########7| 76/78 [00:05<00:00, 19.15it/s][AIteration: 100%|##########| 78/78 [00:05<00:00, 14.20it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.49s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.49s/it]
05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   w_emb: 15016824

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   p_emb: 251904

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   t_emb: 984

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   ln_emb: 984

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 457504

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 457068

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   intermediate_numel: 457504

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   output_numel: 458052

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 457504

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 457068

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   intermediate_numel: 457504

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   output_numel: 458052

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 457504

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 457068

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   intermediate_numel: 457504

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   output_numel: 458052

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   layer_numel: 5660292
05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   dense_numel: 242556
05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   emb_numel: 15270696

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   encoder_numel: 5660292

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   pooler_numel: 242556

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   all parameters: 21173544

05/28/2023 13:57:23 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:23 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [928, 928, 928], 'sample_qkv_sizes': [492, 492, 492]}
parameter size = 21173544
best_acc = 0.5306859205776173
time_per_batch_infer = 2.904 ms
infer_cnt = 63
**************E*************

05/28/2023 13:57:23 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [768, 768, 768], 'sample_qkv_sizes': [432, 432, 432]}
05/28/2023 13:57:23 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:57:23 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:57:23 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:23 - INFO - __main__ -   guid: train-0
05/28/2023 13:57:23 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:57:23 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:23 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:23 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:23 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:23 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:25 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:57:25 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:25 - INFO - __main__ -   guid: dev-0
05/28/2023 13:57:25 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:57:25 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:25 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:25 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:25 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:57:25 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:57:26 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:57:26 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:57:26 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:57:26 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:57:26 - INFO - __main__ -     Batch size = 32
05/28/2023 13:57:26 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.39it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.65it/s][A05/28/2023 13:57:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:26 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:57:26 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.54it/s]
05/28/2023 13:57:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:26 - INFO - __main__ -    dev: acc = 0.48014440433212996
05/28/2023 13:57:26 - INFO - __main__ -    dev: eval_loss = 0.6939394010437859
05/28/2023 13:57:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:26 - INFO - __main__ -    dev: infer_time = 2.8894444444444445
05/28/2023 13:57:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:26 - INFO - __main__ -     acc = 0.48014440433212996
05/28/2023 13:57:26 - INFO - __main__ -     cls_loss = 0.700288262632158
05/28/2023 13:57:26 - INFO - __main__ -     eval_loss = 0.6939394010437859
05/28/2023 13:57:26 - INFO - __main__ -     global_step = 9
05/28/2023 13:57:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:26 - INFO - __main__ -     infer_time = 2.8894444444444445
05/28/2023 13:57:26 - INFO - __main__ -     loss = 0.700288262632158
05/28/2023 13:57:26 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.95it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.87it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.03it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.39it/s][A05/28/2023 13:57:28 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:28 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:57:28 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:28 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.30it/s]
05/28/2023 13:57:28 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:28 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 13:57:28 - INFO - __main__ -    dev: eval_loss = 0.6930840677685208
05/28/2023 13:57:28 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:28 - INFO - __main__ -    dev: infer_time = 2.916111111111111
05/28/2023 13:57:28 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:28 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 13:57:28 - INFO - __main__ -     cls_loss = 0.6983376459071511
05/28/2023 13:57:28 - INFO - __main__ -     eval_loss = 0.6930840677685208
05/28/2023 13:57:28 - INFO - __main__ -     global_step = 19
05/28/2023 13:57:28 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:28 - INFO - __main__ -     infer_time = 2.916111111111111
05/28/2023 13:57:28 - INFO - __main__ -     loss = 0.6983376459071511
05/28/2023 13:57:28 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.36it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.90it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.69it/s][A05/28/2023 13:57:30 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:30 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:57:30 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:30 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.61it/s]
05/28/2023 13:57:30 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:30 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:30 - INFO - __main__ -    dev: eval_loss = 0.6978181136978997
05/28/2023 13:57:30 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:30 - INFO - __main__ -    dev: infer_time = 2.9095555555555555
05/28/2023 13:57:30 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:30 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:30 - INFO - __main__ -     cls_loss = 0.6975905196420078
05/28/2023 13:57:30 - INFO - __main__ -     eval_loss = 0.6978181136978997
05/28/2023 13:57:30 - INFO - __main__ -     global_step = 29
05/28/2023 13:57:30 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:30 - INFO - __main__ -     infer_time = 2.9095555555555555
05/28/2023 13:57:30 - INFO - __main__ -     loss = 0.6975905196420078

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.41it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.66it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.99it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 15.09it/s][A05/28/2023 13:57:30 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:30 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:57:30 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:30 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.74it/s]
05/28/2023 13:57:30 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:30 - INFO - __main__ -    dev: acc = 0.516245487364621
05/28/2023 13:57:30 - INFO - __main__ -    dev: eval_loss = 0.6915064983897738
05/28/2023 13:57:30 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:30 - INFO - __main__ -    dev: infer_time = 2.900666666666667
05/28/2023 13:57:30 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:30 - INFO - __main__ -     acc = 0.516245487364621
05/28/2023 13:57:30 - INFO - __main__ -     cls_loss = 0.6966757025474157
05/28/2023 13:57:30 - INFO - __main__ -     eval_loss = 0.6915064983897738
05/28/2023 13:57:30 - INFO - __main__ -     global_step = 39
05/28/2023 13:57:30 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:30 - INFO - __main__ -     infer_time = 2.900666666666667
05/28/2023 13:57:30 - INFO - __main__ -     loss = 0.6966757025474157

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.94it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.83it/s][A
Iteration:  60%|######    | 47/78 [00:04<00:01, 18.58it/s][A05/28/2023 13:57:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:31 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:57:31 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.89it/s]
05/28/2023 13:57:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:31 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 13:57:31 - INFO - __main__ -    dev: eval_loss = 0.6908837755521139
05/28/2023 13:57:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:31 - INFO - __main__ -    dev: infer_time = 2.8845555555555555
05/28/2023 13:57:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:31 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 13:57:31 - INFO - __main__ -     cls_loss = 0.6959320501405366
05/28/2023 13:57:31 - INFO - __main__ -     eval_loss = 0.6908837755521139
05/28/2023 13:57:31 - INFO - __main__ -     global_step = 49
05/28/2023 13:57:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:31 - INFO - __main__ -     infer_time = 2.8845555555555555
05/28/2023 13:57:31 - INFO - __main__ -     loss = 0.6959320501405366

Iteration:  64%|######4   | 50/78 [00:05<00:01, 17.08it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 18.76it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 20.05it/s][A05/28/2023 13:57:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:31 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:57:31 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.85it/s]
05/28/2023 13:57:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:31 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:31 - INFO - __main__ -    dev: eval_loss = 0.6956845919291178
05/28/2023 13:57:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:31 - INFO - __main__ -    dev: infer_time = 2.898333333333333
05/28/2023 13:57:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:31 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:31 - INFO - __main__ -     cls_loss = 0.6953281154066829
05/28/2023 13:57:31 - INFO - __main__ -     eval_loss = 0.6956845919291178
05/28/2023 13:57:31 - INFO - __main__ -     global_step = 59
05/28/2023 13:57:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:31 - INFO - __main__ -     infer_time = 2.898333333333333
05/28/2023 13:57:31 - INFO - __main__ -     loss = 0.6953281154066829

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 17.62it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:00, 18.70it/s][A
Iteration:  83%|########3 | 65/78 [00:05<00:00, 20.10it/s][A
Iteration:  87%|########7 | 68/78 [00:05<00:00, 21.06it/s][A05/28/2023 13:57:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:32 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:57:32 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.83it/s]
05/28/2023 13:57:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:32 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 13:57:32 - INFO - __main__ -    dev: eval_loss = 0.6933491230010986
05/28/2023 13:57:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:32 - INFO - __main__ -    dev: infer_time = 2.916
05/28/2023 13:57:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:32 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 13:57:32 - INFO - __main__ -     cls_loss = 0.6956091929173124
05/28/2023 13:57:32 - INFO - __main__ -     eval_loss = 0.6933491230010986
05/28/2023 13:57:32 - INFO - __main__ -     global_step = 69
05/28/2023 13:57:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:32 - INFO - __main__ -     infer_time = 2.916
05/28/2023 13:57:32 - INFO - __main__ -     loss = 0.6956091929173124

Iteration:  91%|#########1| 71/78 [00:06<00:00, 18.24it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 19.72it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 20.68it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.09it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.45s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.45s/it]
05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   w_emb: 13185504

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   p_emb: 221184

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   t_emb: 864

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   ln_emb: 864

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 332544

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 332208

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   intermediate_numel: 332544

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   output_numel: 333072

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 332544

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 332208

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   intermediate_numel: 332544

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   output_numel: 333072

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 332544

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 332208

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   intermediate_numel: 332544

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   output_numel: 333072

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   layer_numel: 4244112
05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   dense_numel: 187056
05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   emb_numel: 13408416

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   encoder_numel: 4244112

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   pooler_numel: 187056

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   all parameters: 17839584

05/28/2023 13:57:32 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:32 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [768, 768, 768], 'sample_qkv_sizes': [432, 432, 432]}
parameter size = 17839584
best_acc = 0.51985559566787
time_per_batch_infer = 2.902 ms
infer_cnt = 63
**************E*************

05/28/2023 13:57:32 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [800, 800, 800], 'sample_qkv_sizes': [432, 432, 432]}
05/28/2023 13:57:32 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:57:32 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:57:32 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:32 - INFO - __main__ -   guid: train-0
05/28/2023 13:57:32 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:57:32 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:32 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:32 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:32 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:32 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:34 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:57:34 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:34 - INFO - __main__ -   guid: dev-0
05/28/2023 13:57:34 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:57:34 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:34 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:34 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:34 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:34 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:34 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:57:34 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:57:35 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:57:35 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:57:35 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:57:35 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:57:35 - INFO - __main__ -     Batch size = 32
05/28/2023 13:57:35 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.61it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.58it/s][A05/28/2023 13:57:35 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:35 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:57:35 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:35 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.65it/s]
05/28/2023 13:57:35 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:35 - INFO - __main__ -    dev: acc = 0.48736462093862815
05/28/2023 13:57:35 - INFO - __main__ -    dev: eval_loss = 0.6929986675580343
05/28/2023 13:57:35 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:35 - INFO - __main__ -    dev: infer_time = 2.872
05/28/2023 13:57:35 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:35 - INFO - __main__ -     acc = 0.48736462093862815
05/28/2023 13:57:35 - INFO - __main__ -     cls_loss = 0.7030734618504842
05/28/2023 13:57:35 - INFO - __main__ -     eval_loss = 0.6929986675580343
05/28/2023 13:57:35 - INFO - __main__ -     global_step = 9
05/28/2023 13:57:35 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:35 - INFO - __main__ -     infer_time = 2.872
05/28/2023 13:57:35 - INFO - __main__ -     loss = 0.7030734618504842
05/28/2023 13:57:35 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.75it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.59it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.74it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.01it/s][A05/28/2023 13:57:37 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:37 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:57:37 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:37 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.01it/s]
05/28/2023 13:57:37 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:37 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:37 - INFO - __main__ -    dev: eval_loss = 0.6966197755601671
05/28/2023 13:57:37 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:37 - INFO - __main__ -    dev: infer_time = 2.9314444444444447
05/28/2023 13:57:37 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:37 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:37 - INFO - __main__ -     cls_loss = 0.7022428575314974
05/28/2023 13:57:37 - INFO - __main__ -     eval_loss = 0.6966197755601671
05/28/2023 13:57:37 - INFO - __main__ -     global_step = 19
05/28/2023 13:57:37 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:37 - INFO - __main__ -     infer_time = 2.9314444444444447
05/28/2023 13:57:37 - INFO - __main__ -     loss = 0.7022428575314974

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.06it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 13.25it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.38it/s][A05/28/2023 13:57:38 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:38 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:57:38 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:38 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.48it/s]
05/28/2023 13:57:38 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:38 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:38 - INFO - __main__ -    dev: eval_loss = 0.6923294133610196
05/28/2023 13:57:38 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:38 - INFO - __main__ -    dev: infer_time = 2.8840000000000003
05/28/2023 13:57:38 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:38 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:38 - INFO - __main__ -     cls_loss = 0.7002571122399692
05/28/2023 13:57:38 - INFO - __main__ -     eval_loss = 0.6923294133610196
05/28/2023 13:57:38 - INFO - __main__ -     global_step = 29
05/28/2023 13:57:38 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:38 - INFO - __main__ -     infer_time = 2.8840000000000003
05/28/2023 13:57:38 - INFO - __main__ -     loss = 0.7002571122399692
05/28/2023 13:57:38 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:09,  4.92it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:06,  6.47it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:05,  8.32it/s][A05/28/2023 13:57:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:39 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:57:39 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.14it/s]
05/28/2023 13:57:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:39 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:39 - INFO - __main__ -    dev: eval_loss = 0.6912515693240695
05/28/2023 13:57:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:39 - INFO - __main__ -    dev: infer_time = 2.890888888888889
05/28/2023 13:57:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:39 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:39 - INFO - __main__ -     cls_loss = 0.7003199320573074
05/28/2023 13:57:39 - INFO - __main__ -     eval_loss = 0.6912515693240695
05/28/2023 13:57:39 - INFO - __main__ -     global_step = 39
05/28/2023 13:57:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:39 - INFO - __main__ -     infer_time = 2.890888888888889
05/28/2023 13:57:39 - INFO - __main__ -     loss = 0.7003199320573074

Iteration:  50%|#####     | 39/78 [00:04<00:04,  9.55it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:03, 11.60it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:02, 13.77it/s][A
Iteration:  62%|######1   | 48/78 [00:05<00:01, 15.70it/s][A05/28/2023 13:57:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:40 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:57:40 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.47it/s]
05/28/2023 13:57:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:40 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:40 - INFO - __main__ -    dev: eval_loss = 0.6965242028236389
05/28/2023 13:57:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:40 - INFO - __main__ -    dev: infer_time = 2.910444444444445
05/28/2023 13:57:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:40 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:40 - INFO - __main__ -     cls_loss = 0.6994603762821275
05/28/2023 13:57:40 - INFO - __main__ -     eval_loss = 0.6965242028236389
05/28/2023 13:57:40 - INFO - __main__ -     global_step = 49
05/28/2023 13:57:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:40 - INFO - __main__ -     infer_time = 2.910444444444445
05/28/2023 13:57:40 - INFO - __main__ -     loss = 0.6994603762821275

Iteration:  65%|######5   | 51/78 [00:05<00:01, 15.20it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 17.07it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 18.60it/s][A05/28/2023 13:57:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:40 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:57:40 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.54it/s]
05/28/2023 13:57:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:40 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:40 - INFO - __main__ -    dev: eval_loss = 0.6963192953003777
05/28/2023 13:57:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:40 - INFO - __main__ -    dev: infer_time = 2.8816666666666664
05/28/2023 13:57:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:40 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:40 - INFO - __main__ -     cls_loss = 0.6986634741395207
05/28/2023 13:57:40 - INFO - __main__ -     eval_loss = 0.6963192953003777
05/28/2023 13:57:40 - INFO - __main__ -     global_step = 59
05/28/2023 13:57:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:40 - INFO - __main__ -     infer_time = 2.8816666666666664
05/28/2023 13:57:40 - INFO - __main__ -     loss = 0.6986634741395207

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 17.05it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 18.73it/s][A
Iteration:  85%|########4 | 66/78 [00:06<00:00, 19.87it/s][A05/28/2023 13:57:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:41 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:57:41 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.64it/s]
05/28/2023 13:57:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:41 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:41 - INFO - __main__ -    dev: eval_loss = 0.6951980921957228
05/28/2023 13:57:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:41 - INFO - __main__ -    dev: infer_time = 2.8826666666666667
05/28/2023 13:57:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:41 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:41 - INFO - __main__ -     cls_loss = 0.6980165528214496
05/28/2023 13:57:41 - INFO - __main__ -     eval_loss = 0.6951980921957228
05/28/2023 13:57:41 - INFO - __main__ -     global_step = 69
05/28/2023 13:57:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:41 - INFO - __main__ -     infer_time = 2.8826666666666667
05/28/2023 13:57:41 - INFO - __main__ -     loss = 0.6980165528214496

Iteration:  88%|########8 | 69/78 [00:06<00:00, 17.94it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 19.30it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 20.40it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.87it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.57s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.57s/it]
05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   w_emb: 13185504

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   p_emb: 221184

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   t_emb: 864

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   ln_emb: 864

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 346400

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 346032

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   intermediate_numel: 346400

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   output_numel: 346896

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 346400

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 346032

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   intermediate_numel: 346400

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   output_numel: 346896

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 346400

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 346032

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   intermediate_numel: 346400

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   output_numel: 346896

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   layer_numel: 4327152
05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   dense_numel: 187056
05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   emb_numel: 13408416

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   encoder_numel: 4327152

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   pooler_numel: 187056

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   all parameters: 17922624

05/28/2023 13:57:41 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:41 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [800, 800, 800], 'sample_qkv_sizes': [432, 432, 432]}
parameter size = 17922624
best_acc = 0.5270758122743683
time_per_batch_infer = 2.893 ms
infer_cnt = 63
**************E*************

05/28/2023 13:57:41 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [252, 252, 252, 252]}
05/28/2023 13:57:41 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:57:41 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:57:41 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:41 - INFO - __main__ -   guid: train-0
05/28/2023 13:57:41 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:57:41 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:41 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:41 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:41 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:41 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:43 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:57:43 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:43 - INFO - __main__ -   guid: dev-0
05/28/2023 13:57:43 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:57:43 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:43 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:43 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:43 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:57:43 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:57:44 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:57:44 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:57:44 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:57:44 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:57:44 - INFO - __main__ -     Batch size = 32
05/28/2023 13:57:44 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.91it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.76it/s][A05/28/2023 13:57:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:44 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:57:44 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.82it/s]
05/28/2023 13:57:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:44 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:44 - INFO - __main__ -    dev: eval_loss = 0.6955308450592889
05/28/2023 13:57:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:44 - INFO - __main__ -    dev: infer_time = 3.511444444444445
05/28/2023 13:57:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:44 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:44 - INFO - __main__ -     cls_loss = 0.6902771790822347
05/28/2023 13:57:44 - INFO - __main__ -     eval_loss = 0.6955308450592889
05/28/2023 13:57:44 - INFO - __main__ -     global_step = 9
05/28/2023 13:57:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:44 - INFO - __main__ -     infer_time = 3.511444444444445
05/28/2023 13:57:44 - INFO - __main__ -     loss = 0.6902771790822347
05/28/2023 13:57:44 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.02it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.94it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.20it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.69it/s][A05/28/2023 13:57:46 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:46 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:57:46 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:46 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.98it/s]
05/28/2023 13:57:46 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:46 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:46 - INFO - __main__ -    dev: eval_loss = 0.7010010414653354
05/28/2023 13:57:46 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:46 - INFO - __main__ -    dev: infer_time = 3.5118888888888886
05/28/2023 13:57:46 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:46 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:46 - INFO - __main__ -     cls_loss = 0.6976595928794459
05/28/2023 13:57:46 - INFO - __main__ -     eval_loss = 0.7010010414653354
05/28/2023 13:57:46 - INFO - __main__ -     global_step = 19
05/28/2023 13:57:46 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:46 - INFO - __main__ -     infer_time = 3.5118888888888886
05/28/2023 13:57:46 - INFO - __main__ -     loss = 0.6976595928794459

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.89it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.34it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.60it/s][A05/28/2023 13:57:46 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:46 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:57:46 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:46 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.03it/s]
05/28/2023 13:57:46 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:46 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:46 - INFO - __main__ -    dev: eval_loss = 0.6983085142241584
05/28/2023 13:57:46 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:46 - INFO - __main__ -    dev: infer_time = 3.5218888888888884
05/28/2023 13:57:46 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:46 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:46 - INFO - __main__ -     cls_loss = 0.6974693660078377
05/28/2023 13:57:46 - INFO - __main__ -     eval_loss = 0.6983085142241584
05/28/2023 13:57:46 - INFO - __main__ -     global_step = 29
05/28/2023 13:57:46 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:46 - INFO - __main__ -     infer_time = 3.5218888888888884
05/28/2023 13:57:46 - INFO - __main__ -     loss = 0.6974693660078377

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.42it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 18.46it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 20.26it/s][A05/28/2023 13:57:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:47 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:57:47 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.05it/s]
05/28/2023 13:57:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:47 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:47 - INFO - __main__ -    dev: eval_loss = 0.6962288816769918
05/28/2023 13:57:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:47 - INFO - __main__ -    dev: infer_time = 3.5269999999999997
05/28/2023 13:57:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:47 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:47 - INFO - __main__ -     cls_loss = 0.6958782703448565
05/28/2023 13:57:47 - INFO - __main__ -     eval_loss = 0.6962288816769918
05/28/2023 13:57:47 - INFO - __main__ -     global_step = 39
05/28/2023 13:57:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:47 - INFO - __main__ -     infer_time = 3.5269999999999997
05/28/2023 13:57:47 - INFO - __main__ -     loss = 0.6958782703448565

Iteration:  50%|#####     | 39/78 [00:03<00:02, 19.03it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 20.45it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 21.84it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 22.42it/s][A05/28/2023 13:57:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:47 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:57:47 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.43it/s]
05/28/2023 13:57:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:47 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:47 - INFO - __main__ -    dev: eval_loss = 0.6980416907204522
05/28/2023 13:57:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:47 - INFO - __main__ -    dev: infer_time = 3.5163333333333333
05/28/2023 13:57:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:47 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:47 - INFO - __main__ -     cls_loss = 0.6951780002944323
05/28/2023 13:57:47 - INFO - __main__ -     eval_loss = 0.6980416907204522
05/28/2023 13:57:47 - INFO - __main__ -     global_step = 49
05/28/2023 13:57:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:47 - INFO - __main__ -     infer_time = 3.5163333333333333
05/28/2023 13:57:47 - INFO - __main__ -     loss = 0.6951780002944323

Iteration:  65%|######5   | 51/78 [00:03<00:01, 20.16it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 21.54it/s][A
Iteration:  73%|#######3  | 57/78 [00:03<00:00, 22.76it/s][A05/28/2023 13:57:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:48 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:57:48 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.45it/s]
05/28/2023 13:57:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:48 - INFO - __main__ -    dev: acc = 0.4693140794223827
05/28/2023 13:57:48 - INFO - __main__ -    dev: eval_loss = 0.6935663885540433
05/28/2023 13:57:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:48 - INFO - __main__ -    dev: infer_time = 3.5052222222222222
05/28/2023 13:57:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:48 - INFO - __main__ -     acc = 0.4693140794223827
05/28/2023 13:57:48 - INFO - __main__ -     cls_loss = 0.6955698546716722
05/28/2023 13:57:48 - INFO - __main__ -     eval_loss = 0.6935663885540433
05/28/2023 13:57:48 - INFO - __main__ -     global_step = 59
05/28/2023 13:57:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:48 - INFO - __main__ -     infer_time = 3.5052222222222222
05/28/2023 13:57:48 - INFO - __main__ -     loss = 0.6955698546716722

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 20.45it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 21.73it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 22.86it/s][A05/28/2023 13:57:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:48 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:57:48 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.52it/s]
05/28/2023 13:57:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:48 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:48 - INFO - __main__ -    dev: eval_loss = 0.6916475163565742
05/28/2023 13:57:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:48 - INFO - __main__ -    dev: infer_time = 3.5237777777777786
05/28/2023 13:57:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:48 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:48 - INFO - __main__ -     cls_loss = 0.6953653235366379
05/28/2023 13:57:48 - INFO - __main__ -     eval_loss = 0.6916475163565742
05/28/2023 13:57:48 - INFO - __main__ -     global_step = 69
05/28/2023 13:57:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:48 - INFO - __main__ -     infer_time = 3.5237777777777786
05/28/2023 13:57:48 - INFO - __main__ -     loss = 0.6953653235366379

Iteration:  88%|########8 | 69/78 [00:04<00:00, 20.65it/s][A
Iteration:  92%|#########2| 72/78 [00:04<00:00, 21.71it/s][A
Iteration:  96%|#########6| 75/78 [00:04<00:00, 22.68it/s][AIteration: 100%|##########| 78/78 [00:04<00:00, 16.18it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.82s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.82s/it]
05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   w_emb: 7691544

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   p_emb: 129024

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   t_emb: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ln_emb: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 113344

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 113148

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 113344

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   output_numel: 113652

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 113344

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 113148

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 113344

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   output_numel: 113652

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 113344

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 113148

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 113344

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   output_numel: 113652

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 113344

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 113148

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 113344

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   output_numel: 113652

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   layer_numel: 1930096
05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   dense_numel: 63756
05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   emb_numel: 7821576

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   encoder_numel: 1930096

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   pooler_numel: 63756

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   all parameters: 9815428

05/28/2023 13:57:49 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:49 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [252, 252, 252, 252]}
parameter size = 9815428
best_acc = 0.5270758122743683
time_per_batch_infer = 3.517 ms
infer_cnt = 63
**************E*************

05/28/2023 13:57:49 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [832, 832, 832], 'sample_qkv_sizes': [468, 468, 468]}
05/28/2023 13:57:49 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:57:49 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:57:49 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:49 - INFO - __main__ -   guid: train-0
05/28/2023 13:57:49 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:57:49 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:49 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:49 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:50 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:57:50 - INFO - __main__ -   *** Example ***
05/28/2023 13:57:50 - INFO - __main__ -   guid: dev-0
05/28/2023 13:57:50 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:57:50 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:50 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:57:50 - INFO - __main__ -   label: not_entailment
05/28/2023 13:57:50 - INFO - __main__ -   label_id: 1
05/28/2023 13:57:51 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:57:51 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:57:51 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:57:51 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:57:51 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:57:51 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:57:51 - INFO - __main__ -     Batch size = 32
05/28/2023 13:57:51 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.39it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.69it/s][A05/28/2023 13:57:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:52 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:57:52 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.15it/s]
05/28/2023 13:57:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:52 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:52 - INFO - __main__ -    dev: eval_loss = 0.6917236513561673
05/28/2023 13:57:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:52 - INFO - __main__ -    dev: infer_time = 2.893444444444444
05/28/2023 13:57:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:52 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:52 - INFO - __main__ -     cls_loss = 0.6945318645901151
05/28/2023 13:57:52 - INFO - __main__ -     eval_loss = 0.6917236513561673
05/28/2023 13:57:52 - INFO - __main__ -     global_step = 9
05/28/2023 13:57:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:52 - INFO - __main__ -     infer_time = 2.893444444444444
05/28/2023 13:57:52 - INFO - __main__ -     loss = 0.6945318645901151
05/28/2023 13:57:52 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.94it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.82it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  7.97it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.20it/s][A05/28/2023 13:57:53 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:53 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:57:53 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:53 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.87it/s]
05/28/2023 13:57:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:54 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 13:57:54 - INFO - __main__ -    dev: eval_loss = 0.6905730499161614
05/28/2023 13:57:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:54 - INFO - __main__ -    dev: infer_time = 2.912111111111111
05/28/2023 13:57:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:54 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 13:57:54 - INFO - __main__ -     cls_loss = 0.6961298114375064
05/28/2023 13:57:54 - INFO - __main__ -     eval_loss = 0.6905730499161614
05/28/2023 13:57:54 - INFO - __main__ -     global_step = 19
05/28/2023 13:57:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:54 - INFO - __main__ -     infer_time = 2.912111111111111
05/28/2023 13:57:54 - INFO - __main__ -     loss = 0.6961298114375064
05/28/2023 13:57:54 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.35it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.86it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.63it/s][A05/28/2023 13:57:55 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:55 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:57:55 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:55 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.09it/s]
05/28/2023 13:57:55 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:55 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 13:57:55 - INFO - __main__ -    dev: eval_loss = 0.6925035119056702
05/28/2023 13:57:55 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:55 - INFO - __main__ -    dev: infer_time = 2.9159999999999995
05/28/2023 13:57:55 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:55 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 13:57:55 - INFO - __main__ -     cls_loss = 0.6962261323271126
05/28/2023 13:57:55 - INFO - __main__ -     eval_loss = 0.6925035119056702
05/28/2023 13:57:55 - INFO - __main__ -     global_step = 29
05/28/2023 13:57:55 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:55 - INFO - __main__ -     infer_time = 2.9159999999999995
05/28/2023 13:57:55 - INFO - __main__ -     loss = 0.6962261323271126

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.28it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.48it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.66it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.67it/s][A05/28/2023 13:57:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:56 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:57:56 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.05it/s]
05/28/2023 13:57:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:56 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 13:57:56 - INFO - __main__ -    dev: eval_loss = 0.69068400727378
05/28/2023 13:57:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:56 - INFO - __main__ -    dev: infer_time = 2.896777777777778
05/28/2023 13:57:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:56 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 13:57:56 - INFO - __main__ -     cls_loss = 0.6957842508951823
05/28/2023 13:57:56 - INFO - __main__ -     eval_loss = 0.69068400727378
05/28/2023 13:57:56 - INFO - __main__ -     global_step = 39
05/28/2023 13:57:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:56 - INFO - __main__ -     infer_time = 2.896777777777778
05/28/2023 13:57:56 - INFO - __main__ -     loss = 0.6957842508951823

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.30it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.02it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.60it/s][A05/28/2023 13:57:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:56 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:57:56 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.32it/s]
05/28/2023 13:57:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:56 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:57:56 - INFO - __main__ -    dev: eval_loss = 0.6919218169318305
05/28/2023 13:57:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:56 - INFO - __main__ -    dev: infer_time = 2.8968888888888893
05/28/2023 13:57:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:56 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:57:56 - INFO - __main__ -     cls_loss = 0.69533564241565
05/28/2023 13:57:56 - INFO - __main__ -     eval_loss = 0.6919218169318305
05/28/2023 13:57:56 - INFO - __main__ -     global_step = 49
05/28/2023 13:57:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:56 - INFO - __main__ -     infer_time = 2.8968888888888893
05/28/2023 13:57:56 - INFO - __main__ -     loss = 0.69533564241565

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.18it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 17.70it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 19.00it/s][A05/28/2023 13:57:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:57 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:57:57 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.26it/s]
05/28/2023 13:57:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:57 - INFO - __main__ -    dev: acc = 0.5523465703971119
05/28/2023 13:57:57 - INFO - __main__ -    dev: eval_loss = 0.6914249526129829
05/28/2023 13:57:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:57 - INFO - __main__ -    dev: infer_time = 2.8947777777777777
05/28/2023 13:57:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:57 - INFO - __main__ -     acc = 0.5523465703971119
05/28/2023 13:57:57 - INFO - __main__ -     cls_loss = 0.6955843759795367
05/28/2023 13:57:57 - INFO - __main__ -     eval_loss = 0.6914249526129829
05/28/2023 13:57:57 - INFO - __main__ -     global_step = 59
05/28/2023 13:57:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:57 - INFO - __main__ -     infer_time = 2.8947777777777777
05/28/2023 13:57:57 - INFO - __main__ -     loss = 0.6955843759795367
05/28/2023 13:57:57 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  76%|#######5  | 59/78 [00:06<00:03,  5.32it/s][A
Iteration:  79%|#######9  | 62/78 [00:07<00:02,  6.87it/s][A
Iteration:  83%|########3 | 65/78 [00:07<00:01,  8.69it/s][A
Iteration:  87%|########7 | 68/78 [00:07<00:00, 10.66it/s][A05/28/2023 13:57:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:57:59 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:57:59 - INFO - __main__ -     Num examples = 277
05/28/2023 13:57:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.07it/s]
05/28/2023 13:57:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:57:59 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:57:59 - INFO - __main__ -    dev: eval_loss = 0.6952652931213379
05/28/2023 13:57:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:57:59 - INFO - __main__ -    dev: infer_time = 2.883888888888889
05/28/2023 13:57:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:57:59 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:57:59 - INFO - __main__ -     cls_loss = 0.6949235295903855
05/28/2023 13:57:59 - INFO - __main__ -     eval_loss = 0.6952652931213379
05/28/2023 13:57:59 - INFO - __main__ -     global_step = 69
05/28/2023 13:57:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:57:59 - INFO - __main__ -     infer_time = 2.883888888888889
05/28/2023 13:57:59 - INFO - __main__ -     loss = 0.6949235295903855

Iteration:  90%|########9 | 70/78 [00:07<00:00, 10.82it/s][A
Iteration:  94%|#########3| 73/78 [00:07<00:00, 13.02it/s][A
Iteration:  97%|#########7| 76/78 [00:07<00:00, 15.08it/s][AIteration: 100%|##########| 78/78 [00:07<00:00,  9.90it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.88s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.88s/it]
05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   w_emb: 14284296

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   p_emb: 239616

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   t_emb: 936

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   ln_emb: 936

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 390208

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 389844

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   intermediate_numel: 390208

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   output_numel: 390780

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 390208

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 389844

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   intermediate_numel: 390208

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   output_numel: 390780

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 390208

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 389844

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   intermediate_numel: 390208

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   output_numel: 390780

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   layer_numel: 4979676
05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   dense_numel: 219492
05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   emb_numel: 14525784

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   encoder_numel: 4979676

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   pooler_numel: 219492

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   all parameters: 19724952

05/28/2023 13:57:59 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:57:59 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [832, 832, 832], 'sample_qkv_sizes': [468, 468, 468]}
parameter size = 19724952
best_acc = 0.5523465703971119
time_per_batch_infer = 2.899 ms
infer_cnt = 63
**************E*************

Initial eval done
Initialization Pass Done!
>>> Starting Evaluation of EE Iteration 1 ...
05/28/2023 13:58:17 - INFO - __main__ -   device: cuda n_gpu: 1
05/28/2023 13:58:17 - INFO - __main__ -   task_lis: ['rte']
05/28/2023 13:58:17 - INFO - __main__ -   data_dir_lis: ['/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/']
05/28/2023 13:58:17 - INFO - transformer.tokenization -   loading vocabulary file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3/vocab.txt
05/28/2023 13:58:17 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:58:17 - INFO - __main__ -   subbert_configs: [{'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 300, 'sample_intermediate_sizes': [128, 128, 128, 128], 'sample_qkv_sizes': [300, 300, 300, 300]}, {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 504, 'sample_intermediate_sizes': [544, 544, 544], 'sample_qkv_sizes': [504, 504, 504]}, {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}, {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 204, 'sample_intermediate_sizes': [1024, 1024, 1024, 1024], 'sample_qkv_sizes': [204, 204, 204, 204]}, {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [544, 544, 544, 544], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}, {'sample_layer_num': 3, 'sample_hidden_size': 420, 'sample_intermediate_sizes': [768, 768, 768], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [420, 420, 420]}, {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}, {'sample_layer_num': 3, 'sample_hidden_size': 420, 'sample_intermediate_sizes': [736, 736, 736], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [420, 420, 420]}, {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 156, 'sample_intermediate_sizes': [480, 480, 480, 480, 480], 'sample_qkv_sizes': [156, 156, 156, 156, 156]}, {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}, {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 168, 'sample_intermediate_sizes': [704, 704, 704, 704, 704], 'sample_qkv_sizes': [168, 168, 168, 168, 168]}, {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [800, 800, 800], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}, {'sample_layer_num': 3, 'sample_hidden_size': 444, 'sample_intermediate_sizes': [736, 736, 736], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [444, 444, 444]}, {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 204, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_qkv_sizes': [204, 204, 204, 204, 204]}, {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [432, 432, 432, 432]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [448, 448, 448], 'sample_qkv_sizes': [456, 456, 456]}, {'sample_layer_num': 3, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [704, 704, 704], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [432, 432, 432]}, {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [640, 640, 640, 640], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 156, 'sample_intermediate_sizes': [960, 960, 960, 960], 'sample_qkv_sizes': [156, 156, 156, 156]}, {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 168, 'sample_intermediate_sizes': [416, 416, 416, 416, 416], 'sample_qkv_sizes': [168, 168, 168, 168, 168]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 324, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [324, 324, 324, 324]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [640, 640, 640, 640], 'sample_qkv_sizes': [288, 288, 288, 288]}]
05/28/2023 13:58:17 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 300, 'sample_intermediate_sizes': [128, 128, 128, 128], 'sample_qkv_sizes': [300, 300, 300, 300]}
05/28/2023 13:58:17 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:58:17 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:58:17 - INFO - __main__ -   *** Example ***
05/28/2023 13:58:17 - INFO - __main__ -   guid: train-0
05/28/2023 13:58:17 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:58:17 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:17 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:17 - INFO - __main__ -   label: not_entailment
05/28/2023 13:58:17 - INFO - __main__ -   label_id: 1
05/28/2023 13:58:19 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:58:19 - INFO - __main__ -   *** Example ***
05/28/2023 13:58:19 - INFO - __main__ -   guid: dev-0
05/28/2023 13:58:19 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:58:19 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:19 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:19 - INFO - __main__ -   label: not_entailment
05/28/2023 13:58:19 - INFO - __main__ -   label_id: 1
05/28/2023 13:58:19 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:58:19 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:58:20 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:58:23 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:58:23 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:58:23 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:58:23 - INFO - __main__ -     Batch size = 32
05/28/2023 13:58:23 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A/n/home00/lbailey/bigger_and_faster/transformer/optimization.py:248: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  next_m.mul_(beta1).add_(1 - beta1, grad)

Iteration:   3%|2         | 2/78 [00:00<00:04, 15.59it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 20.39it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 22.46it/s][A05/28/2023 13:58:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:23 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:58:23 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 163.42it/s]
05/28/2023 13:58:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:23 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:58:23 - INFO - __main__ -    dev: eval_loss = 0.7059756649865044
05/28/2023 13:58:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:23 - INFO - __main__ -    dev: infer_time = 3.4795555555555553
05/28/2023 13:58:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:23 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:58:23 - INFO - __main__ -     cls_loss = 0.6966557833883498
05/28/2023 13:58:23 - INFO - __main__ -     eval_loss = 0.7059756649865044
05/28/2023 13:58:23 - INFO - __main__ -     global_step = 9
05/28/2023 13:58:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:23 - INFO - __main__ -     infer_time = 3.4795555555555553
05/28/2023 13:58:23 - INFO - __main__ -     loss = 0.6966557833883498
05/28/2023 13:58:23 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:01<00:15,  4.28it/s][A
Iteration:  18%|#7        | 14/78 [00:01<00:10,  6.19it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:07,  8.36it/s][A05/28/2023 13:58:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:25 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:58:25 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.92it/s]
05/28/2023 13:58:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:25 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:58:25 - INFO - __main__ -    dev: eval_loss = 0.7109590437677171
05/28/2023 13:58:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:25 - INFO - __main__ -    dev: infer_time = 3.4608888888888893
05/28/2023 13:58:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:25 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:58:25 - INFO - __main__ -     cls_loss = 0.7083059204252142
05/28/2023 13:58:25 - INFO - __main__ -     eval_loss = 0.7109590437677171
05/28/2023 13:58:25 - INFO - __main__ -     global_step = 19
05/28/2023 13:58:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:25 - INFO - __main__ -     infer_time = 3.4608888888888893
05/28/2023 13:58:25 - INFO - __main__ -     loss = 0.7083059204252142

Iteration:  26%|##5       | 20/78 [00:02<00:05, 10.00it/s][A
Iteration:  29%|##9       | 23/78 [00:02<00:04, 12.44it/s][A
Iteration:  33%|###3      | 26/78 [00:02<00:03, 14.90it/s][A05/28/2023 13:58:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:25 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:58:25 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.83it/s]
05/28/2023 13:58:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:26 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:58:26 - INFO - __main__ -    dev: eval_loss = 0.6946171919504801
05/28/2023 13:58:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:26 - INFO - __main__ -    dev: infer_time = 3.4691111111111113
05/28/2023 13:58:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:26 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:58:26 - INFO - __main__ -     cls_loss = 0.7039743250813978
05/28/2023 13:58:26 - INFO - __main__ -     eval_loss = 0.6946171919504801
05/28/2023 13:58:26 - INFO - __main__ -     global_step = 29
05/28/2023 13:58:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:26 - INFO - __main__ -     infer_time = 3.4691111111111113
05/28/2023 13:58:26 - INFO - __main__ -     loss = 0.7039743250813978

Iteration:  37%|###7      | 29/78 [00:02<00:03, 15.37it/s][A
Iteration:  41%|####1     | 32/78 [00:02<00:02, 17.40it/s][A
Iteration:  45%|####4     | 35/78 [00:02<00:02, 19.17it/s][A
Iteration:  49%|####8     | 38/78 [00:03<00:01, 20.79it/s][A05/28/2023 13:58:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:26 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:58:26 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.96it/s]
05/28/2023 13:58:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:26 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:58:26 - INFO - __main__ -    dev: eval_loss = 0.6939088039928012
05/28/2023 13:58:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:26 - INFO - __main__ -    dev: infer_time = 3.4764444444444447
05/28/2023 13:58:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:26 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:58:26 - INFO - __main__ -     cls_loss = 0.701070120701423
05/28/2023 13:58:26 - INFO - __main__ -     eval_loss = 0.6939088039928012
05/28/2023 13:58:26 - INFO - __main__ -     global_step = 39
05/28/2023 13:58:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:26 - INFO - __main__ -     infer_time = 3.4764444444444447
05/28/2023 13:58:26 - INFO - __main__ -     loss = 0.701070120701423

Iteration:  53%|#####2    | 41/78 [00:03<00:01, 19.09it/s][A
Iteration:  56%|#####6    | 44/78 [00:03<00:01, 20.49it/s][A
Iteration:  60%|######    | 47/78 [00:03<00:01, 21.64it/s][A05/28/2023 13:58:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:26 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:58:26 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.17it/s]
05/28/2023 13:58:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:26 - INFO - __main__ -    dev: acc = 0.5018050541516246
05/28/2023 13:58:26 - INFO - __main__ -    dev: eval_loss = 0.6930309865209792
05/28/2023 13:58:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:26 - INFO - __main__ -    dev: infer_time = 3.467666666666667
05/28/2023 13:58:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:26 - INFO - __main__ -     acc = 0.5018050541516246
05/28/2023 13:58:26 - INFO - __main__ -     cls_loss = 0.6994189936287549
05/28/2023 13:58:26 - INFO - __main__ -     eval_loss = 0.6930309865209792
05/28/2023 13:58:26 - INFO - __main__ -     global_step = 49
05/28/2023 13:58:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:26 - INFO - __main__ -     infer_time = 3.467666666666667
05/28/2023 13:58:26 - INFO - __main__ -     loss = 0.6994189936287549

Iteration:  64%|######4   | 50/78 [00:03<00:01, 19.73it/s][A
Iteration:  68%|######7   | 53/78 [00:03<00:01, 21.10it/s][A
Iteration:  72%|#######1  | 56/78 [00:03<00:00, 22.33it/s][A05/28/2023 13:58:27 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:27 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:58:27 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:27 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.71it/s]
05/28/2023 13:58:27 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:27 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 13:58:27 - INFO - __main__ -    dev: eval_loss = 0.6925884816381667
05/28/2023 13:58:27 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:27 - INFO - __main__ -    dev: infer_time = 3.4917777777777776
05/28/2023 13:58:27 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:27 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 13:58:27 - INFO - __main__ -     cls_loss = 0.6982029181415752
05/28/2023 13:58:27 - INFO - __main__ -     eval_loss = 0.6925884816381667
05/28/2023 13:58:27 - INFO - __main__ -     global_step = 59
05/28/2023 13:58:27 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:27 - INFO - __main__ -     infer_time = 3.4917777777777776
05/28/2023 13:58:27 - INFO - __main__ -     loss = 0.6982029181415752

Iteration:  76%|#######5  | 59/78 [00:04<00:00, 20.28it/s][A
Iteration:  79%|#######9  | 62/78 [00:04<00:00, 21.51it/s][A
Iteration:  83%|########3 | 65/78 [00:04<00:00, 22.48it/s][A
Iteration:  87%|########7 | 68/78 [00:04<00:00, 23.38it/s][A05/28/2023 13:58:27 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:27 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:58:27 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:27 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.16it/s]
05/28/2023 13:58:27 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:27 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 13:58:27 - INFO - __main__ -    dev: eval_loss = 0.6918073495229086
05/28/2023 13:58:27 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:27 - INFO - __main__ -    dev: infer_time = 3.4591111111111115
05/28/2023 13:58:27 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:27 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 13:58:27 - INFO - __main__ -     cls_loss = 0.6973397869994675
05/28/2023 13:58:27 - INFO - __main__ -     eval_loss = 0.6918073495229086
05/28/2023 13:58:27 - INFO - __main__ -     global_step = 69
05/28/2023 13:58:27 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:27 - INFO - __main__ -     infer_time = 3.4591111111111115
05/28/2023 13:58:27 - INFO - __main__ -     loss = 0.6973397869994675

Iteration:  91%|#########1| 71/78 [00:04<00:00, 20.58it/s][A
Iteration:  95%|#########4| 74/78 [00:04<00:00, 21.72it/s][A
Iteration:  99%|#########8| 77/78 [00:04<00:00, 22.81it/s][AIteration: 100%|##########| 78/78 [00:04<00:00, 15.99it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.88s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.88s/it]
05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   w_emb: 9156600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   p_emb: 153600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   t_emb: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ln_emb: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 38528

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 38700

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   intermediate_numel: 38528

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   output_numel: 39300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 38528

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 38700

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   intermediate_numel: 38528

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   output_numel: 39300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 38528

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 38700

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   intermediate_numel: 38528

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   output_numel: 39300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 38528

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 38700

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   intermediate_numel: 38528

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   output_numel: 39300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   layer_numel: 1758512
05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   dense_numel: 90300
05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   emb_numel: 9311400

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   encoder_numel: 1758512

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   pooler_numel: 90300

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   all parameters: 11160212

05/28/2023 13:58:28 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:58:28 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 300, 'sample_intermediate_sizes': [128, 128, 128, 128], 'sample_qkv_sizes': [300, 300, 300, 300]}
parameter size = 11160212
best_acc = 0.5270758122743683
time_per_batch_infer = 3.472 ms
infer_cnt = 63
**************E*************

05/28/2023 13:58:28 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
05/28/2023 13:58:28 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:58:28 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:58:28 - INFO - __main__ -   *** Example ***
05/28/2023 13:58:28 - INFO - __main__ -   guid: train-0
05/28/2023 13:58:28 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:58:28 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:28 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:28 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:28 - INFO - __main__ -   label: not_entailment
05/28/2023 13:58:28 - INFO - __main__ -   label_id: 1
05/28/2023 13:58:29 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:58:29 - INFO - __main__ -   *** Example ***
05/28/2023 13:58:29 - INFO - __main__ -   guid: dev-0
05/28/2023 13:58:29 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:58:29 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:29 - INFO - __main__ -   label: not_entailment
05/28/2023 13:58:29 - INFO - __main__ -   label_id: 1
05/28/2023 13:58:30 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:58:30 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:58:30 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:58:30 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:58:30 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:58:30 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:58:30 - INFO - __main__ -     Batch size = 32
05/28/2023 13:58:30 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.75it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.46it/s][A05/28/2023 13:58:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:31 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:58:31 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 156.88it/s]
05/28/2023 13:58:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:31 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:58:31 - INFO - __main__ -    dev: eval_loss = 0.6912072367138333
05/28/2023 13:58:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:31 - INFO - __main__ -    dev: infer_time = 3.4492222222222226
05/28/2023 13:58:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:31 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:58:31 - INFO - __main__ -     cls_loss = 0.6921962367163764
05/28/2023 13:58:31 - INFO - __main__ -     eval_loss = 0.6912072367138333
05/28/2023 13:58:31 - INFO - __main__ -     global_step = 9
05/28/2023 13:58:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:31 - INFO - __main__ -     infer_time = 3.4492222222222226
05/28/2023 13:58:31 - INFO - __main__ -     loss = 0.6921962367163764
05/28/2023 13:58:31 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.00it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.92it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.16it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.44it/s][A05/28/2023 13:58:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:32 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:58:32 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.23it/s]
05/28/2023 13:58:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:33 - INFO - __main__ -    dev: acc = 0.48375451263537905
05/28/2023 13:58:33 - INFO - __main__ -    dev: eval_loss = 0.6951358318328857
05/28/2023 13:58:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:33 - INFO - __main__ -    dev: infer_time = 3.4295555555555555
05/28/2023 13:58:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:33 - INFO - __main__ -     acc = 0.48375451263537905
05/28/2023 13:58:33 - INFO - __main__ -     cls_loss = 0.6948177249808061
05/28/2023 13:58:33 - INFO - __main__ -     eval_loss = 0.6951358318328857
05/28/2023 13:58:33 - INFO - __main__ -     global_step = 19
05/28/2023 13:58:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:33 - INFO - __main__ -     infer_time = 3.4295555555555555
05/28/2023 13:58:33 - INFO - __main__ -     loss = 0.6948177249808061

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.70it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.07it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.29it/s][A05/28/2023 13:58:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:33 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:58:33 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.41it/s]
05/28/2023 13:58:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:33 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:58:33 - INFO - __main__ -    dev: eval_loss = 0.6917051474253336
05/28/2023 13:58:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:33 - INFO - __main__ -    dev: infer_time = 3.4153333333333333
05/28/2023 13:58:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:33 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:58:33 - INFO - __main__ -     cls_loss = 0.6929961504607365
05/28/2023 13:58:33 - INFO - __main__ -     eval_loss = 0.6917051474253336
05/28/2023 13:58:33 - INFO - __main__ -     global_step = 29
05/28/2023 13:58:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:33 - INFO - __main__ -     infer_time = 3.4153333333333333
05/28/2023 13:58:33 - INFO - __main__ -     loss = 0.6929961504607365

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.29it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 18.27it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 19.88it/s][A05/28/2023 13:58:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:33 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:58:33 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.78it/s]
05/28/2023 13:58:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:33 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:58:33 - INFO - __main__ -    dev: eval_loss = 0.6989733643001981
05/28/2023 13:58:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:33 - INFO - __main__ -    dev: infer_time = 3.413444444444444
05/28/2023 13:58:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:33 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:58:33 - INFO - __main__ -     cls_loss = 0.6941031966453943
05/28/2023 13:58:33 - INFO - __main__ -     eval_loss = 0.6989733643001981
05/28/2023 13:58:33 - INFO - __main__ -     global_step = 39
05/28/2023 13:58:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:33 - INFO - __main__ -     infer_time = 3.413444444444444
05/28/2023 13:58:33 - INFO - __main__ -     loss = 0.6941031966453943

Iteration:  50%|#####     | 39/78 [00:03<00:02, 18.61it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 19.87it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 21.14it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 22.14it/s][A05/28/2023 13:58:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:34 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:58:34 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.42it/s]
05/28/2023 13:58:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:34 - INFO - __main__ -    dev: acc = 0.5703971119133574
05/28/2023 13:58:34 - INFO - __main__ -    dev: eval_loss = 0.6904776957299974
05/28/2023 13:58:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:34 - INFO - __main__ -    dev: infer_time = 3.4458888888888892
05/28/2023 13:58:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:34 - INFO - __main__ -     acc = 0.5703971119133574
05/28/2023 13:58:34 - INFO - __main__ -     cls_loss = 0.6946120383788128
05/28/2023 13:58:34 - INFO - __main__ -     eval_loss = 0.6904776957299974
05/28/2023 13:58:34 - INFO - __main__ -     global_step = 49
05/28/2023 13:58:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:34 - INFO - __main__ -     infer_time = 3.4458888888888892
05/28/2023 13:58:34 - INFO - __main__ -     loss = 0.6946120383788128
05/28/2023 13:58:34 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  65%|######5   | 51/78 [00:05<00:04,  5.54it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:03,  7.23it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:02,  9.19it/s][A05/28/2023 13:58:36 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:36 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:58:36 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:36 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 156.99it/s]
05/28/2023 13:58:36 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:36 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:58:36 - INFO - __main__ -    dev: eval_loss = 0.7003884845309787
05/28/2023 13:58:36 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:36 - INFO - __main__ -    dev: infer_time = 3.4987777777777778
05/28/2023 13:58:36 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:36 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:58:36 - INFO - __main__ -     cls_loss = 0.6945155172024743
05/28/2023 13:58:36 - INFO - __main__ -     eval_loss = 0.7003884845309787
05/28/2023 13:58:36 - INFO - __main__ -     global_step = 59
05/28/2023 13:58:36 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:36 - INFO - __main__ -     infer_time = 3.4987777777777778
05/28/2023 13:58:36 - INFO - __main__ -     loss = 0.6945155172024743

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 10.50it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:01, 12.70it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 14.87it/s][A05/28/2023 13:58:36 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:36 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:58:36 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:36 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.48it/s]
05/28/2023 13:58:36 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:36 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:58:36 - INFO - __main__ -    dev: eval_loss = 0.6985259254773458
05/28/2023 13:58:36 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:36 - INFO - __main__ -    dev: infer_time = 3.472
05/28/2023 13:58:36 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:36 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:58:36 - INFO - __main__ -     cls_loss = 0.6952434976895651
05/28/2023 13:58:36 - INFO - __main__ -     eval_loss = 0.6985259254773458
05/28/2023 13:58:36 - INFO - __main__ -     global_step = 69
05/28/2023 13:58:36 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:36 - INFO - __main__ -     infer_time = 3.472
05/28/2023 13:58:36 - INFO - __main__ -     loss = 0.6952434976895651

Iteration:  88%|########8 | 69/78 [00:05<00:00, 15.25it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 17.30it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 19.08it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.56it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.21s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.21s/it]
05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   w_emb: 8057808

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   p_emb: 135168

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   t_emb: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ln_emb: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 127200

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 126984

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   intermediate_numel: 127200

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   output_numel: 127512

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 127200

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 126984

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   intermediate_numel: 127200

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   output_numel: 127512

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 127200

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 126984

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   intermediate_numel: 127200

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   output_numel: 127512

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 127200

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 126984

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   intermediate_numel: 127200

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   output_numel: 127512

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   layer_numel: 2140320
05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   dense_numel: 69960
05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   emb_numel: 8194032

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   encoder_numel: 2140320

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   pooler_numel: 69960

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   all parameters: 10404312

05/28/2023 13:58:37 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:58:37 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
parameter size = 10404312
best_acc = 0.5703971119133574
time_per_batch_infer = 3.446 ms
infer_cnt = 63
**************E*************

05/28/2023 13:58:37 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 504, 'sample_intermediate_sizes': [544, 544, 544], 'sample_qkv_sizes': [504, 504, 504]}
05/28/2023 13:58:37 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:58:37 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:58:37 - INFO - __main__ -   *** Example ***
05/28/2023 13:58:37 - INFO - __main__ -   guid: train-0
05/28/2023 13:58:37 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:58:37 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:37 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:37 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:37 - INFO - __main__ -   label: not_entailment
05/28/2023 13:58:37 - INFO - __main__ -   label_id: 1
05/28/2023 13:58:38 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:58:38 - INFO - __main__ -   *** Example ***
05/28/2023 13:58:38 - INFO - __main__ -   guid: dev-0
05/28/2023 13:58:38 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:58:38 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:38 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:38 - INFO - __main__ -   label: not_entailment
05/28/2023 13:58:38 - INFO - __main__ -   label_id: 1
05/28/2023 13:58:38 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:58:39 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:58:39 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:58:39 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:58:39 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:58:39 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:58:39 - INFO - __main__ -     Batch size = 32
05/28/2023 13:58:39 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.91it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.69it/s][A05/28/2023 13:58:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:39 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:58:39 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.94it/s]
05/28/2023 13:58:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:40 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 13:58:40 - INFO - __main__ -    dev: eval_loss = 0.692161758740743
05/28/2023 13:58:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:40 - INFO - __main__ -    dev: infer_time = 2.8260000000000005
05/28/2023 13:58:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:40 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 13:58:40 - INFO - __main__ -     cls_loss = 0.7043340934647454
05/28/2023 13:58:40 - INFO - __main__ -     eval_loss = 0.692161758740743
05/28/2023 13:58:40 - INFO - __main__ -     global_step = 9
05/28/2023 13:58:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:40 - INFO - __main__ -     infer_time = 2.8260000000000005
05/28/2023 13:58:40 - INFO - __main__ -     loss = 0.7043340934647454
05/28/2023 13:58:40 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.79it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.64it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.76it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.07it/s][A05/28/2023 13:58:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:41 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:58:41 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.79it/s]
05/28/2023 13:58:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:41 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:58:41 - INFO - __main__ -    dev: eval_loss = 0.6967380377981398
05/28/2023 13:58:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:41 - INFO - __main__ -    dev: infer_time = 2.833
05/28/2023 13:58:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:41 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:58:41 - INFO - __main__ -     cls_loss = 0.7045376269440902
05/28/2023 13:58:41 - INFO - __main__ -     eval_loss = 0.6967380377981398
05/28/2023 13:58:41 - INFO - __main__ -     global_step = 19
05/28/2023 13:58:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:41 - INFO - __main__ -     infer_time = 2.833
05/28/2023 13:58:41 - INFO - __main__ -     loss = 0.7045376269440902

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.06it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 13.39it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.54it/s][A05/28/2023 13:58:42 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:42 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:58:42 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:42 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.05it/s]
05/28/2023 13:58:42 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:42 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 13:58:42 - INFO - __main__ -    dev: eval_loss = 0.6921995414627923
05/28/2023 13:58:42 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:42 - INFO - __main__ -    dev: infer_time = 2.817888888888889
05/28/2023 13:58:42 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:42 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 13:58:42 - INFO - __main__ -     cls_loss = 0.7018116363163652
05/28/2023 13:58:42 - INFO - __main__ -     eval_loss = 0.6921995414627923
05/28/2023 13:58:42 - INFO - __main__ -     global_step = 29
05/28/2023 13:58:42 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:42 - INFO - __main__ -     infer_time = 2.817888888888889
05/28/2023 13:58:42 - INFO - __main__ -     loss = 0.7018116363163652
05/28/2023 13:58:42 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:09,  5.04it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:06,  6.64it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:04,  8.49it/s][A05/28/2023 13:58:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:44 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:58:44 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.68it/s]
05/28/2023 13:58:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:44 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:58:44 - INFO - __main__ -    dev: eval_loss = 0.6901786790953742
05/28/2023 13:58:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:44 - INFO - __main__ -    dev: infer_time = 2.852666666666667
05/28/2023 13:58:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:44 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:58:44 - INFO - __main__ -     cls_loss = 0.7000448657916143
05/28/2023 13:58:44 - INFO - __main__ -     eval_loss = 0.6901786790953742
05/28/2023 13:58:44 - INFO - __main__ -     global_step = 39
05/28/2023 13:58:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:44 - INFO - __main__ -     infer_time = 2.852666666666667
05/28/2023 13:58:44 - INFO - __main__ -     loss = 0.7000448657916143

Iteration:  50%|#####     | 39/78 [00:04<00:04,  9.64it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:03, 11.73it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:02, 13.91it/s][A
Iteration:  62%|######1   | 48/78 [00:05<00:01, 15.85it/s][A05/28/2023 13:58:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:44 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:58:44 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.09it/s]
05/28/2023 13:58:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:44 - INFO - __main__ -    dev: acc = 0.5703971119133574
05/28/2023 13:58:44 - INFO - __main__ -    dev: eval_loss = 0.6917033526632521
05/28/2023 13:58:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:44 - INFO - __main__ -    dev: infer_time = 2.816222222222222
05/28/2023 13:58:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:44 - INFO - __main__ -     acc = 0.5703971119133574
05/28/2023 13:58:44 - INFO - __main__ -     cls_loss = 0.6981755154473441
05/28/2023 13:58:44 - INFO - __main__ -     eval_loss = 0.6917033526632521
05/28/2023 13:58:44 - INFO - __main__ -     global_step = 49
05/28/2023 13:58:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:44 - INFO - __main__ -     infer_time = 2.816222222222222
05/28/2023 13:58:44 - INFO - __main__ -     loss = 0.6981755154473441
05/28/2023 13:58:44 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  65%|######5   | 51/78 [00:06<00:05,  5.16it/s][A
Iteration:  69%|######9   | 54/78 [00:06<00:03,  6.75it/s][A
Iteration:  73%|#######3  | 57/78 [00:06<00:02,  8.59it/s][A05/28/2023 13:58:46 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:46 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:58:46 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:46 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.97it/s]
05/28/2023 13:58:46 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:46 - INFO - __main__ -    dev: acc = 0.5018050541516246
05/28/2023 13:58:46 - INFO - __main__ -    dev: eval_loss = 0.6926617158783807
05/28/2023 13:58:46 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:46 - INFO - __main__ -    dev: infer_time = 2.8101111111111114
05/28/2023 13:58:46 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:46 - INFO - __main__ -     acc = 0.5018050541516246
05/28/2023 13:58:46 - INFO - __main__ -     cls_loss = 0.6972419255870884
05/28/2023 13:58:46 - INFO - __main__ -     eval_loss = 0.6926617158783807
05/28/2023 13:58:46 - INFO - __main__ -     global_step = 59
05/28/2023 13:58:46 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:46 - INFO - __main__ -     infer_time = 2.8101111111111114
05/28/2023 13:58:46 - INFO - __main__ -     loss = 0.6972419255870884

Iteration:  76%|#######5  | 59/78 [00:06<00:02,  9.18it/s][A
Iteration:  79%|#######9  | 62/78 [00:07<00:01, 11.44it/s][A
Iteration:  83%|########3 | 65/78 [00:07<00:00, 13.74it/s][A
Iteration:  87%|########7 | 68/78 [00:07<00:00, 15.79it/s][A05/28/2023 13:58:46 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:46 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:58:46 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:46 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.09it/s]
05/28/2023 13:58:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:47 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 13:58:47 - INFO - __main__ -    dev: eval_loss = 0.6933047241634793
05/28/2023 13:58:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:47 - INFO - __main__ -    dev: infer_time = 2.806333333333333
05/28/2023 13:58:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:47 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 13:58:47 - INFO - __main__ -     cls_loss = 0.6964563640995302
05/28/2023 13:58:47 - INFO - __main__ -     eval_loss = 0.6933047241634793
05/28/2023 13:58:47 - INFO - __main__ -     global_step = 69
05/28/2023 13:58:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:47 - INFO - __main__ -     infer_time = 2.806333333333333
05/28/2023 13:58:47 - INFO - __main__ -     loss = 0.6964563640995302

Iteration:  91%|#########1| 71/78 [00:07<00:00, 15.42it/s][A
Iteration:  95%|#########4| 74/78 [00:07<00:00, 17.21it/s][A
Iteration:  99%|#########8| 77/78 [00:07<00:00, 18.86it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.03it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.78s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.78s/it]
05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   w_emb: 15383088

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   p_emb: 258048

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   t_emb: 1008

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   ln_emb: 1008

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   query_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   key_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   value_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   ln_numel: 1008

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   self_numel: 763560

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   output_numel: 255528

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 274720

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 274680

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   ln_numel: 1008

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   attention_numel: 1019088

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   intermediate_numel: 274720

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   output_numel: 275688

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   query_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   key_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   value_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   ln_numel: 1008

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   self_numel: 763560

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   output_numel: 255528

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 274720

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 274680

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   ln_numel: 1008

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   attention_numel: 1019088

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   intermediate_numel: 274720

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   output_numel: 275688

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   query_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   key_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   value_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   ln_numel: 1008

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   self_numel: 763560

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   output_numel: 255528

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 274720

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 274680

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   ln_numel: 1008

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   attention_numel: 1019088

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   intermediate_numel: 274720

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   output_numel: 275688

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   layer_numel: 4708488
05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   dense_numel: 254520
05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   emb_numel: 15643152

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   encoder_numel: 4708488

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   pooler_numel: 254520

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   all parameters: 20606160

05/28/2023 13:58:47 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:58:47 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 504, 'sample_intermediate_sizes': [544, 544, 544], 'sample_qkv_sizes': [504, 504, 504]}
parameter size = 20606160
best_acc = 0.5703971119133574
time_per_batch_infer = 2.823 ms
infer_cnt = 63
**************E*************

05/28/2023 13:58:47 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
05/28/2023 13:58:47 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:58:47 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:58:47 - INFO - __main__ -   *** Example ***
05/28/2023 13:58:47 - INFO - __main__ -   guid: train-0
05/28/2023 13:58:47 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:58:47 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:47 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:47 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:47 - INFO - __main__ -   label: not_entailment
05/28/2023 13:58:47 - INFO - __main__ -   label_id: 1
05/28/2023 13:58:49 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:58:49 - INFO - __main__ -   *** Example ***
05/28/2023 13:58:49 - INFO - __main__ -   guid: dev-0
05/28/2023 13:58:49 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:58:49 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:49 - INFO - __main__ -   label: not_entailment
05/28/2023 13:58:49 - INFO - __main__ -   label_id: 1
05/28/2023 13:58:49 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:58:49 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:58:49 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:58:49 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:58:49 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:58:49 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:58:49 - INFO - __main__ -     Batch size = 32
05/28/2023 13:58:49 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.68it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.43it/s][A05/28/2023 13:58:50 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:50 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:58:50 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:50 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.92it/s]
05/28/2023 13:58:50 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:50 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:58:50 - INFO - __main__ -    dev: eval_loss = 0.7073996596866183
05/28/2023 13:58:50 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:50 - INFO - __main__ -    dev: infer_time = 3.4290000000000003
05/28/2023 13:58:50 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:50 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:58:50 - INFO - __main__ -     cls_loss = 0.6887362466918098
05/28/2023 13:58:50 - INFO - __main__ -     eval_loss = 0.7073996596866183
05/28/2023 13:58:50 - INFO - __main__ -     global_step = 9
05/28/2023 13:58:50 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:50 - INFO - __main__ -     infer_time = 3.4290000000000003
05/28/2023 13:58:50 - INFO - __main__ -     loss = 0.6887362466918098
05/28/2023 13:58:50 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.75it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.60it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.77it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.14it/s][A05/28/2023 13:58:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:52 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:58:52 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.81it/s]
05/28/2023 13:58:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:52 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:58:52 - INFO - __main__ -    dev: eval_loss = 0.6990913218922086
05/28/2023 13:58:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:52 - INFO - __main__ -    dev: infer_time = 3.3914444444444443
05/28/2023 13:58:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:52 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:58:52 - INFO - __main__ -     cls_loss = 0.6908063417986819
05/28/2023 13:58:52 - INFO - __main__ -     eval_loss = 0.6990913218922086
05/28/2023 13:58:52 - INFO - __main__ -     global_step = 19
05/28/2023 13:58:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:52 - INFO - __main__ -     infer_time = 3.3914444444444443
05/28/2023 13:58:52 - INFO - __main__ -     loss = 0.6908063417986819

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.47it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.81it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.07it/s][A05/28/2023 13:58:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:52 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:58:52 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.43it/s]
05/28/2023 13:58:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:52 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:58:52 - INFO - __main__ -    dev: eval_loss = 0.690214667055342
05/28/2023 13:58:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:52 - INFO - __main__ -    dev: infer_time = 3.421555555555555
05/28/2023 13:58:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:52 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:58:52 - INFO - __main__ -     cls_loss = 0.6926488876342773
05/28/2023 13:58:52 - INFO - __main__ -     eval_loss = 0.690214667055342
05/28/2023 13:58:52 - INFO - __main__ -     global_step = 29
05/28/2023 13:58:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:52 - INFO - __main__ -     infer_time = 3.421555555555555
05/28/2023 13:58:52 - INFO - __main__ -     loss = 0.6926488876342773
05/28/2023 13:58:52 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:09,  5.03it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:06,  6.61it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:04,  8.51it/s][A05/28/2023 13:58:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:54 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:58:54 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 155.08it/s]
05/28/2023 13:58:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:54 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 13:58:54 - INFO - __main__ -    dev: eval_loss = 0.6901907126108805
05/28/2023 13:58:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:54 - INFO - __main__ -    dev: infer_time = 3.3934444444444454
05/28/2023 13:58:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:54 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 13:58:54 - INFO - __main__ -     cls_loss = 0.6939311073376582
05/28/2023 13:58:54 - INFO - __main__ -     eval_loss = 0.6901907126108805
05/28/2023 13:58:54 - INFO - __main__ -     global_step = 39
05/28/2023 13:58:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:54 - INFO - __main__ -     infer_time = 3.3934444444444454
05/28/2023 13:58:54 - INFO - __main__ -     loss = 0.6939311073376582

Iteration:  50%|#####     | 39/78 [00:04<00:03,  9.93it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:03, 11.95it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:02, 14.10it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 16.20it/s][A05/28/2023 13:58:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:54 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:58:54 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.81it/s]
05/28/2023 13:58:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:54 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 13:58:54 - INFO - __main__ -    dev: eval_loss = 0.6921187904145982
05/28/2023 13:58:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:54 - INFO - __main__ -    dev: infer_time = 3.397111111111111
05/28/2023 13:58:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:54 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 13:58:54 - INFO - __main__ -     cls_loss = 0.694480760973327
05/28/2023 13:58:54 - INFO - __main__ -     eval_loss = 0.6921187904145982
05/28/2023 13:58:54 - INFO - __main__ -     global_step = 49
05/28/2023 13:58:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:54 - INFO - __main__ -     infer_time = 3.397111111111111
05/28/2023 13:58:54 - INFO - __main__ -     loss = 0.694480760973327
05/28/2023 13:58:54 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  65%|######5   | 51/78 [00:06<00:05,  5.20it/s][A
Iteration:  69%|######9   | 54/78 [00:06<00:03,  6.80it/s][A
Iteration:  73%|#######3  | 57/78 [00:06<00:02,  8.69it/s][A05/28/2023 13:58:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:56 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:58:56 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.80it/s]
05/28/2023 13:58:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:56 - INFO - __main__ -    dev: acc = 0.5595667870036101
05/28/2023 13:58:56 - INFO - __main__ -    dev: eval_loss = 0.6902778413560655
05/28/2023 13:58:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:56 - INFO - __main__ -    dev: infer_time = 3.4069999999999996
05/28/2023 13:58:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:56 - INFO - __main__ -     acc = 0.5595667870036101
05/28/2023 13:58:56 - INFO - __main__ -     cls_loss = 0.694014045141511
05/28/2023 13:58:56 - INFO - __main__ -     eval_loss = 0.6902778413560655
05/28/2023 13:58:56 - INFO - __main__ -     global_step = 59
05/28/2023 13:58:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:56 - INFO - __main__ -     infer_time = 3.4069999999999996
05/28/2023 13:58:56 - INFO - __main__ -     loss = 0.694014045141511
05/28/2023 13:58:56 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  77%|#######6  | 60/78 [00:08<00:04,  4.37it/s][A
Iteration:  81%|########  | 63/78 [00:08<00:02,  5.80it/s][A
Iteration:  85%|########4 | 66/78 [00:08<00:01,  7.54it/s][A05/28/2023 13:58:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:58:58 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:58:58 - INFO - __main__ -     Num examples = 277
05/28/2023 13:58:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.53it/s]
05/28/2023 13:58:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:58:58 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 13:58:58 - INFO - __main__ -    dev: eval_loss = 0.689722650580936
05/28/2023 13:58:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:58:58 - INFO - __main__ -    dev: infer_time = 3.4082222222222223
05/28/2023 13:58:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:58:58 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 13:58:58 - INFO - __main__ -     cls_loss = 0.6933894019195999
05/28/2023 13:58:58 - INFO - __main__ -     eval_loss = 0.689722650580936
05/28/2023 13:58:58 - INFO - __main__ -     global_step = 69
05/28/2023 13:58:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:58:58 - INFO - __main__ -     infer_time = 3.4082222222222223
05/28/2023 13:58:58 - INFO - __main__ -     loss = 0.6933894019195999

Iteration:  88%|########8 | 69/78 [00:08<00:01,  8.97it/s][A
Iteration:  92%|#########2| 72/78 [00:08<00:00, 11.04it/s][A
Iteration:  96%|#########6| 75/78 [00:08<00:00, 13.25it/s][AIteration: 100%|##########| 78/78 [00:08<00:00,  8.70it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.96s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.96s/it]
05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   w_emb: 8057808

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   p_emb: 135168

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   t_emb: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ln_emb: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 135680

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 135432

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   intermediate_numel: 135680

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   output_numel: 135960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 135680

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 135432

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   intermediate_numel: 135680

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   output_numel: 135960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 135680

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 135432

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   intermediate_numel: 135680

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   output_numel: 135960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 135680

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 135432

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   intermediate_numel: 135680

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   output_numel: 135960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   layer_numel: 2208032
05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   dense_numel: 69960
05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   emb_numel: 8194032

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   encoder_numel: 2208032

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   pooler_numel: 69960

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   all parameters: 10472024

05/28/2023 13:58:58 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:58:58 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
parameter size = 10472024
best_acc = 0.5595667870036101
time_per_batch_infer = 3.407 ms
infer_cnt = 63
**************E*************

05/28/2023 13:58:58 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
05/28/2023 13:58:58 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:58:58 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:58:58 - INFO - __main__ -   *** Example ***
05/28/2023 13:58:58 - INFO - __main__ -   guid: train-0
05/28/2023 13:58:58 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:58:58 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:58:58 - INFO - __main__ -   label: not_entailment
05/28/2023 13:58:58 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:00 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:59:00 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:00 - INFO - __main__ -   guid: dev-0
05/28/2023 13:59:00 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:59:00 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:00 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:00 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:00 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:59:00 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:59:01 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:59:01 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:59:01 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:59:01 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:59:01 - INFO - __main__ -     Batch size = 32
05/28/2023 13:59:01 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.57it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.16it/s][A05/28/2023 13:59:01 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:01 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:59:01 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:01 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.24it/s]
05/28/2023 13:59:01 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:01 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 13:59:01 - INFO - __main__ -    dev: eval_loss = 0.6919728782441881
05/28/2023 13:59:01 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:01 - INFO - __main__ -    dev: infer_time = 3.4352222222222224
05/28/2023 13:59:01 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:01 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 13:59:01 - INFO - __main__ -     cls_loss = 0.6973096794552274
05/28/2023 13:59:01 - INFO - __main__ -     eval_loss = 0.6919728782441881
05/28/2023 13:59:01 - INFO - __main__ -     global_step = 9
05/28/2023 13:59:01 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:01 - INFO - __main__ -     infer_time = 3.4352222222222224
05/28/2023 13:59:01 - INFO - __main__ -     loss = 0.6973096794552274
05/28/2023 13:59:01 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.88it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.72it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.80it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.02it/s][A05/28/2023 13:59:03 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:03 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:59:03 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:03 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.00it/s]
05/28/2023 13:59:03 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:03 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:03 - INFO - __main__ -    dev: eval_loss = 0.6945826146337721
05/28/2023 13:59:03 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:03 - INFO - __main__ -    dev: infer_time = 3.4874444444444443
05/28/2023 13:59:03 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:03 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:03 - INFO - __main__ -     cls_loss = 0.6966005344139902
05/28/2023 13:59:03 - INFO - __main__ -     eval_loss = 0.6945826146337721
05/28/2023 13:59:03 - INFO - __main__ -     global_step = 19
05/28/2023 13:59:03 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:03 - INFO - __main__ -     infer_time = 3.4874444444444443
05/28/2023 13:59:03 - INFO - __main__ -     loss = 0.6966005344139902
05/28/2023 13:59:03 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:14,  3.93it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.47it/s][A
Iteration:  33%|###3      | 26/78 [00:03<00:07,  7.29it/s][A05/28/2023 13:59:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:05 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:59:05 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.27it/s]
05/28/2023 13:59:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:05 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:05 - INFO - __main__ -    dev: eval_loss = 0.7100029256608751
05/28/2023 13:59:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:05 - INFO - __main__ -    dev: infer_time = 3.453333333333334
05/28/2023 13:59:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:05 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:05 - INFO - __main__ -     cls_loss = 0.697388914124719
05/28/2023 13:59:05 - INFO - __main__ -     eval_loss = 0.7100029256608751
05/28/2023 13:59:05 - INFO - __main__ -     global_step = 29
05/28/2023 13:59:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:05 - INFO - __main__ -     infer_time = 3.453333333333334
05/28/2023 13:59:05 - INFO - __main__ -     loss = 0.697388914124719

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.63it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.68it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.76it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.70it/s][A05/28/2023 13:59:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:06 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:59:06 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.21it/s]
05/28/2023 13:59:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:06 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 13:59:06 - INFO - __main__ -    dev: eval_loss = 0.6922455959849887
05/28/2023 13:59:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:06 - INFO - __main__ -    dev: infer_time = 3.4502222222222225
05/28/2023 13:59:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:06 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 13:59:06 - INFO - __main__ -     cls_loss = 0.698183781061417
05/28/2023 13:59:06 - INFO - __main__ -     eval_loss = 0.6922455959849887
05/28/2023 13:59:06 - INFO - __main__ -     global_step = 39
05/28/2023 13:59:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:06 - INFO - __main__ -     infer_time = 3.4502222222222225
05/28/2023 13:59:06 - INFO - __main__ -     loss = 0.698183781061417
05/28/2023 13:59:06 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  53%|#####2    | 41/78 [00:06<00:07,  4.97it/s][A
Iteration:  56%|#####6    | 44/78 [00:06<00:05,  6.48it/s][A
Iteration:  60%|######    | 47/78 [00:06<00:03,  8.26it/s][A05/28/2023 13:59:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:07 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:59:07 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 129.80it/s]
05/28/2023 13:59:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:07 - INFO - __main__ -    dev: eval_loss = 0.6924903127882216
05/28/2023 13:59:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:07 - INFO - __main__ -    dev: infer_time = 3.5054444444444446
05/28/2023 13:59:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:07 - INFO - __main__ -     cls_loss = 0.6974629115085212
05/28/2023 13:59:07 - INFO - __main__ -     eval_loss = 0.6924903127882216
05/28/2023 13:59:07 - INFO - __main__ -     global_step = 49
05/28/2023 13:59:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:07 - INFO - __main__ -     infer_time = 3.5054444444444446
05/28/2023 13:59:07 - INFO - __main__ -     loss = 0.6974629115085212

Iteration:  63%|######2   | 49/78 [00:06<00:03,  8.85it/s][A
Iteration:  67%|######6   | 52/78 [00:06<00:02, 10.97it/s][A
Iteration:  71%|#######   | 55/78 [00:06<00:01, 13.07it/s][A
Iteration:  74%|#######4  | 58/78 [00:06<00:01, 15.06it/s][A05/28/2023 13:59:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:08 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:59:08 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.40it/s]
05/28/2023 13:59:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:08 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:08 - INFO - __main__ -    dev: eval_loss = 0.6913886533843147
05/28/2023 13:59:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:08 - INFO - __main__ -    dev: infer_time = 3.4579999999999997
05/28/2023 13:59:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:08 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:08 - INFO - __main__ -     cls_loss = 0.6975954003253225
05/28/2023 13:59:08 - INFO - __main__ -     eval_loss = 0.6913886533843147
05/28/2023 13:59:08 - INFO - __main__ -     global_step = 59
05/28/2023 13:59:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:08 - INFO - __main__ -     infer_time = 3.4579999999999997
05/28/2023 13:59:08 - INFO - __main__ -     loss = 0.6975954003253225

Iteration:  78%|#######8  | 61/78 [00:07<00:01, 14.81it/s][A
Iteration:  82%|########2 | 64/78 [00:07<00:00, 16.56it/s][A
Iteration:  86%|########5 | 67/78 [00:07<00:00, 18.00it/s][A05/28/2023 13:59:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:08 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:59:08 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.25it/s]
05/28/2023 13:59:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:08 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:08 - INFO - __main__ -    dev: eval_loss = 0.6922488411267599
05/28/2023 13:59:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:08 - INFO - __main__ -    dev: infer_time = 3.4515555555555557
05/28/2023 13:59:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:08 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:08 - INFO - __main__ -     cls_loss = 0.6973551131676936
05/28/2023 13:59:08 - INFO - __main__ -     eval_loss = 0.6922488411267599
05/28/2023 13:59:08 - INFO - __main__ -     global_step = 69
05/28/2023 13:59:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:08 - INFO - __main__ -     infer_time = 3.4515555555555557
05/28/2023 13:59:08 - INFO - __main__ -     loss = 0.6973551131676936

Iteration:  90%|########9 | 70/78 [00:07<00:00, 16.67it/s][A
Iteration:  94%|#########3| 73/78 [00:07<00:00, 18.13it/s][A
Iteration:  97%|#########7| 76/78 [00:07<00:00, 19.24it/s][AIteration: 100%|##########| 78/78 [00:07<00:00,  9.84it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.93s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.93s/it]
05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   w_emb: 9889128

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   p_emb: 165888

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   t_emb: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ln_emb: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 187200

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 186948

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 187200

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   output_numel: 187596

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 187200

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 186948

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 187200

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   output_numel: 187596

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 187200

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 186948

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 187200

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   output_numel: 187596

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 187200

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 186948

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 187200

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   output_numel: 187596

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   layer_numel: 3186576
05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   dense_numel: 105300
05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   emb_numel: 10056312

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   encoder_numel: 3186576

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   pooler_numel: 105300

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   all parameters: 13348188

05/28/2023 13:59:09 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:09 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
parameter size = 13348188
best_acc = 0.5342960288808665
time_per_batch_infer = 3.463 ms
infer_cnt = 63
**************E*************

05/28/2023 13:59:09 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 204, 'sample_intermediate_sizes': [1024, 1024, 1024, 1024], 'sample_qkv_sizes': [204, 204, 204, 204]}
05/28/2023 13:59:09 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:59:09 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:59:09 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:09 - INFO - __main__ -   guid: train-0
05/28/2023 13:59:09 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:59:09 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:09 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:09 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:09 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:11 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:59:11 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:11 - INFO - __main__ -   guid: dev-0
05/28/2023 13:59:11 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:59:11 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:11 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:11 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:11 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:11 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:11 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:59:11 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:59:11 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:59:11 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:59:11 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:59:11 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:59:11 - INFO - __main__ -     Batch size = 32
05/28/2023 13:59:11 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.90it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.50it/s][A05/28/2023 13:59:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:12 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:59:12 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.40it/s]
05/28/2023 13:59:12 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:12 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:12 - INFO - __main__ -    dev: eval_loss = 0.6919865674442716
05/28/2023 13:59:12 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:12 - INFO - __main__ -    dev: infer_time = 3.4392222222222224
05/28/2023 13:59:12 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:12 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:12 - INFO - __main__ -     cls_loss = 0.6933332218064202
05/28/2023 13:59:12 - INFO - __main__ -     eval_loss = 0.6919865674442716
05/28/2023 13:59:12 - INFO - __main__ -     global_step = 9
05/28/2023 13:59:12 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:12 - INFO - __main__ -     infer_time = 3.4392222222222224
05/28/2023 13:59:12 - INFO - __main__ -     loss = 0.6933332218064202
05/28/2023 13:59:12 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.00it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.95it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.20it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.62it/s][A05/28/2023 13:59:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:13 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:59:13 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.36it/s]
05/28/2023 13:59:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:14 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:14 - INFO - __main__ -    dev: eval_loss = 0.6915019088321261
05/28/2023 13:59:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:14 - INFO - __main__ -    dev: infer_time = 3.4527777777777775
05/28/2023 13:59:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:14 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:14 - INFO - __main__ -     cls_loss = 0.6936357272298712
05/28/2023 13:59:14 - INFO - __main__ -     eval_loss = 0.6915019088321261
05/28/2023 13:59:14 - INFO - __main__ -     global_step = 19
05/28/2023 13:59:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:14 - INFO - __main__ -     infer_time = 3.4527777777777775
05/28/2023 13:59:14 - INFO - __main__ -     loss = 0.6936357272298712

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.84it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.24it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.46it/s][A05/28/2023 13:59:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:14 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:59:14 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.80it/s]
05/28/2023 13:59:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:14 - INFO - __main__ -    dev: acc = 0.516245487364621
05/28/2023 13:59:14 - INFO - __main__ -    dev: eval_loss = 0.6924304829703437
05/28/2023 13:59:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:14 - INFO - __main__ -    dev: infer_time = 3.4306666666666663
05/28/2023 13:59:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:14 - INFO - __main__ -     acc = 0.516245487364621
05/28/2023 13:59:14 - INFO - __main__ -     cls_loss = 0.6932388708509248
05/28/2023 13:59:14 - INFO - __main__ -     eval_loss = 0.6924304829703437
05/28/2023 13:59:14 - INFO - __main__ -     global_step = 29
05/28/2023 13:59:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:14 - INFO - __main__ -     infer_time = 3.4306666666666663
05/28/2023 13:59:14 - INFO - __main__ -     loss = 0.6932388708509248

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.24it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 18.13it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 19.72it/s][A05/28/2023 13:59:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:14 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:59:14 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.82it/s]
05/28/2023 13:59:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:14 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 13:59:14 - INFO - __main__ -    dev: eval_loss = 0.6917030811309814
05/28/2023 13:59:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:14 - INFO - __main__ -    dev: infer_time = 3.437777777777778
05/28/2023 13:59:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:14 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 13:59:14 - INFO - __main__ -     cls_loss = 0.6930433679849674
05/28/2023 13:59:14 - INFO - __main__ -     eval_loss = 0.6917030811309814
05/28/2023 13:59:14 - INFO - __main__ -     global_step = 39
05/28/2023 13:59:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:14 - INFO - __main__ -     infer_time = 3.437777777777778
05/28/2023 13:59:14 - INFO - __main__ -     loss = 0.6930433679849674
05/28/2023 13:59:15 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:04<00:07,  5.43it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:05,  7.05it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:03,  8.99it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:02, 11.13it/s][A05/28/2023 13:59:16 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:16 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:59:16 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:16 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.45it/s]
05/28/2023 13:59:16 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:16 - INFO - __main__ -    dev: acc = 0.48375451263537905
05/28/2023 13:59:16 - INFO - __main__ -    dev: eval_loss = 0.6936242116822137
05/28/2023 13:59:16 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:16 - INFO - __main__ -    dev: infer_time = 3.4247777777777784
05/28/2023 13:59:16 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:16 - INFO - __main__ -     acc = 0.48375451263537905
05/28/2023 13:59:16 - INFO - __main__ -     cls_loss = 0.6935594921209374
05/28/2023 13:59:16 - INFO - __main__ -     eval_loss = 0.6936242116822137
05/28/2023 13:59:16 - INFO - __main__ -     global_step = 49
05/28/2023 13:59:16 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:16 - INFO - __main__ -     infer_time = 3.4247777777777784
05/28/2023 13:59:16 - INFO - __main__ -     loss = 0.6935594921209374

Iteration:  65%|######5   | 51/78 [00:05<00:02, 12.20it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 14.39it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 16.46it/s][A05/28/2023 13:59:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:17 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:59:17 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.68it/s]
05/28/2023 13:59:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:17 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:17 - INFO - __main__ -    dev: eval_loss = 0.6984231869379679
05/28/2023 13:59:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:17 - INFO - __main__ -    dev: infer_time = 3.4648888888888885
05/28/2023 13:59:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:17 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:17 - INFO - __main__ -     cls_loss = 0.692972047854278
05/28/2023 13:59:17 - INFO - __main__ -     eval_loss = 0.6984231869379679
05/28/2023 13:59:17 - INFO - __main__ -     global_step = 59
05/28/2023 13:59:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:17 - INFO - __main__ -     infer_time = 3.4648888888888885
05/28/2023 13:59:17 - INFO - __main__ -     loss = 0.692972047854278

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 16.23it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 18.10it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 19.72it/s][A05/28/2023 13:59:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:17 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:59:17 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.69it/s]
05/28/2023 13:59:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:17 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:17 - INFO - __main__ -    dev: eval_loss = 0.697233670287662
05/28/2023 13:59:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:17 - INFO - __main__ -    dev: infer_time = 3.433888888888889
05/28/2023 13:59:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:17 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:17 - INFO - __main__ -     cls_loss = 0.6934305438096973
05/28/2023 13:59:17 - INFO - __main__ -     eval_loss = 0.697233670287662
05/28/2023 13:59:17 - INFO - __main__ -     global_step = 69
05/28/2023 13:59:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:17 - INFO - __main__ -     infer_time = 3.433888888888889
05/28/2023 13:59:17 - INFO - __main__ -     loss = 0.6934305438096973

Iteration:  88%|########8 | 69/78 [00:05<00:00, 18.38it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 19.79it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 20.61it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.55it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.22s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.22s/it]
05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   w_emb: 6226488

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   p_emb: 104448

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   t_emb: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ln_emb: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 209920

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 209100

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   intermediate_numel: 209920

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   output_numel: 209508

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 209920

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 209100

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   intermediate_numel: 209920

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   output_numel: 209508

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 209920

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 209100

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   intermediate_numel: 209920

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   output_numel: 209508

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 209920

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 209100

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   intermediate_numel: 209920

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   output_numel: 209508

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   layer_numel: 2348464
05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   dense_numel: 41820
05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   emb_numel: 6331752

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   encoder_numel: 2348464

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   pooler_numel: 41820

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   all parameters: 8722036

05/28/2023 13:59:18 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:18 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 204, 'sample_intermediate_sizes': [1024, 1024, 1024, 1024], 'sample_qkv_sizes': [204, 204, 204, 204]}
parameter size = 8722036
best_acc = 0.5306859205776173
time_per_batch_infer = 3.441 ms
infer_cnt = 63
**************E*************

05/28/2023 13:59:18 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [544, 544, 544, 544], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
05/28/2023 13:59:18 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:59:18 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:59:18 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:18 - INFO - __main__ -   guid: train-0
05/28/2023 13:59:18 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:59:18 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:18 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:18 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:19 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:59:19 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:19 - INFO - __main__ -   guid: dev-0
05/28/2023 13:59:19 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:59:19 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:19 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:19 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:19 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:19 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:59:19 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:59:20 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:59:20 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:59:20 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:59:20 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:59:20 - INFO - __main__ -     Batch size = 32
05/28/2023 13:59:20 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.61it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.07it/s][A05/28/2023 13:59:20 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:20 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:59:20 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:20 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 152.93it/s]
05/28/2023 13:59:20 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:20 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:20 - INFO - __main__ -    dev: eval_loss = 0.694586435953776
05/28/2023 13:59:20 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:20 - INFO - __main__ -    dev: infer_time = 3.4101111111111115
05/28/2023 13:59:20 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:20 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:20 - INFO - __main__ -     cls_loss = 0.6985635956128439
05/28/2023 13:59:20 - INFO - __main__ -     eval_loss = 0.694586435953776
05/28/2023 13:59:20 - INFO - __main__ -     global_step = 9
05/28/2023 13:59:20 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:20 - INFO - __main__ -     infer_time = 3.4101111111111115
05/28/2023 13:59:20 - INFO - __main__ -     loss = 0.6985635956128439
05/28/2023 13:59:20 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.68it/s][A
Iteration:  15%|#5        | 12/78 [00:02<00:12,  5.49it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.63it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.98it/s][A05/28/2023 13:59:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:22 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:59:22 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 152.79it/s]
05/28/2023 13:59:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:22 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 13:59:22 - INFO - __main__ -    dev: eval_loss = 0.6917378769980537
05/28/2023 13:59:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:22 - INFO - __main__ -    dev: infer_time = 3.4498888888888883
05/28/2023 13:59:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:22 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 13:59:22 - INFO - __main__ -     cls_loss = 0.6970614853658175
05/28/2023 13:59:22 - INFO - __main__ -     eval_loss = 0.6917378769980537
05/28/2023 13:59:22 - INFO - __main__ -     global_step = 19
05/28/2023 13:59:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:22 - INFO - __main__ -     infer_time = 3.4498888888888883
05/28/2023 13:59:22 - INFO - __main__ -     loss = 0.6970614853658175
05/28/2023 13:59:22 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.36it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.89it/s][A
Iteration:  35%|###4      | 27/78 [00:04<00:06,  7.73it/s][A05/28/2023 13:59:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:24 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:59:24 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 152.82it/s]
05/28/2023 13:59:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:24 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:24 - INFO - __main__ -    dev: eval_loss = 0.6978140672047933
05/28/2023 13:59:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:24 - INFO - __main__ -    dev: infer_time = 3.4400000000000004
05/28/2023 13:59:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:24 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:24 - INFO - __main__ -     cls_loss = 0.6961971911890753
05/28/2023 13:59:24 - INFO - __main__ -     eval_loss = 0.6978140672047933
05/28/2023 13:59:24 - INFO - __main__ -     global_step = 29
05/28/2023 13:59:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:24 - INFO - __main__ -     infer_time = 3.4400000000000004
05/28/2023 13:59:24 - INFO - __main__ -     loss = 0.6961971911890753

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.57it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.89it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 13.27it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 15.53it/s][A05/28/2023 13:59:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:25 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:59:25 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 152.82it/s]
05/28/2023 13:59:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:25 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:25 - INFO - __main__ -    dev: eval_loss = 0.6939094728893704
05/28/2023 13:59:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:25 - INFO - __main__ -    dev: infer_time = 3.4218888888888888
05/28/2023 13:59:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:25 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:25 - INFO - __main__ -     cls_loss = 0.695829131664374
05/28/2023 13:59:25 - INFO - __main__ -     eval_loss = 0.6939094728893704
05/28/2023 13:59:25 - INFO - __main__ -     global_step = 39
05/28/2023 13:59:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:25 - INFO - __main__ -     infer_time = 3.4218888888888888
05/28/2023 13:59:25 - INFO - __main__ -     loss = 0.695829131664374

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 15.57it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:01, 17.57it/s][A
Iteration:  60%|######    | 47/78 [00:04<00:01, 19.27it/s][A05/28/2023 13:59:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:25 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:59:25 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 152.90it/s]
05/28/2023 13:59:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:25 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:25 - INFO - __main__ -    dev: eval_loss = 0.6916298535135057
05/28/2023 13:59:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:25 - INFO - __main__ -    dev: infer_time = 3.4255555555555555
05/28/2023 13:59:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:25 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:25 - INFO - __main__ -     cls_loss = 0.6953630629850893
05/28/2023 13:59:25 - INFO - __main__ -     eval_loss = 0.6916298535135057
05/28/2023 13:59:25 - INFO - __main__ -     global_step = 49
05/28/2023 13:59:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:25 - INFO - __main__ -     infer_time = 3.4255555555555555
05/28/2023 13:59:25 - INFO - __main__ -     loss = 0.6953630629850893

Iteration:  64%|######4   | 50/78 [00:05<00:01, 18.06it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 19.64it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 20.51it/s][A05/28/2023 13:59:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:26 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:59:26 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 153.13it/s]
05/28/2023 13:59:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:26 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 13:59:26 - INFO - __main__ -    dev: eval_loss = 0.6922896305720011
05/28/2023 13:59:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:26 - INFO - __main__ -    dev: infer_time = 3.4588888888888887
05/28/2023 13:59:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:26 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 13:59:26 - INFO - __main__ -     cls_loss = 0.6949693520190352
05/28/2023 13:59:26 - INFO - __main__ -     eval_loss = 0.6922896305720011
05/28/2023 13:59:26 - INFO - __main__ -     global_step = 59
05/28/2023 13:59:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:26 - INFO - __main__ -     infer_time = 3.4588888888888887
05/28/2023 13:59:26 - INFO - __main__ -     loss = 0.6949693520190352

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 18.78it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:00, 19.93it/s][A
Iteration:  83%|########3 | 65/78 [00:05<00:00, 21.08it/s][A
Iteration:  87%|########7 | 68/78 [00:05<00:00, 22.04it/s][A05/28/2023 13:59:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:26 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:59:26 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 152.86it/s]
05/28/2023 13:59:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:26 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 13:59:26 - INFO - __main__ -    dev: eval_loss = 0.6925008495648702
05/28/2023 13:59:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:26 - INFO - __main__ -    dev: infer_time = 3.452
05/28/2023 13:59:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:26 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 13:59:26 - INFO - __main__ -     cls_loss = 0.6943494833034017
05/28/2023 13:59:26 - INFO - __main__ -     eval_loss = 0.6925008495648702
05/28/2023 13:59:26 - INFO - __main__ -     global_step = 69
05/28/2023 13:59:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:26 - INFO - __main__ -     infer_time = 3.452
05/28/2023 13:59:26 - INFO - __main__ -     loss = 0.6943494833034017

Iteration:  91%|#########1| 71/78 [00:06<00:00, 19.62it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 20.92it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 21.96it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.19it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.40s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.40s/it]
05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   w_emb: 8424072

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   p_emb: 141312

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   t_emb: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ln_emb: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 150688

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 150420

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   intermediate_numel: 150688

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   output_numel: 150972

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 150688

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 150420

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   intermediate_numel: 150688

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   output_numel: 150972

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 150688

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 150420

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   intermediate_numel: 150688

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   output_numel: 150972

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 150688

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 150420

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   intermediate_numel: 150688

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   output_numel: 150972

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   layer_numel: 2432080
05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   dense_numel: 76452
05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   emb_numel: 8566488

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   encoder_numel: 2432080

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   pooler_numel: 76452

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   all parameters: 11075020

05/28/2023 13:59:26 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:26 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [544, 544, 544, 544], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
parameter size = 11075020
best_acc = 0.5306859205776173
time_per_batch_infer = 3.437 ms
infer_cnt = 63
**************E*************

05/28/2023 13:59:26 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 420, 'sample_intermediate_sizes': [768, 768, 768], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [420, 420, 420]}
05/28/2023 13:59:26 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:59:26 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:59:26 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:26 - INFO - __main__ -   guid: train-0
05/28/2023 13:59:26 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:59:26 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:26 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:26 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:26 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:26 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:28 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:59:28 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:28 - INFO - __main__ -   guid: dev-0
05/28/2023 13:59:28 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:59:28 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:28 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:28 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:28 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:28 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:28 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:59:28 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:59:29 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:59:29 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:59:29 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:59:29 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:59:29 - INFO - __main__ -     Batch size = 32
05/28/2023 13:59:29 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.04it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.78it/s][A05/28/2023 13:59:29 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:29 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:59:29 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:29 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 126.82it/s]
05/28/2023 13:59:29 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:29 - INFO - __main__ -    dev: acc = 0.516245487364621
05/28/2023 13:59:29 - INFO - __main__ -    dev: eval_loss = 0.6927489969465468
05/28/2023 13:59:29 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:29 - INFO - __main__ -    dev: infer_time = 2.8021111111111114
05/28/2023 13:59:29 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:29 - INFO - __main__ -     acc = 0.516245487364621
05/28/2023 13:59:29 - INFO - __main__ -     cls_loss = 0.7056949999597337
05/28/2023 13:59:29 - INFO - __main__ -     eval_loss = 0.6927489969465468
05/28/2023 13:59:29 - INFO - __main__ -     global_step = 9
05/28/2023 13:59:29 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:29 - INFO - __main__ -     infer_time = 2.8021111111111114
05/28/2023 13:59:29 - INFO - __main__ -     loss = 0.7056949999597337
05/28/2023 13:59:29 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.98it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.91it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.11it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.51it/s][A05/28/2023 13:59:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:31 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:59:31 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 126.15it/s]
05/28/2023 13:59:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:31 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:31 - INFO - __main__ -    dev: eval_loss = 0.6946634186638726
05/28/2023 13:59:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:31 - INFO - __main__ -    dev: infer_time = 2.844333333333333
05/28/2023 13:59:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:31 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:31 - INFO - __main__ -     cls_loss = 0.7036811458437067
05/28/2023 13:59:31 - INFO - __main__ -     eval_loss = 0.6946634186638726
05/28/2023 13:59:31 - INFO - __main__ -     global_step = 19
05/28/2023 13:59:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:31 - INFO - __main__ -     infer_time = 2.844333333333333
05/28/2023 13:59:31 - INFO - __main__ -     loss = 0.7036811458437067

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.50it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.83it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.00it/s][A05/28/2023 13:59:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:32 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:59:32 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 126.83it/s]
05/28/2023 13:59:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:32 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:32 - INFO - __main__ -    dev: eval_loss = 0.6919149094157748
05/28/2023 13:59:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:32 - INFO - __main__ -    dev: infer_time = 2.8296666666666668
05/28/2023 13:59:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:32 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:32 - INFO - __main__ -     cls_loss = 0.7011496774081526
05/28/2023 13:59:32 - INFO - __main__ -     eval_loss = 0.6919149094157748
05/28/2023 13:59:32 - INFO - __main__ -     global_step = 29
05/28/2023 13:59:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:32 - INFO - __main__ -     infer_time = 2.8296666666666668
05/28/2023 13:59:32 - INFO - __main__ -     loss = 0.7011496774081526
05/28/2023 13:59:32 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:09,  5.06it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:06,  6.65it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:04,  8.55it/s][A05/28/2023 13:59:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:33 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:59:33 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 126.29it/s]
05/28/2023 13:59:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:34 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:34 - INFO - __main__ -    dev: eval_loss = 0.6913011206520928
05/28/2023 13:59:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:34 - INFO - __main__ -    dev: infer_time = 2.846222222222222
05/28/2023 13:59:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:34 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:34 - INFO - __main__ -     cls_loss = 0.700420876344045
05/28/2023 13:59:34 - INFO - __main__ -     eval_loss = 0.6913011206520928
05/28/2023 13:59:34 - INFO - __main__ -     global_step = 39
05/28/2023 13:59:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:34 - INFO - __main__ -     infer_time = 2.846222222222222
05/28/2023 13:59:34 - INFO - __main__ -     loss = 0.700420876344045

Iteration:  50%|#####     | 39/78 [00:04<00:03,  9.80it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:03, 11.88it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:02, 14.07it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 16.09it/s][A05/28/2023 13:59:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:34 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:59:34 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 126.60it/s]
05/28/2023 13:59:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:34 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:34 - INFO - __main__ -    dev: eval_loss = 0.6973409785164727
05/28/2023 13:59:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:34 - INFO - __main__ -    dev: infer_time = 2.8265555555555553
05/28/2023 13:59:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:34 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:34 - INFO - __main__ -     cls_loss = 0.6993338149421069
05/28/2023 13:59:34 - INFO - __main__ -     eval_loss = 0.6973409785164727
05/28/2023 13:59:34 - INFO - __main__ -     global_step = 49
05/28/2023 13:59:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:34 - INFO - __main__ -     infer_time = 2.8265555555555553
05/28/2023 13:59:34 - INFO - __main__ -     loss = 0.6993338149421069

Iteration:  65%|######5   | 51/78 [00:05<00:01, 15.38it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 17.28it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 18.96it/s][A05/28/2023 13:59:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:34 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:59:34 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 126.32it/s]
05/28/2023 13:59:35 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:35 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:35 - INFO - __main__ -    dev: eval_loss = 0.6962971025043063
05/28/2023 13:59:35 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:35 - INFO - __main__ -    dev: infer_time = 2.8397777777777775
05/28/2023 13:59:35 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:35 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:35 - INFO - __main__ -     cls_loss = 0.6984053082385305
05/28/2023 13:59:35 - INFO - __main__ -     eval_loss = 0.6962971025043063
05/28/2023 13:59:35 - INFO - __main__ -     global_step = 59
05/28/2023 13:59:35 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:35 - INFO - __main__ -     infer_time = 2.8397777777777775
05/28/2023 13:59:35 - INFO - __main__ -     loss = 0.6984053082385305

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 17.36it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 18.91it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 20.32it/s][A05/28/2023 13:59:35 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:35 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:59:35 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:35 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.68it/s]
05/28/2023 13:59:35 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:35 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:35 - INFO - __main__ -    dev: eval_loss = 0.6950298282835219
05/28/2023 13:59:35 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:35 - INFO - __main__ -    dev: infer_time = 2.9343333333333335
05/28/2023 13:59:35 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:35 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:35 - INFO - __main__ -     cls_loss = 0.6977628901384879
05/28/2023 13:59:35 - INFO - __main__ -     eval_loss = 0.6950298282835219
05/28/2023 13:59:35 - INFO - __main__ -     global_step = 69
05/28/2023 13:59:35 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:35 - INFO - __main__ -     infer_time = 2.9343333333333335
05/28/2023 13:59:35 - INFO - __main__ -     loss = 0.6977628901384879

Iteration:  88%|########8 | 69/78 [00:06<00:00, 18.11it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 19.56it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 20.83it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.24it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.37s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.37s/it]
05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   w_emb: 12819240

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   p_emb: 215040

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   t_emb: 840

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   ln_emb: 840

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   query_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   key_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   value_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   self_numel: 530460

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   output_numel: 177660

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 323328

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 322980

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   attention_numel: 708120

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   intermediate_numel: 323328

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   output_numel: 323820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   query_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   key_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   value_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   self_numel: 530460

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   output_numel: 177660

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 323328

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 322980

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   attention_numel: 708120

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   intermediate_numel: 323328

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   output_numel: 323820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   query_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   key_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   value_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   self_numel: 530460

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   output_numel: 177660

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 323328

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 322980

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   attention_numel: 708120

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   intermediate_numel: 323328

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   output_numel: 323820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   layer_numel: 4065804
05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   dense_numel: 176820
05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   emb_numel: 13035960

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   encoder_numel: 4065804

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   pooler_numel: 176820

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   all parameters: 17278584

05/28/2023 13:59:35 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:35 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 420, 'sample_intermediate_sizes': [768, 768, 768], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [420, 420, 420]}
parameter size = 17278584
best_acc = 0.5270758122743683
time_per_batch_infer = 2.846 ms
infer_cnt = 63
**************E*************

05/28/2023 13:59:35 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
05/28/2023 13:59:35 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:59:35 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:59:35 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:35 - INFO - __main__ -   guid: train-0
05/28/2023 13:59:35 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:59:35 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:35 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:35 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:37 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:59:37 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:37 - INFO - __main__ -   guid: dev-0
05/28/2023 13:59:37 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:59:37 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:37 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:37 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:37 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:37 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:37 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:59:37 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:59:38 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:59:38 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:59:38 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:59:38 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:59:38 - INFO - __main__ -     Batch size = 32
05/28/2023 13:59:38 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.78it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.84it/s][A05/28/2023 13:59:38 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:38 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:59:38 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:38 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.67it/s]
05/28/2023 13:59:38 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:38 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:38 - INFO - __main__ -    dev: eval_loss = 0.7035831544134352
05/28/2023 13:59:38 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:38 - INFO - __main__ -    dev: infer_time = 2.8274444444444446
05/28/2023 13:59:38 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:38 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:38 - INFO - __main__ -     cls_loss = 0.6909764541520013
05/28/2023 13:59:38 - INFO - __main__ -     eval_loss = 0.7035831544134352
05/28/2023 13:59:38 - INFO - __main__ -     global_step = 9
05/28/2023 13:59:38 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:38 - INFO - __main__ -     infer_time = 2.8274444444444446
05/28/2023 13:59:38 - INFO - __main__ -     loss = 0.6909764541520013
05/28/2023 13:59:38 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.88it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.73it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.74it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.96it/s][A05/28/2023 13:59:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:40 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:59:40 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.58it/s]
05/28/2023 13:59:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:40 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:40 - INFO - __main__ -    dev: eval_loss = 0.7211086816257901
05/28/2023 13:59:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:40 - INFO - __main__ -    dev: infer_time = 2.8471111111111105
05/28/2023 13:59:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:40 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:40 - INFO - __main__ -     cls_loss = 0.7012195210707816
05/28/2023 13:59:40 - INFO - __main__ -     eval_loss = 0.7211086816257901
05/28/2023 13:59:40 - INFO - __main__ -     global_step = 19
05/28/2023 13:59:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:40 - INFO - __main__ -     infer_time = 2.8471111111111105
05/28/2023 13:59:40 - INFO - __main__ -     loss = 0.7012195210707816

Iteration:  26%|##5       | 20/78 [00:02<00:05, 10.13it/s][A
Iteration:  29%|##9       | 23/78 [00:02<00:04, 12.46it/s][A
Iteration:  33%|###3      | 26/78 [00:02<00:03, 14.60it/s][A05/28/2023 13:59:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:41 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:59:41 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.50it/s]
05/28/2023 13:59:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:41 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:41 - INFO - __main__ -    dev: eval_loss = 0.6914813982115852
05/28/2023 13:59:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:41 - INFO - __main__ -    dev: infer_time = 2.8389999999999995
05/28/2023 13:59:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:41 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:41 - INFO - __main__ -     cls_loss = 0.7007409560269323
05/28/2023 13:59:41 - INFO - __main__ -     eval_loss = 0.6914813982115852
05/28/2023 13:59:41 - INFO - __main__ -     global_step = 29
05/28/2023 13:59:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:41 - INFO - __main__ -     infer_time = 2.8389999999999995
05/28/2023 13:59:41 - INFO - __main__ -     loss = 0.7007409560269323

Iteration:  37%|###7      | 29/78 [00:02<00:03, 14.16it/s][A
Iteration:  41%|####1     | 32/78 [00:03<00:02, 16.00it/s][A
Iteration:  45%|####4     | 35/78 [00:03<00:02, 17.54it/s][A
Iteration:  49%|####8     | 38/78 [00:03<00:02, 18.66it/s][A05/28/2023 13:59:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:41 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:59:41 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.78it/s]
05/28/2023 13:59:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:41 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:41 - INFO - __main__ -    dev: eval_loss = 0.7067750692367554
05/28/2023 13:59:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:41 - INFO - __main__ -    dev: infer_time = 2.8468888888888895
05/28/2023 13:59:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:41 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:41 - INFO - __main__ -     cls_loss = 0.6989379861415961
05/28/2023 13:59:41 - INFO - __main__ -     eval_loss = 0.7067750692367554
05/28/2023 13:59:41 - INFO - __main__ -     global_step = 39
05/28/2023 13:59:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:41 - INFO - __main__ -     infer_time = 2.8468888888888895
05/28/2023 13:59:41 - INFO - __main__ -     loss = 0.6989379861415961

Iteration:  53%|#####2    | 41/78 [00:03<00:02, 16.71it/s][A
Iteration:  56%|#####6    | 44/78 [00:03<00:01, 18.04it/s][A
Iteration:  60%|######    | 47/78 [00:03<00:01, 19.09it/s][A05/28/2023 13:59:42 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:42 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:59:42 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:42 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.82it/s]
05/28/2023 13:59:42 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:42 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:42 - INFO - __main__ -    dev: eval_loss = 0.6979442636171976
05/28/2023 13:59:42 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:42 - INFO - __main__ -    dev: infer_time = 2.8533333333333335
05/28/2023 13:59:42 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:42 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:42 - INFO - __main__ -     cls_loss = 0.6985673332700923
05/28/2023 13:59:42 - INFO - __main__ -     eval_loss = 0.6979442636171976
05/28/2023 13:59:42 - INFO - __main__ -     global_step = 49
05/28/2023 13:59:42 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:42 - INFO - __main__ -     infer_time = 2.8533333333333335
05/28/2023 13:59:42 - INFO - __main__ -     loss = 0.6985673332700923

Iteration:  64%|######4   | 50/78 [00:04<00:01, 16.84it/s][A
Iteration:  68%|######7   | 53/78 [00:04<00:01, 18.08it/s][A
Iteration:  72%|#######1  | 56/78 [00:04<00:01, 19.19it/s][A05/28/2023 13:59:42 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:42 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:59:42 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:42 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.77it/s]
05/28/2023 13:59:42 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:42 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:42 - INFO - __main__ -    dev: eval_loss = 0.6909003059069315
05/28/2023 13:59:42 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:42 - INFO - __main__ -    dev: infer_time = 2.8371111111111116
05/28/2023 13:59:42 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:42 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:42 - INFO - __main__ -     cls_loss = 0.6974046957694878
05/28/2023 13:59:42 - INFO - __main__ -     eval_loss = 0.6909003059069315
05/28/2023 13:59:42 - INFO - __main__ -     global_step = 59
05/28/2023 13:59:42 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:42 - INFO - __main__ -     infer_time = 2.8371111111111116
05/28/2023 13:59:42 - INFO - __main__ -     loss = 0.6974046957694878

Iteration:  76%|#######5  | 59/78 [00:04<00:01, 16.92it/s][A
Iteration:  79%|#######9  | 62/78 [00:04<00:00, 18.22it/s][A
Iteration:  83%|########3 | 65/78 [00:04<00:00, 19.18it/s][A
Iteration:  87%|########7 | 68/78 [00:04<00:00, 20.02it/s][A05/28/2023 13:59:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:43 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:59:43 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.71it/s]
05/28/2023 13:59:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:43 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:43 - INFO - __main__ -    dev: eval_loss = 0.6937133338716295
05/28/2023 13:59:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:43 - INFO - __main__ -    dev: infer_time = 2.8321111111111117
05/28/2023 13:59:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:43 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:43 - INFO - __main__ -     cls_loss = 0.6976089209750078
05/28/2023 13:59:43 - INFO - __main__ -     eval_loss = 0.6937133338716295
05/28/2023 13:59:43 - INFO - __main__ -     global_step = 69
05/28/2023 13:59:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:43 - INFO - __main__ -     infer_time = 2.8321111111111117
05/28/2023 13:59:43 - INFO - __main__ -     loss = 0.6976089209750078

Iteration:  91%|#########1| 71/78 [00:05<00:00, 17.36it/s][A
Iteration:  95%|#########4| 74/78 [00:05<00:00, 18.57it/s][A
Iteration:  99%|#########8| 77/78 [00:05<00:00, 19.55it/s][AIteration: 100%|##########| 78/78 [00:05<00:00, 14.35it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.44s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.44s/it]
05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   w_emb: 14284296

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   p_emb: 239616

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   t_emb: 936

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   ln_emb: 936

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 435232

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 434772

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   intermediate_numel: 435232

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   output_numel: 435708

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 435232

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 434772

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   intermediate_numel: 435232

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   output_numel: 435708

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 435232

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 434772

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   intermediate_numel: 435232

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   output_numel: 435708

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   layer_numel: 5249532
05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   dense_numel: 219492
05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   emb_numel: 14525784

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   encoder_numel: 5249532

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   pooler_numel: 219492

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   all parameters: 19994808

05/28/2023 13:59:43 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:43 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
parameter size = 19994808
best_acc = 0.5270758122743683
time_per_batch_infer = 2.840 ms
infer_cnt = 63
**************E*************

05/28/2023 13:59:43 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 420, 'sample_intermediate_sizes': [736, 736, 736], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [420, 420, 420]}
05/28/2023 13:59:43 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:59:43 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:59:43 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:43 - INFO - __main__ -   guid: train-0
05/28/2023 13:59:43 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:59:43 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:43 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:43 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:45 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:59:45 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:45 - INFO - __main__ -   guid: dev-0
05/28/2023 13:59:45 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:59:45 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:45 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:45 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:45 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:45 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:45 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:59:45 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:59:46 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:59:46 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:59:46 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:59:46 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:59:46 - INFO - __main__ -     Batch size = 32
05/28/2023 13:59:46 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.22it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.06it/s][A05/28/2023 13:59:46 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:46 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:59:46 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:46 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 126.89it/s]
05/28/2023 13:59:46 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:46 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 13:59:46 - INFO - __main__ -    dev: eval_loss = 0.6942550606197782
05/28/2023 13:59:46 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:46 - INFO - __main__ -    dev: infer_time = 2.8855555555555554
05/28/2023 13:59:46 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:46 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 13:59:46 - INFO - __main__ -     cls_loss = 0.6918207936816745
05/28/2023 13:59:46 - INFO - __main__ -     eval_loss = 0.6942550606197782
05/28/2023 13:59:46 - INFO - __main__ -     global_step = 9
05/28/2023 13:59:46 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:46 - INFO - __main__ -     infer_time = 2.8855555555555554
05/28/2023 13:59:46 - INFO - __main__ -     loss = 0.6918207936816745
05/28/2023 13:59:46 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.97it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.86it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.09it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.48it/s][A05/28/2023 13:59:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:48 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:59:48 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 126.94it/s]
05/28/2023 13:59:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:48 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 13:59:48 - INFO - __main__ -    dev: eval_loss = 0.6946869492530823
05/28/2023 13:59:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:48 - INFO - __main__ -    dev: infer_time = 2.8791111111111114
05/28/2023 13:59:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:48 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 13:59:48 - INFO - __main__ -     cls_loss = 0.692341010821493
05/28/2023 13:59:48 - INFO - __main__ -     eval_loss = 0.6946869492530823
05/28/2023 13:59:48 - INFO - __main__ -     global_step = 19
05/28/2023 13:59:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:48 - INFO - __main__ -     infer_time = 2.8791111111111114
05/28/2023 13:59:48 - INFO - __main__ -     loss = 0.692341010821493

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.53it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.81it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.01it/s][A05/28/2023 13:59:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:49 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:59:49 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 126.96it/s]
05/28/2023 13:59:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:49 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 13:59:49 - INFO - __main__ -    dev: eval_loss = 0.6984923813078139
05/28/2023 13:59:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:49 - INFO - __main__ -    dev: infer_time = 2.8858888888888887
05/28/2023 13:59:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:49 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 13:59:49 - INFO - __main__ -     cls_loss = 0.6943349324423691
05/28/2023 13:59:49 - INFO - __main__ -     eval_loss = 0.6984923813078139
05/28/2023 13:59:49 - INFO - __main__ -     global_step = 29
05/28/2023 13:59:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:49 - INFO - __main__ -     infer_time = 2.8858888888888887
05/28/2023 13:59:49 - INFO - __main__ -     loss = 0.6943349324423691

Iteration:  38%|###8      | 30/78 [00:02<00:03, 15.61it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 17.39it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 19.06it/s][A05/28/2023 13:59:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:49 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:59:49 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.19it/s]
05/28/2023 13:59:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:49 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:49 - INFO - __main__ -    dev: eval_loss = 0.6918563577863905
05/28/2023 13:59:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:49 - INFO - __main__ -    dev: infer_time = 2.8713333333333333
05/28/2023 13:59:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:49 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:49 - INFO - __main__ -     cls_loss = 0.6948831570454133
05/28/2023 13:59:49 - INFO - __main__ -     eval_loss = 0.6918563577863905
05/28/2023 13:59:49 - INFO - __main__ -     global_step = 39
05/28/2023 13:59:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:49 - INFO - __main__ -     infer_time = 2.8713333333333333
05/28/2023 13:59:49 - INFO - __main__ -     loss = 0.6948831570454133
05/28/2023 13:59:49 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:04<00:07,  5.33it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:05,  6.94it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:03,  8.81it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:02, 10.92it/s][A05/28/2023 13:59:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:51 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:59:51 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.44it/s]
05/28/2023 13:59:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:51 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:51 - INFO - __main__ -    dev: eval_loss = 0.6931231088108487
05/28/2023 13:59:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:51 - INFO - __main__ -    dev: infer_time = 2.8271111111111114
05/28/2023 13:59:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:51 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:51 - INFO - __main__ -     cls_loss = 0.6944788548411155
05/28/2023 13:59:51 - INFO - __main__ -     eval_loss = 0.6931231088108487
05/28/2023 13:59:51 - INFO - __main__ -     global_step = 49
05/28/2023 13:59:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:51 - INFO - __main__ -     infer_time = 2.8271111111111114
05/28/2023 13:59:51 - INFO - __main__ -     loss = 0.6944788548411155

Iteration:  65%|######5   | 51/78 [00:05<00:02, 11.71it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 13.62it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 15.66it/s][A05/28/2023 13:59:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:51 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:59:51 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.56it/s]
05/28/2023 13:59:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:51 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:51 - INFO - __main__ -    dev: eval_loss = 0.6919799049695333
05/28/2023 13:59:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:51 - INFO - __main__ -    dev: infer_time = 2.8171111111111116
05/28/2023 13:59:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:51 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:51 - INFO - __main__ -     cls_loss = 0.6944386928768481
05/28/2023 13:59:51 - INFO - __main__ -     eval_loss = 0.6919799049695333
05/28/2023 13:59:51 - INFO - __main__ -     global_step = 59
05/28/2023 13:59:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:51 - INFO - __main__ -     infer_time = 2.8171111111111116
05/28/2023 13:59:51 - INFO - __main__ -     loss = 0.6944386928768481

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 15.43it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 17.36it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 19.07it/s][A05/28/2023 13:59:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:52 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 13:59:52 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.60it/s]
05/28/2023 13:59:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:52 - INFO - __main__ -    dev: acc = 0.48736462093862815
05/28/2023 13:59:52 - INFO - __main__ -    dev: eval_loss = 0.6936507357491387
05/28/2023 13:59:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:52 - INFO - __main__ -    dev: infer_time = 2.814666666666666
05/28/2023 13:59:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:52 - INFO - __main__ -     acc = 0.48736462093862815
05/28/2023 13:59:52 - INFO - __main__ -     cls_loss = 0.694181065628494
05/28/2023 13:59:52 - INFO - __main__ -     eval_loss = 0.6936507357491387
05/28/2023 13:59:52 - INFO - __main__ -     global_step = 69
05/28/2023 13:59:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:52 - INFO - __main__ -     infer_time = 2.814666666666666
05/28/2023 13:59:52 - INFO - __main__ -     loss = 0.694181065628494

Iteration:  88%|########8 | 69/78 [00:06<00:00, 17.65it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 19.20it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 20.31it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.23it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.38s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.38s/it]
05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   w_emb: 12819240

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   p_emb: 215040

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   t_emb: 840

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   ln_emb: 840

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   query_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   key_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   value_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   self_numel: 530460

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   output_numel: 177660

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 309856

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 309540

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   attention_numel: 708120

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   intermediate_numel: 309856

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   output_numel: 310380

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   query_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   key_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   value_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   self_numel: 530460

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   output_numel: 177660

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 309856

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 309540

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   attention_numel: 708120

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   intermediate_numel: 309856

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   output_numel: 310380

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   query_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   key_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   value_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   self_numel: 530460

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   output_numel: 177660

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 309856

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 309540

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   ln_numel: 840

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   attention_numel: 708120

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   intermediate_numel: 309856

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   output_numel: 310380

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   layer_numel: 3985068
05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   dense_numel: 176820
05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   emb_numel: 13035960

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   encoder_numel: 3985068

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   pooler_numel: 176820

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   all parameters: 17197848

05/28/2023 13:59:52 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 13:59:52 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 420, 'sample_intermediate_sizes': [736, 736, 736], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [420, 420, 420]}
parameter size = 17197848
best_acc = 0.5270758122743683
time_per_batch_infer = 2.854 ms
infer_cnt = 63
**************E*************

05/28/2023 13:59:52 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 156, 'sample_intermediate_sizes': [480, 480, 480, 480, 480], 'sample_qkv_sizes': [156, 156, 156, 156, 156]}
05/28/2023 13:59:52 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 13:59:52 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 13:59:52 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:52 - INFO - __main__ -   guid: train-0
05/28/2023 13:59:52 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 13:59:52 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:52 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:52 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:52 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:52 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:54 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 13:59:54 - INFO - __main__ -   *** Example ***
05/28/2023 13:59:54 - INFO - __main__ -   guid: dev-0
05/28/2023 13:59:54 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 13:59:54 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:54 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:54 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 13:59:54 - INFO - __main__ -   label: not_entailment
05/28/2023 13:59:54 - INFO - __main__ -   label_id: 1
05/28/2023 13:59:54 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 13:59:54 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 13:59:55 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 13:59:55 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 13:59:55 - INFO - __main__ -   ***** Running training *****
05/28/2023 13:59:55 - INFO - __main__ -     Num examples = 2490
05/28/2023 13:59:55 - INFO - __main__ -     Batch size = 32
05/28/2023 13:59:55 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 20.78it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.96it/s][A05/28/2023 13:59:55 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:55 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 13:59:55 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:55 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.73it/s]
05/28/2023 13:59:55 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:55 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:55 - INFO - __main__ -    dev: eval_loss = 0.6915310091442533
05/28/2023 13:59:55 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:55 - INFO - __main__ -    dev: infer_time = 4.214111111111111
05/28/2023 13:59:55 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:55 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:55 - INFO - __main__ -     cls_loss = 0.6961199773682488
05/28/2023 13:59:55 - INFO - __main__ -     eval_loss = 0.6915310091442533
05/28/2023 13:59:55 - INFO - __main__ -     global_step = 9
05/28/2023 13:59:55 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:55 - INFO - __main__ -     infer_time = 4.214111111111111
05/28/2023 13:59:55 - INFO - __main__ -     loss = 0.6961199773682488
05/28/2023 13:59:55 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.67it/s][A
Iteration:  14%|#4        | 11/78 [00:02<00:13,  4.87it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:09,  7.03it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:06,  9.35it/s][A05/28/2023 13:59:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:57 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 13:59:57 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 170.37it/s]
05/28/2023 13:59:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:57 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:57 - INFO - __main__ -    dev: eval_loss = 0.6918288601769341
05/28/2023 13:59:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:57 - INFO - __main__ -    dev: infer_time = 4.188888888888889
05/28/2023 13:59:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:57 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:57 - INFO - __main__ -     cls_loss = 0.6943025181168004
05/28/2023 13:59:57 - INFO - __main__ -     eval_loss = 0.6918288601769341
05/28/2023 13:59:57 - INFO - __main__ -     global_step = 19
05/28/2023 13:59:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:57 - INFO - __main__ -     infer_time = 4.188888888888889
05/28/2023 13:59:57 - INFO - __main__ -     loss = 0.6943025181168004

Iteration:  24%|##4       | 19/78 [00:02<00:05, 10.17it/s][A
Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.59it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.25it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.51it/s][A05/28/2023 13:59:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:58 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 13:59:58 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 170.74it/s]
05/28/2023 13:59:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:58 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:58 - INFO - __main__ -    dev: eval_loss = 0.6923455794652303
05/28/2023 13:59:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:58 - INFO - __main__ -    dev: infer_time = 4.198555555555556
05/28/2023 13:59:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:58 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:58 - INFO - __main__ -     cls_loss = 0.6947338375551947
05/28/2023 13:59:58 - INFO - __main__ -     eval_loss = 0.6923455794652303
05/28/2023 13:59:58 - INFO - __main__ -     global_step = 29
05/28/2023 13:59:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:58 - INFO - __main__ -     infer_time = 4.198555555555556
05/28/2023 13:59:58 - INFO - __main__ -     loss = 0.6947338375551947

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.12it/s][A
Iteration:  42%|####2     | 33/78 [00:03<00:02, 17.72it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 18.92it/s][A05/28/2023 13:59:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:58 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 13:59:58 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 171.57it/s]
05/28/2023 13:59:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:58 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 13:59:58 - INFO - __main__ -    dev: eval_loss = 0.6919740703370836
05/28/2023 13:59:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:58 - INFO - __main__ -    dev: infer_time = 4.159222222222223
05/28/2023 13:59:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:58 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 13:59:58 - INFO - __main__ -     cls_loss = 0.6951573063165714
05/28/2023 13:59:58 - INFO - __main__ -     eval_loss = 0.6919740703370836
05/28/2023 13:59:58 - INFO - __main__ -     global_step = 39
05/28/2023 13:59:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:58 - INFO - __main__ -     infer_time = 4.159222222222223
05/28/2023 13:59:58 - INFO - __main__ -     loss = 0.6951573063165714

Iteration:  50%|#####     | 39/78 [00:03<00:02, 17.95it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 19.11it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 20.32it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 21.27it/s][A05/28/2023 13:59:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:59 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 13:59:59 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 171.59it/s]
05/28/2023 13:59:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:59 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:59 - INFO - __main__ -    dev: eval_loss = 0.6955364147822062
05/28/2023 13:59:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:59 - INFO - __main__ -    dev: infer_time = 4.147444444444444
05/28/2023 13:59:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:59 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:59 - INFO - __main__ -     cls_loss = 0.6952603933762531
05/28/2023 13:59:59 - INFO - __main__ -     eval_loss = 0.6955364147822062
05/28/2023 13:59:59 - INFO - __main__ -     global_step = 49
05/28/2023 13:59:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:59 - INFO - __main__ -     infer_time = 4.147444444444444
05/28/2023 13:59:59 - INFO - __main__ -     loss = 0.6952603933762531

Iteration:  65%|######5   | 51/78 [00:04<00:01, 19.06it/s][A
Iteration:  69%|######9   | 54/78 [00:04<00:01, 20.12it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:01, 20.85it/s][A05/28/2023 13:59:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 13:59:59 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 13:59:59 - INFO - __main__ -     Num examples = 277
05/28/2023 13:59:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 171.47it/s]
05/28/2023 13:59:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 13:59:59 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 13:59:59 - INFO - __main__ -    dev: eval_loss = 0.6973023414611816
05/28/2023 13:59:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 13:59:59 - INFO - __main__ -    dev: infer_time = 4.180888888888889
05/28/2023 13:59:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 13:59:59 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 13:59:59 - INFO - __main__ -     cls_loss = 0.6947420401088262
05/28/2023 13:59:59 - INFO - __main__ -     eval_loss = 0.6973023414611816
05/28/2023 13:59:59 - INFO - __main__ -     global_step = 59
05/28/2023 13:59:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 13:59:59 - INFO - __main__ -     infer_time = 4.180888888888889
05/28/2023 13:59:59 - INFO - __main__ -     loss = 0.6947420401088262

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 18.87it/s][A
Iteration:  79%|#######9  | 62/78 [00:04<00:00, 19.02it/s][A
Iteration:  83%|########3 | 65/78 [00:04<00:00, 20.03it/s][A
Iteration:  87%|########7 | 68/78 [00:04<00:00, 21.05it/s][A05/28/2023 14:00:00 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:00 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:00:00 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:00 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 171.79it/s]
05/28/2023 14:00:00 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:00 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:00 - INFO - __main__ -    dev: eval_loss = 0.6967212955156962
05/28/2023 14:00:00 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:00 - INFO - __main__ -    dev: infer_time = 4.171333333333333
05/28/2023 14:00:00 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:00 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:00 - INFO - __main__ -     cls_loss = 0.6947917782742045
05/28/2023 14:00:00 - INFO - __main__ -     eval_loss = 0.6967212955156962
05/28/2023 14:00:00 - INFO - __main__ -     global_step = 69
05/28/2023 14:00:00 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:00 - INFO - __main__ -     infer_time = 4.171333333333333
05/28/2023 14:00:00 - INFO - __main__ -     loss = 0.6947917782742045

Iteration:  91%|#########1| 71/78 [00:05<00:00, 18.84it/s][A
Iteration:  95%|#########4| 74/78 [00:05<00:00, 19.79it/s][A
Iteration:  99%|#########8| 77/78 [00:05<00:00, 20.98it/s][AIteration: 100%|##########| 78/78 [00:05<00:00, 14.77it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.28s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.28s/it]
05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   w_emb: 4761432

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   p_emb: 79872

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   t_emb: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_emb: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75036

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 75348

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75036

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 75348

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75036

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 75348

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75036

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 75348

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 75036

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 75360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   output_numel: 75348

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   layer_numel: 1244940
05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   dense_numel: 24492
05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   emb_numel: 4841928

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   encoder_numel: 1244940

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   pooler_numel: 24492

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   all parameters: 6111360

05/28/2023 14:00:00 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:00 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 156, 'sample_intermediate_sizes': [480, 480, 480, 480, 480], 'sample_qkv_sizes': [156, 156, 156, 156, 156]}
parameter size = 6111360
best_acc = 0.5270758122743683
time_per_batch_infer = 4.180 ms
infer_cnt = 63
**************E*************

05/28/2023 14:00:00 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}
05/28/2023 14:00:00 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:00:00 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:00:00 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:00 - INFO - __main__ -   guid: train-0
05/28/2023 14:00:00 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:00:00 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:00 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:00 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:02 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:00:02 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:02 - INFO - __main__ -   guid: dev-0
05/28/2023 14:00:02 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:00:02 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:02 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:02 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:02 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:02 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:00:02 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:00:03 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:00:03 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:00:03 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:00:03 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:00:03 - INFO - __main__ -     Batch size = 32
05/28/2023 14:00:03 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   3%|2         | 2/78 [00:00<00:03, 19.78it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 21.49it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 21.81it/s][A05/28/2023 14:00:03 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:03 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:00:03 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:03 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.73it/s]
05/28/2023 14:00:03 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:03 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:03 - INFO - __main__ -    dev: eval_loss = 0.6940362983279758
05/28/2023 14:00:03 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:03 - INFO - __main__ -    dev: infer_time = 3.5773333333333337
05/28/2023 14:00:03 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:03 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:03 - INFO - __main__ -     cls_loss = 0.6915688382254707
05/28/2023 14:00:03 - INFO - __main__ -     eval_loss = 0.6940362983279758
05/28/2023 14:00:03 - INFO - __main__ -     global_step = 9
05/28/2023 14:00:03 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:03 - INFO - __main__ -     infer_time = 3.5773333333333337
05/28/2023 14:00:03 - INFO - __main__ -     loss = 0.6915688382254707
05/28/2023 14:00:03 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:01<00:15,  4.24it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:10,  6.05it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:07,  8.08it/s][A05/28/2023 14:00:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:05 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:00:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.17it/s]
05/28/2023 14:00:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:05 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:05 - INFO - __main__ -    dev: eval_loss = 0.699477473894755
05/28/2023 14:00:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:05 - INFO - __main__ -    dev: infer_time = 3.617444444444444
05/28/2023 14:00:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:05 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:05 - INFO - __main__ -     cls_loss = 0.6998177892283389
05/28/2023 14:00:05 - INFO - __main__ -     eval_loss = 0.699477473894755
05/28/2023 14:00:05 - INFO - __main__ -     global_step = 19
05/28/2023 14:00:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:05 - INFO - __main__ -     infer_time = 3.617444444444444
05/28/2023 14:00:05 - INFO - __main__ -     loss = 0.6998177892283389

Iteration:  24%|##4       | 19/78 [00:02<00:06,  8.76it/s][A
Iteration:  27%|##6       | 21/78 [00:02<00:05, 10.26it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 12.72it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 14.85it/s][A05/28/2023 14:00:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:05 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:00:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.66it/s]
05/28/2023 14:00:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:05 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:05 - INFO - __main__ -    dev: eval_loss = 0.6908943520651923
05/28/2023 14:00:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:05 - INFO - __main__ -    dev: infer_time = 3.5822222222222218
05/28/2023 14:00:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:05 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:05 - INFO - __main__ -     cls_loss = 0.6980035161149913
05/28/2023 14:00:05 - INFO - __main__ -     eval_loss = 0.6908943520651923
05/28/2023 14:00:05 - INFO - __main__ -     global_step = 29
05/28/2023 14:00:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:05 - INFO - __main__ -     infer_time = 3.5822222222222218
05/28/2023 14:00:05 - INFO - __main__ -     loss = 0.6980035161149913

Iteration:  37%|###7      | 29/78 [00:02<00:03, 14.04it/s][A
Iteration:  41%|####1     | 32/78 [00:03<00:02, 16.01it/s][A
Iteration:  45%|####4     | 35/78 [00:03<00:02, 17.67it/s][A
Iteration:  49%|####8     | 38/78 [00:03<00:02, 18.92it/s][A05/28/2023 14:00:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:06 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:00:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.57it/s]
05/28/2023 14:00:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:06 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:06 - INFO - __main__ -    dev: eval_loss = 0.6906439794434441
05/28/2023 14:00:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:06 - INFO - __main__ -    dev: infer_time = 3.576666666666666
05/28/2023 14:00:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:06 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:06 - INFO - __main__ -     cls_loss = 0.6968781260343698
05/28/2023 14:00:06 - INFO - __main__ -     eval_loss = 0.6906439794434441
05/28/2023 14:00:06 - INFO - __main__ -     global_step = 39
05/28/2023 14:00:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:06 - INFO - __main__ -     infer_time = 3.576666666666666
05/28/2023 14:00:06 - INFO - __main__ -     loss = 0.6968781260343698

Iteration:  53%|#####2    | 41/78 [00:03<00:02, 17.00it/s][A
Iteration:  56%|#####6    | 44/78 [00:03<00:01, 18.40it/s][A
Iteration:  60%|######    | 47/78 [00:03<00:01, 19.40it/s][A05/28/2023 14:00:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:06 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:00:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.63it/s]
05/28/2023 14:00:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:06 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:00:06 - INFO - __main__ -    dev: eval_loss = 0.6920430064201355
05/28/2023 14:00:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:06 - INFO - __main__ -    dev: infer_time = 3.5919999999999996
05/28/2023 14:00:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:07 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:00:07 - INFO - __main__ -     cls_loss = 0.6964261069589731
05/28/2023 14:00:07 - INFO - __main__ -     eval_loss = 0.6920430064201355
05/28/2023 14:00:07 - INFO - __main__ -     global_step = 49
05/28/2023 14:00:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:07 - INFO - __main__ -     infer_time = 3.5919999999999996
05/28/2023 14:00:07 - INFO - __main__ -     loss = 0.6964261069589731
05/28/2023 14:00:07 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  64%|######4   | 50/78 [00:05<00:05,  4.82it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:03,  6.31it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:02,  8.06it/s][A05/28/2023 14:00:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:08 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:00:08 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.47it/s]
05/28/2023 14:00:09 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:09 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:09 - INFO - __main__ -    dev: eval_loss = 0.698657837178972
05/28/2023 14:00:09 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:09 - INFO - __main__ -    dev: infer_time = 3.5815555555555556
05/28/2023 14:00:09 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:09 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:09 - INFO - __main__ -     cls_loss = 0.6956143096341925
05/28/2023 14:00:09 - INFO - __main__ -     eval_loss = 0.698657837178972
05/28/2023 14:00:09 - INFO - __main__ -     global_step = 59
05/28/2023 14:00:09 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:09 - INFO - __main__ -     infer_time = 3.5815555555555556
05/28/2023 14:00:09 - INFO - __main__ -     loss = 0.6956143096341925

Iteration:  76%|#######5  | 59/78 [00:05<00:02,  9.04it/s][A
Iteration:  78%|#######8  | 61/78 [00:06<00:01, 10.29it/s][A
Iteration:  82%|########2 | 64/78 [00:06<00:01, 12.47it/s][A
Iteration:  86%|########5 | 67/78 [00:06<00:00, 14.49it/s][A05/28/2023 14:00:09 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:09 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:00:09 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:09 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.72it/s]
05/28/2023 14:00:09 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:09 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:09 - INFO - __main__ -    dev: eval_loss = 0.702174617184533
05/28/2023 14:00:09 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:09 - INFO - __main__ -    dev: infer_time = 3.5817777777777784
05/28/2023 14:00:09 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:09 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:09 - INFO - __main__ -     cls_loss = 0.6947958175686823
05/28/2023 14:00:09 - INFO - __main__ -     eval_loss = 0.702174617184533
05/28/2023 14:00:09 - INFO - __main__ -     global_step = 69
05/28/2023 14:00:09 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:09 - INFO - __main__ -     infer_time = 3.5817777777777784
05/28/2023 14:00:09 - INFO - __main__ -     loss = 0.6947958175686823

Iteration:  90%|########9 | 70/78 [00:06<00:00, 14.24it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 16.09it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 17.59it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.37it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.86s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.86s/it]
05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   w_emb: 10621656

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   p_emb: 178176

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   t_emb: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ln_emb: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 178688

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 178524

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 178688

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   output_numel: 179220

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 178688

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 178524

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 178688

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   output_numel: 179220

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 178688

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 178524

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 178688

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   output_numel: 179220

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 178688

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 178524

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 178688

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   output_numel: 179220

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   layer_numel: 3377648
05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121452
05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   emb_numel: 10801224

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   encoder_numel: 3377648

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   pooler_numel: 121452

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   all parameters: 14300324

05/28/2023 14:00:09 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:09 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}
parameter size = 14300324
best_acc = 0.5379061371841155
time_per_batch_infer = 3.587 ms
infer_cnt = 63
**************E*************

05/28/2023 14:00:09 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 168, 'sample_intermediate_sizes': [704, 704, 704, 704, 704], 'sample_qkv_sizes': [168, 168, 168, 168, 168]}
05/28/2023 14:00:09 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:00:09 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:00:09 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:09 - INFO - __main__ -   guid: train-0
05/28/2023 14:00:09 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:00:09 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:09 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:09 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:09 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:11 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:00:11 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:11 - INFO - __main__ -   guid: dev-0
05/28/2023 14:00:11 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:00:11 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:11 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:11 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:11 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:11 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:11 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:00:11 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:00:12 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:00:12 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:00:12 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:00:12 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:00:12 - INFO - __main__ -     Batch size = 32
05/28/2023 14:00:12 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.27it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.70it/s][A05/28/2023 14:00:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:12 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:00:12 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.53it/s]
05/28/2023 14:00:12 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:12 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:12 - INFO - __main__ -    dev: eval_loss = 0.6962739957703484
05/28/2023 14:00:12 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:12 - INFO - __main__ -    dev: infer_time = 4.209888888888889
05/28/2023 14:00:12 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:12 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:12 - INFO - __main__ -     cls_loss = 0.6936777763896518
05/28/2023 14:00:12 - INFO - __main__ -     eval_loss = 0.6962739957703484
05/28/2023 14:00:12 - INFO - __main__ -     global_step = 9
05/28/2023 14:00:12 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:12 - INFO - __main__ -     infer_time = 4.209888888888889
05/28/2023 14:00:12 - INFO - __main__ -     loss = 0.6936777763896518
05/28/2023 14:00:12 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.91it/s][A
Iteration:  14%|#4        | 11/78 [00:01<00:13,  5.12it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:08,  7.26it/s][A
Iteration:  21%|##        | 16/78 [00:02<00:07,  8.75it/s][A05/28/2023 14:00:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:14 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:00:14 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.85it/s]
05/28/2023 14:00:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:14 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:14 - INFO - __main__ -    dev: eval_loss = 0.6919473078515794
05/28/2023 14:00:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:14 - INFO - __main__ -    dev: infer_time = 4.2042222222222225
05/28/2023 14:00:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:14 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:14 - INFO - __main__ -     cls_loss = 0.6944253664267691
05/28/2023 14:00:14 - INFO - __main__ -     eval_loss = 0.6919473078515794
05/28/2023 14:00:14 - INFO - __main__ -     global_step = 19
05/28/2023 14:00:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:14 - INFO - __main__ -     infer_time = 4.2042222222222225
05/28/2023 14:00:14 - INFO - __main__ -     loss = 0.6944253664267691
05/28/2023 14:00:14 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  24%|##4       | 19/78 [00:03<00:15,  3.92it/s][A
Iteration:  27%|##6       | 21/78 [00:03<00:11,  4.95it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:07,  6.86it/s][A
Iteration:  35%|###4      | 27/78 [00:04<00:05,  8.95it/s][A05/28/2023 14:00:16 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:16 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:00:16 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:16 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.76it/s]
05/28/2023 14:00:16 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:16 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:16 - INFO - __main__ -    dev: eval_loss = 0.6941834489504496
05/28/2023 14:00:16 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:16 - INFO - __main__ -    dev: infer_time = 4.2412222222222224
05/28/2023 14:00:16 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:16 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:16 - INFO - __main__ -     cls_loss = 0.6942085582634498
05/28/2023 14:00:16 - INFO - __main__ -     eval_loss = 0.6941834489504496
05/28/2023 14:00:16 - INFO - __main__ -     global_step = 29
05/28/2023 14:00:16 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:16 - INFO - __main__ -     infer_time = 4.2412222222222224
05/28/2023 14:00:16 - INFO - __main__ -     loss = 0.6942085582634498

Iteration:  37%|###7      | 29/78 [00:04<00:05,  9.67it/s][A
Iteration:  40%|###9      | 31/78 [00:04<00:04, 11.15it/s][A
Iteration:  44%|####3     | 34/78 [00:04<00:03, 13.58it/s][A
Iteration:  47%|####7     | 37/78 [00:04<00:02, 15.64it/s][A05/28/2023 14:00:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:17 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:00:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.67it/s]
05/28/2023 14:00:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:17 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:17 - INFO - __main__ -    dev: eval_loss = 0.6952748232417636
05/28/2023 14:00:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:17 - INFO - __main__ -    dev: infer_time = 4.219777777777777
05/28/2023 14:00:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:17 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:17 - INFO - __main__ -     cls_loss = 0.6940823273781018
05/28/2023 14:00:17 - INFO - __main__ -     eval_loss = 0.6952748232417636
05/28/2023 14:00:17 - INFO - __main__ -     global_step = 39
05/28/2023 14:00:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:17 - INFO - __main__ -     infer_time = 4.219777777777777
05/28/2023 14:00:17 - INFO - __main__ -     loss = 0.6940823273781018

Iteration:  51%|#####1    | 40/78 [00:04<00:02, 15.03it/s][A
Iteration:  55%|#####5    | 43/78 [00:04<00:02, 16.83it/s][A
Iteration:  59%|#####8    | 46/78 [00:05<00:01, 18.24it/s][A05/28/2023 14:00:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:17 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:00:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.72it/s]
05/28/2023 14:00:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:17 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:00:17 - INFO - __main__ -    dev: eval_loss = 0.6920285026232401
05/28/2023 14:00:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:17 - INFO - __main__ -    dev: infer_time = 4.224333333333333
05/28/2023 14:00:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:17 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:00:17 - INFO - __main__ -     cls_loss = 0.69403860155417
05/28/2023 14:00:17 - INFO - __main__ -     eval_loss = 0.6920285026232401
05/28/2023 14:00:17 - INFO - __main__ -     global_step = 49
05/28/2023 14:00:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:17 - INFO - __main__ -     infer_time = 4.224333333333333
05/28/2023 14:00:17 - INFO - __main__ -     loss = 0.69403860155417
05/28/2023 14:00:17 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  63%|######2   | 49/78 [00:06<00:05,  5.21it/s][A
Iteration:  65%|######5   | 51/78 [00:06<00:04,  6.25it/s][A
Iteration:  69%|######9   | 54/78 [00:06<00:02,  8.08it/s][A
Iteration:  73%|#######3  | 57/78 [00:06<00:02, 10.13it/s][A05/28/2023 14:00:19 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:19 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:00:19 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:19 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.85it/s]
05/28/2023 14:00:19 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:19 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:19 - INFO - __main__ -    dev: eval_loss = 0.6912554568714566
05/28/2023 14:00:19 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:19 - INFO - __main__ -    dev: infer_time = 4.225666666666666
05/28/2023 14:00:19 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:19 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:19 - INFO - __main__ -     cls_loss = 0.6936753634679116
05/28/2023 14:00:19 - INFO - __main__ -     eval_loss = 0.6912554568714566
05/28/2023 14:00:19 - INFO - __main__ -     global_step = 59
05/28/2023 14:00:19 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:19 - INFO - __main__ -     infer_time = 4.225666666666666
05/28/2023 14:00:19 - INFO - __main__ -     loss = 0.6936753634679116

Iteration:  76%|#######5  | 59/78 [00:07<00:01, 10.59it/s][A
Iteration:  78%|#######8  | 61/78 [00:07<00:01, 11.85it/s][A
Iteration:  82%|########2 | 64/78 [00:07<00:00, 14.17it/s][A
Iteration:  86%|########5 | 67/78 [00:07<00:00, 16.07it/s][A05/28/2023 14:00:20 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:20 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:00:20 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:20 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.75it/s]
05/28/2023 14:00:20 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:20 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:20 - INFO - __main__ -    dev: eval_loss = 0.6912571986516317
05/28/2023 14:00:20 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:20 - INFO - __main__ -    dev: infer_time = 4.196222222222222
05/28/2023 14:00:20 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:20 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:20 - INFO - __main__ -     cls_loss = 0.6940261004627615
05/28/2023 14:00:20 - INFO - __main__ -     eval_loss = 0.6912571986516317
05/28/2023 14:00:20 - INFO - __main__ -     global_step = 69
05/28/2023 14:00:20 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:20 - INFO - __main__ -     infer_time = 4.196222222222222
05/28/2023 14:00:20 - INFO - __main__ -     loss = 0.6940261004627615

Iteration:  88%|########8 | 69/78 [00:07<00:00, 14.78it/s][A
Iteration:  91%|#########1| 71/78 [00:07<00:00, 15.65it/s][A
Iteration:  95%|#########4| 74/78 [00:07<00:00, 17.27it/s][A
Iteration:  99%|#########8| 77/78 [00:08<00:00, 18.47it/s][AIteration: 100%|##########| 78/78 [00:08<00:00,  9.74it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.01s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.01s/it]
05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   w_emb: 5127696

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   p_emb: 86016

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   t_emb: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_emb: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118440

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 118776

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118440

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 118776

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118440

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 118776

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118440

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 118776

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 118440

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 118976

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   output_numel: 118776

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   layer_numel: 1758280
05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   dense_numel: 28392
05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   emb_numel: 5214384

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   encoder_numel: 1758280

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   pooler_numel: 28392

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   all parameters: 7001056

05/28/2023 14:00:20 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:20 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 168, 'sample_intermediate_sizes': [704, 704, 704, 704, 704], 'sample_qkv_sizes': [168, 168, 168, 168, 168]}
parameter size = 7001056
best_acc = 0.5306859205776173
time_per_batch_infer = 4.217 ms
infer_cnt = 63
**************E*************

05/28/2023 14:00:20 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [800, 800, 800], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
05/28/2023 14:00:20 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:00:20 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:00:20 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:20 - INFO - __main__ -   guid: train-0
05/28/2023 14:00:20 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:00:20 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:20 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:20 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:22 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:00:22 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:22 - INFO - __main__ -   guid: dev-0
05/28/2023 14:00:22 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:00:22 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:22 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:22 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:22 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:22 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:22 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:00:22 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:00:22 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:00:22 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:00:22 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:00:22 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:00:22 - INFO - __main__ -     Batch size = 32
05/28/2023 14:00:22 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.68it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.43it/s][A05/28/2023 14:00:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:23 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:00:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.39it/s]
05/28/2023 14:00:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:23 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:00:23 - INFO - __main__ -    dev: eval_loss = 0.6925547122955322
05/28/2023 14:00:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:23 - INFO - __main__ -    dev: infer_time = 2.8276666666666666
05/28/2023 14:00:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:23 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:00:23 - INFO - __main__ -     cls_loss = 0.6960393455293443
05/28/2023 14:00:23 - INFO - __main__ -     eval_loss = 0.6925547122955322
05/28/2023 14:00:23 - INFO - __main__ -     global_step = 9
05/28/2023 14:00:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:23 - INFO - __main__ -     infer_time = 2.8276666666666666
05/28/2023 14:00:23 - INFO - __main__ -     loss = 0.6960393455293443
05/28/2023 14:00:23 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.91it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.78it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  7.88it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.14it/s][A05/28/2023 14:00:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:25 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:00:25 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.39it/s]
05/28/2023 14:00:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:25 - INFO - __main__ -    dev: acc = 0.516245487364621
05/28/2023 14:00:25 - INFO - __main__ -    dev: eval_loss = 0.6930012106895447
05/28/2023 14:00:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:25 - INFO - __main__ -    dev: infer_time = 2.8303333333333334
05/28/2023 14:00:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:25 - INFO - __main__ -     acc = 0.516245487364621
05/28/2023 14:00:25 - INFO - __main__ -     cls_loss = 0.6985836687840914
05/28/2023 14:00:25 - INFO - __main__ -     eval_loss = 0.6930012106895447
05/28/2023 14:00:25 - INFO - __main__ -     global_step = 19
05/28/2023 14:00:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:25 - INFO - __main__ -     infer_time = 2.8303333333333334
05/28/2023 14:00:25 - INFO - __main__ -     loss = 0.6985836687840914

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.05it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 13.23it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.24it/s][A05/28/2023 14:00:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:25 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:00:25 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.34it/s]
05/28/2023 14:00:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:25 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:25 - INFO - __main__ -    dev: eval_loss = 0.6914262506696913
05/28/2023 14:00:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:25 - INFO - __main__ -    dev: infer_time = 2.8581111111111115
05/28/2023 14:00:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:25 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:25 - INFO - __main__ -     cls_loss = 0.6983437620360275
05/28/2023 14:00:25 - INFO - __main__ -     eval_loss = 0.6914262506696913
05/28/2023 14:00:25 - INFO - __main__ -     global_step = 29
05/28/2023 14:00:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:25 - INFO - __main__ -     infer_time = 2.8581111111111115
05/28/2023 14:00:25 - INFO - __main__ -     loss = 0.6983437620360275
05/28/2023 14:00:25 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:09,  4.99it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:06,  6.55it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:05,  8.35it/s][A05/28/2023 14:00:27 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:27 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:00:27 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:27 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.35it/s]
05/28/2023 14:00:27 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:27 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:27 - INFO - __main__ -    dev: eval_loss = 0.6918147669898139
05/28/2023 14:00:27 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:27 - INFO - __main__ -    dev: infer_time = 2.8508888888888886
05/28/2023 14:00:27 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:27 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:27 - INFO - __main__ -     cls_loss = 0.6969514305774982
05/28/2023 14:00:27 - INFO - __main__ -     eval_loss = 0.6918147669898139
05/28/2023 14:00:27 - INFO - __main__ -     global_step = 39
05/28/2023 14:00:27 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:27 - INFO - __main__ -     infer_time = 2.8508888888888886
05/28/2023 14:00:27 - INFO - __main__ -     loss = 0.6969514305774982

Iteration:  50%|#####     | 39/78 [00:04<00:04,  9.52it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:03, 11.54it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:02, 13.59it/s][A
Iteration:  62%|######1   | 48/78 [00:05<00:01, 15.50it/s][A05/28/2023 14:00:28 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:28 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:00:28 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:28 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.36it/s]
05/28/2023 14:00:28 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:28 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:28 - INFO - __main__ -    dev: eval_loss = 0.6983350184228685
05/28/2023 14:00:28 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:28 - INFO - __main__ -    dev: infer_time = 2.8465555555555557
05/28/2023 14:00:28 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:28 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:28 - INFO - __main__ -     cls_loss = 0.6965184357701516
05/28/2023 14:00:28 - INFO - __main__ -     eval_loss = 0.6983350184228685
05/28/2023 14:00:28 - INFO - __main__ -     global_step = 49
05/28/2023 14:00:28 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:28 - INFO - __main__ -     infer_time = 2.8465555555555557
05/28/2023 14:00:28 - INFO - __main__ -     loss = 0.6965184357701516

Iteration:  65%|######5   | 51/78 [00:05<00:01, 15.01it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 16.78it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 18.25it/s][A05/28/2023 14:00:28 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:28 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:00:28 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:28 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.27it/s]
05/28/2023 14:00:28 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:28 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:28 - INFO - __main__ -    dev: eval_loss = 0.7020853029357063
05/28/2023 14:00:28 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:28 - INFO - __main__ -    dev: infer_time = 2.852555555555556
05/28/2023 14:00:28 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:28 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:28 - INFO - __main__ -     cls_loss = 0.6949681043624878
05/28/2023 14:00:28 - INFO - __main__ -     eval_loss = 0.7020853029357063
05/28/2023 14:00:28 - INFO - __main__ -     global_step = 59
05/28/2023 14:00:28 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:28 - INFO - __main__ -     infer_time = 2.852555555555556
05/28/2023 14:00:28 - INFO - __main__ -     loss = 0.6949681043624878

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 16.53it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 18.06it/s][A
Iteration:  85%|########4 | 66/78 [00:06<00:00, 19.40it/s][A05/28/2023 14:00:29 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:29 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:00:29 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:29 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.53it/s]
05/28/2023 14:00:29 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:29 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:29 - INFO - __main__ -    dev: eval_loss = 0.6987610326872932
05/28/2023 14:00:29 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:29 - INFO - __main__ -    dev: infer_time = 2.857111111111111
05/28/2023 14:00:29 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:29 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:29 - INFO - __main__ -     cls_loss = 0.6951504280601722
05/28/2023 14:00:29 - INFO - __main__ -     eval_loss = 0.6987610326872932
05/28/2023 14:00:29 - INFO - __main__ -     global_step = 69
05/28/2023 14:00:29 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:29 - INFO - __main__ -     infer_time = 2.857111111111111
05/28/2023 14:00:29 - INFO - __main__ -     loss = 0.6951504280601722

Iteration:  88%|########8 | 69/78 [00:06<00:00, 17.34it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 18.75it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 19.49it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.84it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.59s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.59s/it]
05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   w_emb: 14284296

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   p_emb: 239616

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   t_emb: 936

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   ln_emb: 936

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 375200

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 374868

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   intermediate_numel: 375200

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   output_numel: 375804

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 375200

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 374868

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   intermediate_numel: 375200

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   output_numel: 375804

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 375200

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 374868

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   intermediate_numel: 375200

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   output_numel: 375804

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   layer_numel: 4889724
05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   dense_numel: 219492
05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   emb_numel: 14525784

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   encoder_numel: 4889724

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   pooler_numel: 219492

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   all parameters: 19635000

05/28/2023 14:00:29 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:29 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [800, 800, 800], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
parameter size = 19635000
best_acc = 0.5270758122743683
time_per_batch_infer = 2.846 ms
infer_cnt = 63
**************E*************

05/28/2023 14:00:29 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 444, 'sample_intermediate_sizes': [736, 736, 736], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [444, 444, 444]}
05/28/2023 14:00:29 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:00:29 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:00:29 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:29 - INFO - __main__ -   guid: train-0
05/28/2023 14:00:29 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:00:29 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:29 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:29 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:31 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:00:31 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:31 - INFO - __main__ -   guid: dev-0
05/28/2023 14:00:31 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:00:31 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:31 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:31 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:31 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:31 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:31 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:00:31 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:00:32 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:00:32 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:00:32 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:00:32 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:00:32 - INFO - __main__ -     Batch size = 32
05/28/2023 14:00:32 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.30it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.38it/s][A05/28/2023 14:00:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:32 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:00:32 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.67it/s]
05/28/2023 14:00:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:32 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:32 - INFO - __main__ -    dev: eval_loss = 0.7078085210588243
05/28/2023 14:00:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:32 - INFO - __main__ -    dev: infer_time = 2.866444444444445
05/28/2023 14:00:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:32 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:32 - INFO - __main__ -     cls_loss = 0.696554786629147
05/28/2023 14:00:32 - INFO - __main__ -     eval_loss = 0.7078085210588243
05/28/2023 14:00:32 - INFO - __main__ -     global_step = 9
05/28/2023 14:00:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:32 - INFO - __main__ -     infer_time = 2.866444444444445
05/28/2023 14:00:32 - INFO - __main__ -     loss = 0.696554786629147
05/28/2023 14:00:32 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.98it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.89it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.06it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.41it/s][A05/28/2023 14:00:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:34 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:00:34 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.93it/s]
05/28/2023 14:00:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:34 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:34 - INFO - __main__ -    dev: eval_loss = 0.6929691500133939
05/28/2023 14:00:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:34 - INFO - __main__ -    dev: infer_time = 2.8468888888888886
05/28/2023 14:00:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:34 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:34 - INFO - __main__ -     cls_loss = 0.6988574203691984
05/28/2023 14:00:34 - INFO - __main__ -     eval_loss = 0.6929691500133939
05/28/2023 14:00:34 - INFO - __main__ -     global_step = 19
05/28/2023 14:00:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:34 - INFO - __main__ -     infer_time = 2.8468888888888886
05/28/2023 14:00:34 - INFO - __main__ -     loss = 0.6988574203691984
05/28/2023 14:00:34 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.42it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.95it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.78it/s][A05/28/2023 14:00:36 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:36 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:00:36 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:36 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.89it/s]
05/28/2023 14:00:36 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:36 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:36 - INFO - __main__ -    dev: eval_loss = 0.6921146975623237
05/28/2023 14:00:36 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:36 - INFO - __main__ -    dev: infer_time = 2.8586666666666667
05/28/2023 14:00:36 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:36 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:36 - INFO - __main__ -     cls_loss = 0.6963340681174706
05/28/2023 14:00:36 - INFO - __main__ -     eval_loss = 0.6921146975623237
05/28/2023 14:00:36 - INFO - __main__ -     global_step = 29
05/28/2023 14:00:36 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:36 - INFO - __main__ -     infer_time = 2.8586666666666667
05/28/2023 14:00:36 - INFO - __main__ -     loss = 0.6963340681174706

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.38it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.65it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.93it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 15.07it/s][A05/28/2023 14:00:36 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:36 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:00:36 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:36 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.91it/s]
05/28/2023 14:00:36 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:36 - INFO - __main__ -    dev: acc = 0.49458483754512633
05/28/2023 14:00:36 - INFO - __main__ -    dev: eval_loss = 0.6932086216078864
05/28/2023 14:00:36 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:36 - INFO - __main__ -    dev: infer_time = 2.8431111111111114
05/28/2023 14:00:36 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:36 - INFO - __main__ -     acc = 0.49458483754512633
05/28/2023 14:00:36 - INFO - __main__ -     cls_loss = 0.6960536562479459
05/28/2023 14:00:36 - INFO - __main__ -     eval_loss = 0.6932086216078864
05/28/2023 14:00:36 - INFO - __main__ -     global_step = 39
05/28/2023 14:00:36 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:36 - INFO - __main__ -     infer_time = 2.8431111111111114
05/28/2023 14:00:36 - INFO - __main__ -     loss = 0.6960536562479459

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.86it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.72it/s][A
Iteration:  60%|######    | 47/78 [00:04<00:01, 18.42it/s][A05/28/2023 14:00:37 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:37 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:00:37 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:37 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.21it/s]
05/28/2023 14:00:37 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:37 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:37 - INFO - __main__ -    dev: eval_loss = 0.6928914917839898
05/28/2023 14:00:37 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:37 - INFO - __main__ -    dev: infer_time = 2.931111111111111
05/28/2023 14:00:37 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:37 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:37 - INFO - __main__ -     cls_loss = 0.6956749254343461
05/28/2023 14:00:37 - INFO - __main__ -     eval_loss = 0.6928914917839898
05/28/2023 14:00:37 - INFO - __main__ -     global_step = 49
05/28/2023 14:00:37 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:37 - INFO - __main__ -     infer_time = 2.931111111111111
05/28/2023 14:00:37 - INFO - __main__ -     loss = 0.6956749254343461

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.99it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 18.60it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 19.91it/s][A05/28/2023 14:00:37 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:37 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:00:37 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:37 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.99it/s]
05/28/2023 14:00:37 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:37 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:37 - INFO - __main__ -    dev: eval_loss = 0.6920318007469177
05/28/2023 14:00:37 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:37 - INFO - __main__ -    dev: infer_time = 2.858333333333333
05/28/2023 14:00:37 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:37 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:37 - INFO - __main__ -     cls_loss = 0.6954254124124172
05/28/2023 14:00:37 - INFO - __main__ -     eval_loss = 0.6920318007469177
05/28/2023 14:00:37 - INFO - __main__ -     global_step = 59
05/28/2023 14:00:37 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:37 - INFO - __main__ -     infer_time = 2.858333333333333
05/28/2023 14:00:37 - INFO - __main__ -     loss = 0.6954254124124172

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 17.93it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:00, 19.31it/s][A
Iteration:  83%|########3 | 65/78 [00:05<00:00, 20.34it/s][A
Iteration:  87%|########7 | 68/78 [00:05<00:00, 21.40it/s][A05/28/2023 14:00:38 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:38 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:00:38 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:38 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.98it/s]
05/28/2023 14:00:38 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:38 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:38 - INFO - __main__ -    dev: eval_loss = 0.6925546990500556
05/28/2023 14:00:38 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:38 - INFO - __main__ -    dev: infer_time = 2.855111111111111
05/28/2023 14:00:38 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:38 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:38 - INFO - __main__ -     cls_loss = 0.6952025052429973
05/28/2023 14:00:38 - INFO - __main__ -     eval_loss = 0.6925546990500556
05/28/2023 14:00:38 - INFO - __main__ -     global_step = 69
05/28/2023 14:00:38 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:38 - INFO - __main__ -     infer_time = 2.855111111111111
05/28/2023 14:00:38 - INFO - __main__ -     loss = 0.6952025052429973

Iteration:  91%|#########1| 71/78 [00:06<00:00, 18.77it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 20.03it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 21.01it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.17it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.41s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.41s/it]
05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   w_emb: 13551768

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   p_emb: 227328

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   t_emb: 888

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   ln_emb: 888

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   query_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   key_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   value_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   ln_numel: 888

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   self_numel: 592740

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   output_numel: 198468

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 327520

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 327228

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   ln_numel: 888

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   attention_numel: 791208

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   intermediate_numel: 327520

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   output_numel: 328116

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   query_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   key_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   value_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   ln_numel: 888

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   self_numel: 592740

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   output_numel: 198468

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 327520

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 327228

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   ln_numel: 888

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   attention_numel: 791208

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   intermediate_numel: 327520

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   output_numel: 328116

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   query_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   key_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   value_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   ln_numel: 888

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   self_numel: 592740

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   output_numel: 198468

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 327520

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 327228

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   ln_numel: 888

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   attention_numel: 791208

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   intermediate_numel: 327520

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   output_numel: 328116

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   layer_numel: 4340532
05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   dense_numel: 197580
05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   emb_numel: 13780872

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   encoder_numel: 4340532

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   pooler_numel: 197580

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   all parameters: 18318984

05/28/2023 14:00:38 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:38 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 444, 'sample_intermediate_sizes': [736, 736, 736], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [444, 444, 444]}
parameter size = 18318984
best_acc = 0.5270758122743683
time_per_batch_infer = 2.866 ms
infer_cnt = 63
**************E*************

05/28/2023 14:00:38 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 204, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_qkv_sizes': [204, 204, 204, 204, 204]}
05/28/2023 14:00:38 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:00:38 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:00:38 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:38 - INFO - __main__ -   guid: train-0
05/28/2023 14:00:38 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:00:38 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:38 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:38 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:38 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:40 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:00:40 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:40 - INFO - __main__ -   guid: dev-0
05/28/2023 14:00:40 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:00:40 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:40 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:40 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:40 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:40 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:40 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:00:40 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:00:41 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:00:41 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:00:41 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:00:41 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:00:41 - INFO - __main__ -     Batch size = 32
05/28/2023 14:00:41 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 20.75it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.69it/s][A05/28/2023 14:00:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:41 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:00:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.29it/s]
05/28/2023 14:00:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:41 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:41 - INFO - __main__ -    dev: eval_loss = 0.6915708515379164
05/28/2023 14:00:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:41 - INFO - __main__ -    dev: infer_time = 4.181111111111111
05/28/2023 14:00:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:41 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:41 - INFO - __main__ -     cls_loss = 0.6943465405040317
05/28/2023 14:00:41 - INFO - __main__ -     eval_loss = 0.6915708515379164
05/28/2023 14:00:41 - INFO - __main__ -     global_step = 9
05/28/2023 14:00:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:41 - INFO - __main__ -     infer_time = 4.181111111111111
05/28/2023 14:00:41 - INFO - __main__ -     loss = 0.6943465405040317
05/28/2023 14:00:41 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.96it/s][A
Iteration:  14%|#4        | 11/78 [00:01<00:12,  5.21it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:08,  7.48it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:06,  9.87it/s][A05/28/2023 14:00:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:43 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:00:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.26it/s]
05/28/2023 14:00:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:43 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:00:43 - INFO - __main__ -    dev: eval_loss = 0.6921843820148044
05/28/2023 14:00:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:43 - INFO - __main__ -    dev: infer_time = 4.248555555555555
05/28/2023 14:00:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:43 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:00:43 - INFO - __main__ -     cls_loss = 0.6934529197843451
05/28/2023 14:00:43 - INFO - __main__ -     eval_loss = 0.6921843820148044
05/28/2023 14:00:43 - INFO - __main__ -     global_step = 19
05/28/2023 14:00:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:43 - INFO - __main__ -     infer_time = 4.248555555555555
05/28/2023 14:00:43 - INFO - __main__ -     loss = 0.6934529197843451

Iteration:  24%|##4       | 19/78 [00:02<00:05, 10.54it/s][A
Iteration:  28%|##8       | 22/78 [00:02<00:04, 13.02it/s][A
Iteration:  32%|###2      | 25/78 [00:02<00:03, 15.22it/s][A
Iteration:  36%|###5      | 28/78 [00:02<00:02, 17.15it/s][A05/28/2023 14:00:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:43 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:00:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.99it/s]
05/28/2023 14:00:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:43 - INFO - __main__ -    dev: acc = 0.48736462093862815
05/28/2023 14:00:43 - INFO - __main__ -    dev: eval_loss = 0.6939649515681796
05/28/2023 14:00:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:43 - INFO - __main__ -    dev: infer_time = 4.221222222222222
05/28/2023 14:00:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:43 - INFO - __main__ -     acc = 0.48736462093862815
05/28/2023 14:00:43 - INFO - __main__ -     cls_loss = 0.6944452339205248
05/28/2023 14:00:43 - INFO - __main__ -     eval_loss = 0.6939649515681796
05/28/2023 14:00:43 - INFO - __main__ -     global_step = 29
05/28/2023 14:00:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:43 - INFO - __main__ -     infer_time = 4.221222222222222
05/28/2023 14:00:43 - INFO - __main__ -     loss = 0.6944452339205248

Iteration:  40%|###9      | 31/78 [00:02<00:02, 16.52it/s][A
Iteration:  44%|####3     | 34/78 [00:03<00:02, 18.13it/s][A
Iteration:  47%|####7     | 37/78 [00:03<00:02, 19.40it/s][A05/28/2023 14:00:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:44 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:00:44 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.93it/s]
05/28/2023 14:00:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:44 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:44 - INFO - __main__ -    dev: eval_loss = 0.6914311978552077
05/28/2023 14:00:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:44 - INFO - __main__ -    dev: infer_time = 4.222777777777778
05/28/2023 14:00:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:44 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:44 - INFO - __main__ -     cls_loss = 0.6944419069167895
05/28/2023 14:00:44 - INFO - __main__ -     eval_loss = 0.6914311978552077
05/28/2023 14:00:44 - INFO - __main__ -     global_step = 39
05/28/2023 14:00:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:44 - INFO - __main__ -     infer_time = 4.222777777777778
05/28/2023 14:00:44 - INFO - __main__ -     loss = 0.6944419069167895

Iteration:  51%|#####1    | 40/78 [00:03<00:02, 17.38it/s][A
Iteration:  55%|#####5    | 43/78 [00:03<00:01, 18.77it/s][A
Iteration:  59%|#####8    | 46/78 [00:03<00:01, 19.88it/s][A05/28/2023 14:00:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:44 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:00:44 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.93it/s]
05/28/2023 14:00:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:44 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:00:44 - INFO - __main__ -    dev: eval_loss = 0.6919354995091757
05/28/2023 14:00:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:44 - INFO - __main__ -    dev: infer_time = 4.223444444444444
05/28/2023 14:00:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:44 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:00:44 - INFO - __main__ -     cls_loss = 0.6941123884551379
05/28/2023 14:00:44 - INFO - __main__ -     eval_loss = 0.6919354995091757
05/28/2023 14:00:44 - INFO - __main__ -     global_step = 49
05/28/2023 14:00:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:44 - INFO - __main__ -     infer_time = 4.223444444444444
05/28/2023 14:00:44 - INFO - __main__ -     loss = 0.6941123884551379
05/28/2023 14:00:44 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  63%|######2   | 49/78 [00:05<00:05,  5.42it/s][A
Iteration:  65%|######5   | 51/78 [00:05<00:04,  6.41it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:02,  8.31it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:02, 10.41it/s][A05/28/2023 14:00:46 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:46 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:00:46 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:46 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.48it/s]
05/28/2023 14:00:46 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:46 - INFO - __main__ -    dev: acc = 0.5415162454873647
05/28/2023 14:00:46 - INFO - __main__ -    dev: eval_loss = 0.6921346055136787
05/28/2023 14:00:46 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:46 - INFO - __main__ -    dev: infer_time = 4.216888888888889
05/28/2023 14:00:46 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:46 - INFO - __main__ -     acc = 0.5415162454873647
05/28/2023 14:00:46 - INFO - __main__ -     cls_loss = 0.6938184560355494
05/28/2023 14:00:46 - INFO - __main__ -     eval_loss = 0.6921346055136787
05/28/2023 14:00:46 - INFO - __main__ -     global_step = 59
05/28/2023 14:00:46 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:46 - INFO - __main__ -     infer_time = 4.216888888888889
05/28/2023 14:00:46 - INFO - __main__ -     loss = 0.6938184560355494
05/28/2023 14:00:46 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  76%|#######5  | 59/78 [00:06<00:04,  4.13it/s][A
Iteration:  79%|#######9  | 62/78 [00:07<00:02,  5.63it/s][A
Iteration:  83%|########3 | 65/78 [00:07<00:01,  7.44it/s][A
Iteration:  87%|########7 | 68/78 [00:07<00:01,  9.46it/s][A05/28/2023 14:00:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:48 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:00:48 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.74it/s]
05/28/2023 14:00:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:48 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 14:00:48 - INFO - __main__ -    dev: eval_loss = 0.6924999952316284
05/28/2023 14:00:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:48 - INFO - __main__ -    dev: infer_time = 4.144222222222222
05/28/2023 14:00:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:48 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 14:00:48 - INFO - __main__ -     cls_loss = 0.6936030025067537
05/28/2023 14:00:48 - INFO - __main__ -     eval_loss = 0.6924999952316284
05/28/2023 14:00:48 - INFO - __main__ -     global_step = 69
05/28/2023 14:00:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:48 - INFO - __main__ -     infer_time = 4.144222222222222
05/28/2023 14:00:48 - INFO - __main__ -     loss = 0.6936030025067537

Iteration:  90%|########9 | 70/78 [00:07<00:00,  9.96it/s][A
Iteration:  94%|#########3| 73/78 [00:07<00:00, 12.09it/s][A
Iteration:  97%|#########7| 76/78 [00:07<00:00, 14.28it/s][AIteration: 100%|##########| 78/78 [00:07<00:00,  9.98it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.82s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.82s/it]
05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   w_emb: 6226488

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   p_emb: 104448

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   t_emb: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_emb: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   layer_numel: 1626780
05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   dense_numel: 41820
05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   emb_numel: 6331752

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   encoder_numel: 1626780

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   pooler_numel: 41820

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   all parameters: 8000352

05/28/2023 14:00:48 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:48 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 204, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_qkv_sizes': [204, 204, 204, 204, 204]}
parameter size = 8000352
best_acc = 0.5415162454873647
time_per_batch_infer = 4.208 ms
infer_cnt = 63
**************E*************

05/28/2023 14:00:48 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
05/28/2023 14:00:48 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:00:48 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:00:48 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:48 - INFO - __main__ -   guid: train-0
05/28/2023 14:00:48 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:00:48 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:48 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:48 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:50 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:00:50 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:50 - INFO - __main__ -   guid: dev-0
05/28/2023 14:00:50 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:00:50 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:50 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:50 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:50 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:50 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:00:50 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:00:51 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:00:51 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:00:51 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:00:51 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:00:51 - INFO - __main__ -     Batch size = 32
05/28/2023 14:00:51 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.46it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.82it/s][A05/28/2023 14:00:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:51 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:00:51 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.68it/s]
05/28/2023 14:00:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:51 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:00:51 - INFO - __main__ -    dev: eval_loss = 0.6936109397146437
05/28/2023 14:00:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:51 - INFO - __main__ -    dev: infer_time = 2.783777777777778
05/28/2023 14:00:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:51 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:00:51 - INFO - __main__ -     cls_loss = 0.6939905020925734
05/28/2023 14:00:51 - INFO - __main__ -     eval_loss = 0.6936109397146437
05/28/2023 14:00:51 - INFO - __main__ -     global_step = 9
05/28/2023 14:00:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:51 - INFO - __main__ -     infer_time = 2.783777777777778
05/28/2023 14:00:51 - INFO - __main__ -     loss = 0.6939905020925734
05/28/2023 14:00:51 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.74it/s][A
Iteration:  15%|#5        | 12/78 [00:02<00:11,  5.55it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.63it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.86it/s][A05/28/2023 14:00:53 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:53 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:00:53 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:53 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.52it/s]
05/28/2023 14:00:53 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:53 - INFO - __main__ -    dev: acc = 0.5487364620938628
05/28/2023 14:00:53 - INFO - __main__ -    dev: eval_loss = 0.6920802328321669
05/28/2023 14:00:53 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:53 - INFO - __main__ -    dev: infer_time = 2.8343333333333334
05/28/2023 14:00:53 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:53 - INFO - __main__ -     acc = 0.5487364620938628
05/28/2023 14:00:53 - INFO - __main__ -     cls_loss = 0.6915668249130249
05/28/2023 14:00:53 - INFO - __main__ -     eval_loss = 0.6920802328321669
05/28/2023 14:00:53 - INFO - __main__ -     global_step = 19
05/28/2023 14:00:53 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:53 - INFO - __main__ -     infer_time = 2.8343333333333334
05/28/2023 14:00:53 - INFO - __main__ -     loss = 0.6915668249130249
05/28/2023 14:00:53 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.22it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.69it/s][A
Iteration:  35%|###4      | 27/78 [00:04<00:06,  7.43it/s][A05/28/2023 14:00:55 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:55 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:00:55 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:55 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.41it/s]
05/28/2023 14:00:55 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:55 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:55 - INFO - __main__ -    dev: eval_loss = 0.7095873554547628
05/28/2023 14:00:55 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:55 - INFO - __main__ -    dev: infer_time = 2.8240000000000003
05/28/2023 14:00:55 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:55 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:55 - INFO - __main__ -     cls_loss = 0.6914785607107754
05/28/2023 14:00:55 - INFO - __main__ -     eval_loss = 0.7095873554547628
05/28/2023 14:00:55 - INFO - __main__ -     global_step = 29
05/28/2023 14:00:55 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:55 - INFO - __main__ -     infer_time = 2.8240000000000003
05/28/2023 14:00:55 - INFO - __main__ -     loss = 0.6914785607107754

Iteration:  37%|###7      | 29/78 [00:04<00:06,  8.10it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.28it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.53it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.59it/s][A05/28/2023 14:00:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:56 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:00:56 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.59it/s]
05/28/2023 14:00:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:56 - INFO - __main__ -    dev: acc = 0.5054151624548736
05/28/2023 14:00:56 - INFO - __main__ -    dev: eval_loss = 0.692580779393514
05/28/2023 14:00:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:56 - INFO - __main__ -    dev: infer_time = 2.8173333333333326
05/28/2023 14:00:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:56 - INFO - __main__ -     acc = 0.5054151624548736
05/28/2023 14:00:56 - INFO - __main__ -     cls_loss = 0.6945992463674301
05/28/2023 14:00:56 - INFO - __main__ -     eval_loss = 0.692580779393514
05/28/2023 14:00:56 - INFO - __main__ -     global_step = 39
05/28/2023 14:00:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:56 - INFO - __main__ -     infer_time = 2.8173333333333326
05/28/2023 14:00:56 - INFO - __main__ -     loss = 0.6945992463674301

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.31it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.15it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.68it/s][A05/28/2023 14:00:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:56 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:00:56 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.45it/s]
05/28/2023 14:00:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:56 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 14:00:56 - INFO - __main__ -    dev: eval_loss = 0.6924684643745422
05/28/2023 14:00:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:56 - INFO - __main__ -    dev: infer_time = 2.8388888888888886
05/28/2023 14:00:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:56 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 14:00:56 - INFO - __main__ -     cls_loss = 0.6948548555374146
05/28/2023 14:00:56 - INFO - __main__ -     eval_loss = 0.6924684643745422
05/28/2023 14:00:56 - INFO - __main__ -     global_step = 49
05/28/2023 14:00:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:56 - INFO - __main__ -     infer_time = 2.8388888888888886
05/28/2023 14:00:56 - INFO - __main__ -     loss = 0.6948548555374146

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.25it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 17.80it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 19.07it/s][A05/28/2023 14:00:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:57 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:00:57 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.43it/s]
05/28/2023 14:00:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:57 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:00:57 - INFO - __main__ -    dev: eval_loss = 0.6933125456174215
05/28/2023 14:00:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:57 - INFO - __main__ -    dev: infer_time = 2.8003333333333336
05/28/2023 14:00:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:57 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:00:57 - INFO - __main__ -     cls_loss = 0.6947984452974998
05/28/2023 14:00:57 - INFO - __main__ -     eval_loss = 0.6933125456174215
05/28/2023 14:00:57 - INFO - __main__ -     global_step = 59
05/28/2023 14:00:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:57 - INFO - __main__ -     infer_time = 2.8003333333333336
05/28/2023 14:00:57 - INFO - __main__ -     loss = 0.6947984452974998

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 17.11it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:00, 18.32it/s][A
Iteration:  83%|########3 | 65/78 [00:06<00:00, 19.52it/s][A
Iteration:  87%|########7 | 68/78 [00:06<00:00, 20.06it/s][A05/28/2023 14:00:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:00:57 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:00:57 - INFO - __main__ -     Num examples = 277
05/28/2023 14:00:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.35it/s]
05/28/2023 14:00:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:00:57 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:00:57 - INFO - __main__ -    dev: eval_loss = 0.6953127649095323
05/28/2023 14:00:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:00:57 - INFO - __main__ -    dev: infer_time = 2.8219999999999996
05/28/2023 14:00:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:00:57 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:00:57 - INFO - __main__ -     cls_loss = 0.6946565560672594
05/28/2023 14:00:57 - INFO - __main__ -     eval_loss = 0.6953127649095323
05/28/2023 14:00:57 - INFO - __main__ -     global_step = 69
05/28/2023 14:00:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:00:57 - INFO - __main__ -     infer_time = 2.8219999999999996
05/28/2023 14:00:57 - INFO - __main__ -     loss = 0.6946565560672594

Iteration:  91%|#########1| 71/78 [00:06<00:00, 17.39it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 18.75it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 19.75it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.60it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.72s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.72s/it]
05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   w_emb: 13918032

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   p_emb: 233472

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   t_emb: 912

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   ln_emb: 912

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 394848

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 394440

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   intermediate_numel: 394848

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   output_numel: 395352

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 394848

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 394440

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   intermediate_numel: 394848

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   output_numel: 395352

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 394848

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 394440

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   intermediate_numel: 394848

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   output_numel: 395352

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   layer_numel: 4874040
05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   dense_numel: 208392
05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   emb_numel: 14153328

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   encoder_numel: 4874040

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   pooler_numel: 208392

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   all parameters: 19235760

05/28/2023 14:00:58 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:00:58 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
parameter size = 19235760
best_acc = 0.5487364620938628
time_per_batch_infer = 2.817 ms
infer_cnt = 63
**************E*************

05/28/2023 14:00:58 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [432, 432, 432, 432]}
05/28/2023 14:00:58 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:00:58 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:00:58 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:58 - INFO - __main__ -   guid: train-0
05/28/2023 14:00:58 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:00:58 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:58 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:58 - INFO - __main__ -   label_id: 1
05/28/2023 14:00:59 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:00:59 - INFO - __main__ -   *** Example ***
05/28/2023 14:00:59 - INFO - __main__ -   guid: dev-0
05/28/2023 14:00:59 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:00:59 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:00:59 - INFO - __main__ -   label: not_entailment
05/28/2023 14:00:59 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:00 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:01:00 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:01:00 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:01:00 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:01:00 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:01:00 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:01:00 - INFO - __main__ -     Batch size = 32
05/28/2023 14:01:00 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 20.96it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.24it/s][A05/28/2023 14:01:01 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:01 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:01:01 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:01 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.38it/s]
05/28/2023 14:01:01 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:01 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:01 - INFO - __main__ -    dev: eval_loss = 0.7389516830444336
05/28/2023 14:01:01 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:01 - INFO - __main__ -    dev: infer_time = 3.5334444444444446
05/28/2023 14:01:01 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:01 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:01 - INFO - __main__ -     cls_loss = 0.6935790710979037
05/28/2023 14:01:01 - INFO - __main__ -     eval_loss = 0.7389516830444336
05/28/2023 14:01:01 - INFO - __main__ -     global_step = 9
05/28/2023 14:01:01 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:01 - INFO - __main__ -     infer_time = 3.5334444444444446
05/28/2023 14:01:01 - INFO - __main__ -     loss = 0.6935790710979037
05/28/2023 14:01:01 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.89it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.72it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.77it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.91it/s][A05/28/2023 14:01:02 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:02 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:01:02 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:02 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.06it/s]
05/28/2023 14:01:02 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:02 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:02 - INFO - __main__ -    dev: eval_loss = 0.7011423110961914
05/28/2023 14:01:02 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:02 - INFO - __main__ -    dev: infer_time = 3.5217777777777775
05/28/2023 14:01:02 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:02 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:02 - INFO - __main__ -     cls_loss = 0.7049937938389025
05/28/2023 14:01:02 - INFO - __main__ -     eval_loss = 0.7011423110961914
05/28/2023 14:01:02 - INFO - __main__ -     global_step = 19
05/28/2023 14:01:02 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:02 - INFO - __main__ -     infer_time = 3.5217777777777775
05/28/2023 14:01:02 - INFO - __main__ -     loss = 0.7049937938389025
05/28/2023 14:01:02 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:15,  3.73it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.21it/s][A
Iteration:  33%|###3      | 26/78 [00:04<00:07,  6.94it/s][A05/28/2023 14:01:04 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:04 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:01:04 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:04 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.18it/s]
05/28/2023 14:01:04 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:04 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:04 - INFO - __main__ -    dev: eval_loss = 0.7081255846553378
05/28/2023 14:01:04 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:04 - INFO - __main__ -    dev: infer_time = 3.5223333333333335
05/28/2023 14:01:04 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:04 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:04 - INFO - __main__ -     cls_loss = 0.7038079623518319
05/28/2023 14:01:04 - INFO - __main__ -     eval_loss = 0.7081255846553378
05/28/2023 14:01:04 - INFO - __main__ -     global_step = 29
05/28/2023 14:01:04 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:04 - INFO - __main__ -     infer_time = 3.5223333333333335
05/28/2023 14:01:04 - INFO - __main__ -     loss = 0.7038079623518319

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.19it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.15it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.13it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.01it/s][A05/28/2023 14:01:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:05 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:01:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.53it/s]
05/28/2023 14:01:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:05 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:05 - INFO - __main__ -    dev: eval_loss = 0.6975359519322714
05/28/2023 14:01:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:05 - INFO - __main__ -    dev: infer_time = 3.523111111111111
05/28/2023 14:01:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:05 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:05 - INFO - __main__ -     cls_loss = 0.7026954828164517
05/28/2023 14:01:05 - INFO - __main__ -     eval_loss = 0.6975359519322714
05/28/2023 14:01:05 - INFO - __main__ -     global_step = 39
05/28/2023 14:01:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:05 - INFO - __main__ -     infer_time = 3.523111111111111
05/28/2023 14:01:05 - INFO - __main__ -     loss = 0.7026954828164517

Iteration:  51%|#####1    | 40/78 [00:04<00:02, 13.21it/s][A
Iteration:  55%|#####5    | 43/78 [00:05<00:02, 15.06it/s][A
Iteration:  59%|#####8    | 46/78 [00:05<00:01, 16.60it/s][A05/28/2023 14:01:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:05 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:01:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.57it/s]
05/28/2023 14:01:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:06 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:06 - INFO - __main__ -    dev: eval_loss = 0.6915310422579447
05/28/2023 14:01:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:06 - INFO - __main__ -    dev: infer_time = 3.5083333333333333
05/28/2023 14:01:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:06 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:06 - INFO - __main__ -     cls_loss = 0.7006382638094376
05/28/2023 14:01:06 - INFO - __main__ -     eval_loss = 0.6915310422579447
05/28/2023 14:01:06 - INFO - __main__ -     global_step = 49
05/28/2023 14:01:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:06 - INFO - __main__ -     infer_time = 3.5083333333333333
05/28/2023 14:01:06 - INFO - __main__ -     loss = 0.7006382638094376

Iteration:  63%|######2   | 49/78 [00:05<00:01, 15.41it/s][A
Iteration:  65%|######5   | 51/78 [00:05<00:01, 16.08it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 17.52it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 18.62it/s][A05/28/2023 14:01:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:06 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:01:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.30it/s]
05/28/2023 14:01:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:06 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:06 - INFO - __main__ -    dev: eval_loss = 0.692245344320933
05/28/2023 14:01:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:06 - INFO - __main__ -    dev: infer_time = 3.5188888888888887
05/28/2023 14:01:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:06 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:06 - INFO - __main__ -     cls_loss = 0.6997348757113441
05/28/2023 14:01:06 - INFO - __main__ -     eval_loss = 0.692245344320933
05/28/2023 14:01:06 - INFO - __main__ -     global_step = 59
05/28/2023 14:01:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:06 - INFO - __main__ -     infer_time = 3.5188888888888887
05/28/2023 14:01:06 - INFO - __main__ -     loss = 0.6997348757113441

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 16.27it/s][A
Iteration:  79%|#######9  | 62/78 [00:06<00:00, 17.60it/s][A
Iteration:  83%|########3 | 65/78 [00:06<00:00, 18.64it/s][A
Iteration:  87%|########7 | 68/78 [00:06<00:00, 19.36it/s][A05/28/2023 14:01:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:07 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:01:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.06it/s]
05/28/2023 14:01:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:07 - INFO - __main__ -    dev: eval_loss = 0.691593156920539
05/28/2023 14:01:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:07 - INFO - __main__ -    dev: infer_time = 3.8946666666666667
05/28/2023 14:01:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:07 - INFO - __main__ -     cls_loss = 0.6990917592808821
05/28/2023 14:01:07 - INFO - __main__ -     eval_loss = 0.691593156920539
05/28/2023 14:01:07 - INFO - __main__ -     global_step = 69
05/28/2023 14:01:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:07 - INFO - __main__ -     infer_time = 3.8946666666666667
05/28/2023 14:01:07 - INFO - __main__ -     loss = 0.6990917592808821

Iteration:  90%|########9 | 70/78 [00:06<00:00, 16.39it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 17.84it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 18.82it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.28it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.91s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.91s/it]
05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   w_emb: 13185504

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   p_emb: 221184

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   t_emb: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ln_emb: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 83136

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 83376

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83136

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   output_numel: 84240

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 83136

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 83376

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83136

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   output_numel: 84240

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 83136

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 83376

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83136

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   output_numel: 84240

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 83136

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 83376

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83136

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   output_numel: 84240

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   layer_numel: 3665856
05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   dense_numel: 187056
05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   emb_numel: 13408416

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   encoder_numel: 3665856

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   pooler_numel: 187056

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   all parameters: 17261328

05/28/2023 14:01:07 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:07 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [432, 432, 432, 432]}
parameter size = 17261328
best_acc = 0.5270758122743683
time_per_batch_infer = 3.575 ms
infer_cnt = 63
**************E*************

05/28/2023 14:01:07 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [448, 448, 448], 'sample_qkv_sizes': [456, 456, 456]}
05/28/2023 14:01:07 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:01:07 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:01:07 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:07 - INFO - __main__ -   guid: train-0
05/28/2023 14:01:07 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:01:07 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:07 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:07 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:07 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:07 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:09 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:01:09 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:09 - INFO - __main__ -   guid: dev-0
05/28/2023 14:01:09 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:01:09 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:09 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:09 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:09 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:09 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:01:09 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:01:09 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:01:10 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:01:10 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:01:10 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:01:10 - INFO - __main__ -     Batch size = 32
05/28/2023 14:01:10 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.16it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.09it/s][A05/28/2023 14:01:10 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:10 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:01:10 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:10 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.11it/s]
05/28/2023 14:01:10 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:10 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:10 - INFO - __main__ -    dev: eval_loss = 0.6897744602627225
05/28/2023 14:01:10 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:10 - INFO - __main__ -    dev: infer_time = 2.7692222222222225
05/28/2023 14:01:10 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:10 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:10 - INFO - __main__ -     cls_loss = 0.6966509487893846
05/28/2023 14:01:10 - INFO - __main__ -     eval_loss = 0.6897744602627225
05/28/2023 14:01:10 - INFO - __main__ -     global_step = 9
05/28/2023 14:01:10 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:10 - INFO - __main__ -     infer_time = 2.7692222222222225
05/28/2023 14:01:10 - INFO - __main__ -     loss = 0.6966509487893846
05/28/2023 14:01:10 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.97it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.91it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.16it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.55it/s][A05/28/2023 14:01:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:12 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:01:12 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.31it/s]
05/28/2023 14:01:12 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:12 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:01:12 - INFO - __main__ -    dev: eval_loss = 0.6918948425187005
05/28/2023 14:01:12 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:12 - INFO - __main__ -    dev: infer_time = 2.7744444444444443
05/28/2023 14:01:12 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:12 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:01:12 - INFO - __main__ -     cls_loss = 0.7001315072963112
05/28/2023 14:01:12 - INFO - __main__ -     eval_loss = 0.6918948425187005
05/28/2023 14:01:12 - INFO - __main__ -     global_step = 19
05/28/2023 14:01:12 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:12 - INFO - __main__ -     infer_time = 2.7744444444444443
05/28/2023 14:01:12 - INFO - __main__ -     loss = 0.7001315072963112

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.66it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.04it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.22it/s][A05/28/2023 14:01:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:12 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:01:12 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.61it/s]
05/28/2023 14:01:12 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:12 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:12 - INFO - __main__ -    dev: eval_loss = 0.6959923373328315
05/28/2023 14:01:12 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:12 - INFO - __main__ -    dev: infer_time = 2.7751111111111113
05/28/2023 14:01:12 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:12 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:12 - INFO - __main__ -     cls_loss = 0.7012350333148035
05/28/2023 14:01:12 - INFO - __main__ -     eval_loss = 0.6959923373328315
05/28/2023 14:01:12 - INFO - __main__ -     global_step = 29
05/28/2023 14:01:12 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:12 - INFO - __main__ -     infer_time = 2.7751111111111113
05/28/2023 14:01:12 - INFO - __main__ -     loss = 0.7012350333148035

Iteration:  38%|###8      | 30/78 [00:02<00:03, 15.84it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 17.75it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 19.43it/s][A05/28/2023 14:01:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:13 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:01:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.41it/s]
05/28/2023 14:01:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:13 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:13 - INFO - __main__ -    dev: eval_loss = 0.69866410891215
05/28/2023 14:01:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:13 - INFO - __main__ -    dev: infer_time = 2.7883333333333327
05/28/2023 14:01:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:13 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:13 - INFO - __main__ -     cls_loss = 0.6993790238331525
05/28/2023 14:01:13 - INFO - __main__ -     eval_loss = 0.69866410891215
05/28/2023 14:01:13 - INFO - __main__ -     global_step = 39
05/28/2023 14:01:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:13 - INFO - __main__ -     infer_time = 2.7883333333333327
05/28/2023 14:01:13 - INFO - __main__ -     loss = 0.6993790238331525

Iteration:  50%|#####     | 39/78 [00:03<00:02, 18.01it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 19.42it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 20.67it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 21.77it/s][A05/28/2023 14:01:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:13 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:01:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.48it/s]
05/28/2023 14:01:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:13 - INFO - __main__ -    dev: acc = 0.49458483754512633
05/28/2023 14:01:13 - INFO - __main__ -    dev: eval_loss = 0.6928830676608615
05/28/2023 14:01:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:13 - INFO - __main__ -    dev: infer_time = 2.7716666666666665
05/28/2023 14:01:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:13 - INFO - __main__ -     acc = 0.49458483754512633
05/28/2023 14:01:13 - INFO - __main__ -     cls_loss = 0.6982277887208121
05/28/2023 14:01:13 - INFO - __main__ -     eval_loss = 0.6928830676608615
05/28/2023 14:01:13 - INFO - __main__ -     global_step = 49
05/28/2023 14:01:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:13 - INFO - __main__ -     infer_time = 2.7716666666666665
05/28/2023 14:01:13 - INFO - __main__ -     loss = 0.6982277887208121

Iteration:  65%|######5   | 51/78 [00:03<00:01, 19.14it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 20.57it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:00, 21.65it/s][A05/28/2023 14:01:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:14 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:01:14 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.62it/s]
05/28/2023 14:01:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:14 - INFO - __main__ -    dev: acc = 0.5487364620938628
05/28/2023 14:01:14 - INFO - __main__ -    dev: eval_loss = 0.6916255089971755
05/28/2023 14:01:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:14 - INFO - __main__ -    dev: infer_time = 2.792111111111111
05/28/2023 14:01:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:14 - INFO - __main__ -     acc = 0.5487364620938628
05/28/2023 14:01:14 - INFO - __main__ -     cls_loss = 0.6971027770284879
05/28/2023 14:01:14 - INFO - __main__ -     eval_loss = 0.6916255089971755
05/28/2023 14:01:14 - INFO - __main__ -     global_step = 59
05/28/2023 14:01:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:14 - INFO - __main__ -     infer_time = 2.792111111111111
05/28/2023 14:01:14 - INFO - __main__ -     loss = 0.6971027770284879
05/28/2023 14:01:14 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  77%|#######6  | 60/78 [00:05<00:03,  5.54it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:02,  7.21it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:01,  9.17it/s][A05/28/2023 14:01:15 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:15 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:01:15 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:15 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.34it/s]
05/28/2023 14:01:15 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:15 - INFO - __main__ -    dev: acc = 0.5487364620938628
05/28/2023 14:01:15 - INFO - __main__ -    dev: eval_loss = 0.6913720568021139
05/28/2023 14:01:15 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:15 - INFO - __main__ -    dev: infer_time = 2.7929999999999997
05/28/2023 14:01:15 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:15 - INFO - __main__ -     acc = 0.5487364620938628
05/28/2023 14:01:15 - INFO - __main__ -     cls_loss = 0.6963849473690641
05/28/2023 14:01:15 - INFO - __main__ -     eval_loss = 0.6913720568021139
05/28/2023 14:01:15 - INFO - __main__ -     global_step = 69
05/28/2023 14:01:15 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:15 - INFO - __main__ -     infer_time = 2.7929999999999997
05/28/2023 14:01:15 - INFO - __main__ -     loss = 0.6963849473690641

Iteration:  88%|########8 | 69/78 [00:05<00:00, 10.39it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 12.52it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 14.72it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.44it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.27s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.27s/it]
05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   w_emb: 13918032

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   p_emb: 233472

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   t_emb: 912

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   ln_emb: 912

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 204736

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 204744

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   intermediate_numel: 204736

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   output_numel: 205656

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 204736

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 204744

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   intermediate_numel: 204736

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   output_numel: 205656

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 204736

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 204744

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   intermediate_numel: 204736

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   output_numel: 205656

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   layer_numel: 3734616
05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   dense_numel: 208392
05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   emb_numel: 14153328

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   encoder_numel: 3734616

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   pooler_numel: 208392

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   all parameters: 18096336

05/28/2023 14:01:16 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:16 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [448, 448, 448], 'sample_qkv_sizes': [456, 456, 456]}
parameter size = 18096336
best_acc = 0.5487364620938628
time_per_batch_infer = 2.781 ms
infer_cnt = 63
**************E*************

05/28/2023 14:01:16 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [704, 704, 704], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [432, 432, 432]}
05/28/2023 14:01:16 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:01:16 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:01:16 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:16 - INFO - __main__ -   guid: train-0
05/28/2023 14:01:16 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:01:16 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:16 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:16 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:16 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:16 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:18 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:01:18 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:18 - INFO - __main__ -   guid: dev-0
05/28/2023 14:01:18 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:01:18 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:18 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:18 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:18 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:01:18 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:01:18 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:01:18 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:01:18 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:01:18 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:01:18 - INFO - __main__ -     Batch size = 32
05/28/2023 14:01:18 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 24.22it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.63it/s][A05/28/2023 14:01:19 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:19 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:01:19 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:19 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.08it/s]
05/28/2023 14:01:19 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:19 - INFO - __main__ -    dev: acc = 0.4693140794223827
05/28/2023 14:01:19 - INFO - __main__ -    dev: eval_loss = 0.6969896091355218
05/28/2023 14:01:19 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:19 - INFO - __main__ -    dev: infer_time = 2.7477777777777783
05/28/2023 14:01:19 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:19 - INFO - __main__ -     acc = 0.4693140794223827
05/28/2023 14:01:19 - INFO - __main__ -     cls_loss = 0.6975149446063571
05/28/2023 14:01:19 - INFO - __main__ -     eval_loss = 0.6969896091355218
05/28/2023 14:01:19 - INFO - __main__ -     global_step = 9
05/28/2023 14:01:19 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:19 - INFO - __main__ -     infer_time = 2.7477777777777783
05/28/2023 14:01:19 - INFO - __main__ -     loss = 0.6975149446063571
05/28/2023 14:01:19 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.01it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.99it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.25it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.67it/s][A05/28/2023 14:01:20 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:20 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:01:20 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:20 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.99it/s]
05/28/2023 14:01:20 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:20 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:20 - INFO - __main__ -    dev: eval_loss = 0.6934156616528829
05/28/2023 14:01:20 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:20 - INFO - __main__ -    dev: infer_time = 2.7357777777777783
05/28/2023 14:01:20 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:20 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:20 - INFO - __main__ -     cls_loss = 0.6996061613685206
05/28/2023 14:01:20 - INFO - __main__ -     eval_loss = 0.6934156616528829
05/28/2023 14:01:20 - INFO - __main__ -     global_step = 19
05/28/2023 14:01:20 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:20 - INFO - __main__ -     infer_time = 2.7357777777777783
05/28/2023 14:01:20 - INFO - __main__ -     loss = 0.6996061613685206
05/28/2023 14:01:20 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.41it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.97it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.83it/s][A05/28/2023 14:01:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:22 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:01:22 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.16it/s]
05/28/2023 14:01:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:22 - INFO - __main__ -    dev: acc = 0.5054151624548736
05/28/2023 14:01:22 - INFO - __main__ -    dev: eval_loss = 0.6931972172525194
05/28/2023 14:01:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:22 - INFO - __main__ -    dev: infer_time = 2.7447777777777778
05/28/2023 14:01:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:22 - INFO - __main__ -     acc = 0.5054151624548736
05/28/2023 14:01:22 - INFO - __main__ -     cls_loss = 0.6998431107093548
05/28/2023 14:01:22 - INFO - __main__ -     eval_loss = 0.6931972172525194
05/28/2023 14:01:22 - INFO - __main__ -     global_step = 29
05/28/2023 14:01:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:22 - INFO - __main__ -     infer_time = 2.7447777777777778
05/28/2023 14:01:22 - INFO - __main__ -     loss = 0.6998431107093548

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.18it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.37it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.61it/s][A05/28/2023 14:01:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:23 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:01:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.31it/s]
05/28/2023 14:01:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:23 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 14:01:23 - INFO - __main__ -    dev: eval_loss = 0.6930456227726407
05/28/2023 14:01:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:23 - INFO - __main__ -    dev: infer_time = 2.732111111111111
05/28/2023 14:01:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:23 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 14:01:23 - INFO - __main__ -     cls_loss = 0.698847920466692
05/28/2023 14:01:23 - INFO - __main__ -     eval_loss = 0.6930456227726407
05/28/2023 14:01:23 - INFO - __main__ -     global_step = 39
05/28/2023 14:01:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:23 - INFO - __main__ -     infer_time = 2.732111111111111
05/28/2023 14:01:23 - INFO - __main__ -     loss = 0.698847920466692

Iteration:  50%|#####     | 39/78 [00:04<00:02, 14.02it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 16.06it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 17.97it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 19.60it/s][A05/28/2023 14:01:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:23 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:01:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.30it/s]
05/28/2023 14:01:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:23 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:23 - INFO - __main__ -    dev: eval_loss = 0.6985508799552917
05/28/2023 14:01:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:23 - INFO - __main__ -    dev: infer_time = 2.7478888888888893
05/28/2023 14:01:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:23 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:23 - INFO - __main__ -     cls_loss = 0.697594049025555
05/28/2023 14:01:23 - INFO - __main__ -     eval_loss = 0.6985508799552917
05/28/2023 14:01:23 - INFO - __main__ -     global_step = 49
05/28/2023 14:01:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:23 - INFO - __main__ -     infer_time = 2.7478888888888893
05/28/2023 14:01:23 - INFO - __main__ -     loss = 0.697594049025555

Iteration:  65%|######5   | 51/78 [00:05<00:01, 17.87it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 19.50it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 20.86it/s][A05/28/2023 14:01:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:24 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:01:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.50it/s]
05/28/2023 14:01:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:24 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:24 - INFO - __main__ -    dev: eval_loss = 0.6958207885424296
05/28/2023 14:01:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:24 - INFO - __main__ -    dev: infer_time = 2.7463333333333333
05/28/2023 14:01:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:24 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:24 - INFO - __main__ -     cls_loss = 0.6973428574659056
05/28/2023 14:01:24 - INFO - __main__ -     eval_loss = 0.6958207885424296
05/28/2023 14:01:24 - INFO - __main__ -     global_step = 59
05/28/2023 14:01:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:24 - INFO - __main__ -     infer_time = 2.7463333333333333
05/28/2023 14:01:24 - INFO - __main__ -     loss = 0.6973428574659056

Iteration:  77%|#######6  | 60/78 [00:05<00:00, 18.44it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 19.76it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 21.05it/s][A05/28/2023 14:01:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:24 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:01:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.35it/s]
05/28/2023 14:01:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:24 - INFO - __main__ -    dev: acc = 0.4693140794223827
05/28/2023 14:01:24 - INFO - __main__ -    dev: eval_loss = 0.6943944427702162
05/28/2023 14:01:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:24 - INFO - __main__ -    dev: infer_time = 2.9038888888888894
05/28/2023 14:01:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:24 - INFO - __main__ -     acc = 0.4693140794223827
05/28/2023 14:01:24 - INFO - __main__ -     cls_loss = 0.6965673807738484
05/28/2023 14:01:24 - INFO - __main__ -     eval_loss = 0.6943944427702162
05/28/2023 14:01:24 - INFO - __main__ -     global_step = 69
05/28/2023 14:01:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:24 - INFO - __main__ -     infer_time = 2.9038888888888894
05/28/2023 14:01:24 - INFO - __main__ -     loss = 0.6965673807738484

Iteration:  88%|########8 | 69/78 [00:05<00:00, 18.81it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 20.16it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 21.38it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.43it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.28s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.28s/it]
05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   w_emb: 13185504

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   p_emb: 221184

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   t_emb: 864

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   ln_emb: 864

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 304832

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 304560

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   intermediate_numel: 304832

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   output_numel: 305424

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 304832

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 304560

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   intermediate_numel: 304832

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   output_numel: 305424

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 304832

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 304560

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   intermediate_numel: 304832

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   output_numel: 305424

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   layer_numel: 4078032
05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   dense_numel: 187056
05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   emb_numel: 13408416

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   encoder_numel: 4078032

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   pooler_numel: 187056

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   all parameters: 17673504

05/28/2023 14:01:25 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:25 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [704, 704, 704], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [432, 432, 432]}
parameter size = 17673504
best_acc = 0.5270758122743683
time_per_batch_infer = 2.766 ms
infer_cnt = 63
**************E*************

05/28/2023 14:01:25 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [640, 640, 640, 640], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
05/28/2023 14:01:25 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:01:25 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:01:25 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:25 - INFO - __main__ -   guid: train-0
05/28/2023 14:01:25 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:01:25 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:25 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:25 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:26 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:01:26 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:26 - INFO - __main__ -   guid: dev-0
05/28/2023 14:01:26 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:01:26 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:26 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:26 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:26 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:26 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:26 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:01:26 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:01:27 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:01:27 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:01:27 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:01:27 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:01:27 - INFO - __main__ -     Batch size = 32
05/28/2023 14:01:27 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.13it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.65it/s][A05/28/2023 14:01:27 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:27 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:01:27 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:27 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.61it/s]
05/28/2023 14:01:28 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:28 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:28 - INFO - __main__ -    dev: eval_loss = 0.7062734299235873
05/28/2023 14:01:28 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:28 - INFO - __main__ -    dev: infer_time = 3.369444444444444
05/28/2023 14:01:28 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:28 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:28 - INFO - __main__ -     cls_loss = 0.6957896418041654
05/28/2023 14:01:28 - INFO - __main__ -     eval_loss = 0.7062734299235873
05/28/2023 14:01:28 - INFO - __main__ -     global_step = 9
05/28/2023 14:01:28 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:28 - INFO - __main__ -     infer_time = 3.369444444444444
05/28/2023 14:01:28 - INFO - __main__ -     loss = 0.6957896418041654
05/28/2023 14:01:28 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.70it/s][A
Iteration:  15%|#5        | 12/78 [00:02<00:12,  5.48it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.53it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.71it/s][A05/28/2023 14:01:29 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:29 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:01:29 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:29 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.44it/s]
05/28/2023 14:01:29 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:29 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:01:29 - INFO - __main__ -    dev: eval_loss = 0.6915076838599311
05/28/2023 14:01:29 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:29 - INFO - __main__ -    dev: infer_time = 3.3502222222222224
05/28/2023 14:01:29 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:29 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:01:29 - INFO - __main__ -     cls_loss = 0.6954617531676042
05/28/2023 14:01:29 - INFO - __main__ -     eval_loss = 0.6915076838599311
05/28/2023 14:01:29 - INFO - __main__ -     global_step = 19
05/28/2023 14:01:29 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:29 - INFO - __main__ -     infer_time = 3.3502222222222224
05/28/2023 14:01:29 - INFO - __main__ -     loss = 0.6954617531676042
05/28/2023 14:01:29 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:14,  3.91it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.45it/s][A
Iteration:  33%|###3      | 26/78 [00:04<00:07,  7.27it/s][A05/28/2023 14:01:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:31 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:01:31 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.55it/s]
05/28/2023 14:01:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:31 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:31 - INFO - __main__ -    dev: eval_loss = 0.6917764809396532
05/28/2023 14:01:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:31 - INFO - __main__ -    dev: infer_time = 3.376
05/28/2023 14:01:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:31 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:31 - INFO - __main__ -     cls_loss = 0.6938601937787287
05/28/2023 14:01:31 - INFO - __main__ -     eval_loss = 0.6917764809396532
05/28/2023 14:01:31 - INFO - __main__ -     global_step = 29
05/28/2023 14:01:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:31 - INFO - __main__ -     infer_time = 3.376
05/28/2023 14:01:31 - INFO - __main__ -     loss = 0.6938601937787287

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.58it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.62it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.71it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.63it/s][A05/28/2023 14:01:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:32 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:01:32 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.58it/s]
05/28/2023 14:01:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:32 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:01:32 - INFO - __main__ -    dev: eval_loss = 0.6938736240069071
05/28/2023 14:01:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:32 - INFO - __main__ -    dev: infer_time = 3.3393333333333337
05/28/2023 14:01:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:32 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:01:32 - INFO - __main__ -     cls_loss = 0.6939615561411931
05/28/2023 14:01:32 - INFO - __main__ -     eval_loss = 0.6938736240069071
05/28/2023 14:01:32 - INFO - __main__ -     global_step = 39
05/28/2023 14:01:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:32 - INFO - __main__ -     infer_time = 3.3393333333333337
05/28/2023 14:01:32 - INFO - __main__ -     loss = 0.6939615561411931

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.46it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.23it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.73it/s][A05/28/2023 14:01:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:32 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:01:32 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.70it/s]
05/28/2023 14:01:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:32 - INFO - __main__ -    dev: acc = 0.48375451263537905
05/28/2023 14:01:32 - INFO - __main__ -    dev: eval_loss = 0.6970906721221076
05/28/2023 14:01:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:32 - INFO - __main__ -    dev: infer_time = 3.3603333333333336
05/28/2023 14:01:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:32 - INFO - __main__ -     acc = 0.48375451263537905
05/28/2023 14:01:32 - INFO - __main__ -     cls_loss = 0.693636239791403
05/28/2023 14:01:32 - INFO - __main__ -     eval_loss = 0.6970906721221076
05/28/2023 14:01:32 - INFO - __main__ -     global_step = 49
05/28/2023 14:01:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:32 - INFO - __main__ -     infer_time = 3.3603333333333336
05/28/2023 14:01:32 - INFO - __main__ -     loss = 0.693636239791403

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.37it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 17.76it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 18.99it/s][A05/28/2023 14:01:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:33 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:01:33 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.64it/s]
05/28/2023 14:01:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:33 - INFO - __main__ -    dev: acc = 0.5090252707581228
05/28/2023 14:01:33 - INFO - __main__ -    dev: eval_loss = 0.6940048668119643
05/28/2023 14:01:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:33 - INFO - __main__ -    dev: infer_time = 3.347888888888889
05/28/2023 14:01:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:33 - INFO - __main__ -     acc = 0.5090252707581228
05/28/2023 14:01:33 - INFO - __main__ -     cls_loss = 0.693175047130908
05/28/2023 14:01:33 - INFO - __main__ -     eval_loss = 0.6940048668119643
05/28/2023 14:01:33 - INFO - __main__ -     global_step = 59
05/28/2023 14:01:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:33 - INFO - __main__ -     infer_time = 3.347888888888889
05/28/2023 14:01:33 - INFO - __main__ -     loss = 0.693175047130908

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 17.12it/s][A
Iteration:  78%|#######8  | 61/78 [00:05<00:00, 17.44it/s][A
Iteration:  82%|########2 | 64/78 [00:06<00:00, 18.79it/s][A
Iteration:  86%|########5 | 67/78 [00:06<00:00, 19.72it/s][A05/28/2023 14:01:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:33 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:01:33 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 127.78it/s]
05/28/2023 14:01:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:33 - INFO - __main__ -    dev: acc = 0.4657039711191336
05/28/2023 14:01:33 - INFO - __main__ -    dev: eval_loss = 0.6971055335468717
05/28/2023 14:01:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:33 - INFO - __main__ -    dev: infer_time = 3.3484444444444446
05/28/2023 14:01:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:33 - INFO - __main__ -     acc = 0.4657039711191336
05/28/2023 14:01:33 - INFO - __main__ -     cls_loss = 0.6931572193684785
05/28/2023 14:01:33 - INFO - __main__ -     eval_loss = 0.6971055335468717
05/28/2023 14:01:33 - INFO - __main__ -     global_step = 69
05/28/2023 14:01:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:33 - INFO - __main__ -     infer_time = 3.3484444444444446
05/28/2023 14:01:33 - INFO - __main__ -     loss = 0.6931572193684785

Iteration:  90%|########9 | 70/78 [00:06<00:00, 17.59it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 18.85it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 19.84it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.60it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.72s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.72s/it]
05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   w_emb: 9889128

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   p_emb: 165888

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   t_emb: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ln_emb: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 208000

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 207684

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   intermediate_numel: 208000

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   output_numel: 208332

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 208000

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 207684

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   intermediate_numel: 208000

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   output_numel: 208332

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 208000

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 207684

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   intermediate_numel: 208000

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   output_numel: 208332

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 208000

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 207684

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   intermediate_numel: 208000

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   output_numel: 208332

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   layer_numel: 3352720
05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   dense_numel: 105300
05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   emb_numel: 10056312

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   encoder_numel: 3352720

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   pooler_numel: 105300

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   all parameters: 13514332

05/28/2023 14:01:34 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:34 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [640, 640, 640, 640], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
parameter size = 13514332
best_acc = 0.5306859205776173
time_per_batch_infer = 3.356 ms
infer_cnt = 63
**************E*************

05/28/2023 14:01:34 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 156, 'sample_intermediate_sizes': [960, 960, 960, 960], 'sample_qkv_sizes': [156, 156, 156, 156]}
05/28/2023 14:01:34 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:01:34 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:01:34 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:34 - INFO - __main__ -   guid: train-0
05/28/2023 14:01:34 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:01:34 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:34 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:34 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:34 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:34 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:36 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:01:36 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:36 - INFO - __main__ -   guid: dev-0
05/28/2023 14:01:36 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:01:36 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:36 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:36 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:36 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:01:36 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:01:36 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:01:36 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:01:36 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:01:36 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:01:36 - INFO - __main__ -     Batch size = 32
05/28/2023 14:01:36 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 24.32it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 25.91it/s][A05/28/2023 14:01:37 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:37 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:01:37 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:37 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.62it/s]
05/28/2023 14:01:37 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:37 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:37 - INFO - __main__ -    dev: eval_loss = 0.6920967499415079
05/28/2023 14:01:37 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:37 - INFO - __main__ -    dev: infer_time = 3.361888888888889
05/28/2023 14:01:37 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:37 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:37 - INFO - __main__ -     cls_loss = 0.6934559014108446
05/28/2023 14:01:37 - INFO - __main__ -     eval_loss = 0.6920967499415079
05/28/2023 14:01:37 - INFO - __main__ -     global_step = 9
05/28/2023 14:01:37 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:37 - INFO - __main__ -     infer_time = 3.361888888888889
05/28/2023 14:01:37 - INFO - __main__ -     loss = 0.6934559014108446
05/28/2023 14:01:37 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:16,  4.08it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:10,  6.08it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.45it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 11.03it/s][A05/28/2023 14:01:38 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:38 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:01:38 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:38 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.83it/s]
05/28/2023 14:01:38 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:38 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:38 - INFO - __main__ -    dev: eval_loss = 0.6939806342124939
05/28/2023 14:01:38 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:38 - INFO - __main__ -    dev: infer_time = 3.3564444444444446
05/28/2023 14:01:38 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:38 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:38 - INFO - __main__ -     cls_loss = 0.6915382303689656
05/28/2023 14:01:38 - INFO - __main__ -     eval_loss = 0.6939806342124939
05/28/2023 14:01:38 - INFO - __main__ -     global_step = 19
05/28/2023 14:01:38 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:38 - INFO - __main__ -     infer_time = 3.3564444444444446
05/28/2023 14:01:38 - INFO - __main__ -     loss = 0.6915382303689656

Iteration:  27%|##6       | 21/78 [00:02<00:04, 12.33it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.95it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:02, 17.38it/s][A05/28/2023 14:01:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:39 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:01:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.97it/s]
05/28/2023 14:01:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:39 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:39 - INFO - __main__ -    dev: eval_loss = 0.6918359067704942
05/28/2023 14:01:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:39 - INFO - __main__ -    dev: infer_time = 3.339222222222222
05/28/2023 14:01:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:39 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:39 - INFO - __main__ -     cls_loss = 0.694686116843388
05/28/2023 14:01:39 - INFO - __main__ -     eval_loss = 0.6918359067704942
05/28/2023 14:01:39 - INFO - __main__ -     global_step = 29
05/28/2023 14:01:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:39 - INFO - __main__ -     infer_time = 3.339222222222222
05/28/2023 14:01:39 - INFO - __main__ -     loss = 0.694686116843388

Iteration:  38%|###8      | 30/78 [00:02<00:02, 17.43it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 19.53it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:01, 21.34it/s][A05/28/2023 14:01:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:39 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:01:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.77it/s]
05/28/2023 14:01:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:39 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:39 - INFO - __main__ -    dev: eval_loss = 0.6969903045230441
05/28/2023 14:01:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:39 - INFO - __main__ -    dev: infer_time = 3.421222222222222
05/28/2023 14:01:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:39 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:39 - INFO - __main__ -     cls_loss = 0.6943752704522549
05/28/2023 14:01:39 - INFO - __main__ -     eval_loss = 0.6969903045230441
05/28/2023 14:01:39 - INFO - __main__ -     global_step = 39
05/28/2023 14:01:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:39 - INFO - __main__ -     infer_time = 3.421222222222222
05/28/2023 14:01:39 - INFO - __main__ -     loss = 0.6943752704522549

Iteration:  50%|#####     | 39/78 [00:03<00:01, 20.10it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 21.24it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 22.75it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 23.99it/s][A05/28/2023 14:01:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:40 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:01:40 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 174.08it/s]
05/28/2023 14:01:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:40 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:40 - INFO - __main__ -    dev: eval_loss = 0.6949933038817512
05/28/2023 14:01:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:40 - INFO - __main__ -    dev: infer_time = 3.3603333333333336
05/28/2023 14:01:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:40 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:40 - INFO - __main__ -     cls_loss = 0.694714602158994
05/28/2023 14:01:40 - INFO - __main__ -     eval_loss = 0.6949933038817512
05/28/2023 14:01:40 - INFO - __main__ -     global_step = 49
05/28/2023 14:01:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:40 - INFO - __main__ -     infer_time = 3.3603333333333336
05/28/2023 14:01:40 - INFO - __main__ -     loss = 0.694714602158994

Iteration:  65%|######5   | 51/78 [00:03<00:01, 21.37it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 22.82it/s][A
Iteration:  73%|#######3  | 57/78 [00:03<00:00, 24.04it/s][A05/28/2023 14:01:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:40 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:01:40 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.89it/s]
05/28/2023 14:01:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:40 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:40 - INFO - __main__ -    dev: eval_loss = 0.6939408977826437
05/28/2023 14:01:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:40 - INFO - __main__ -    dev: infer_time = 3.3435555555555556
05/28/2023 14:01:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:40 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:40 - INFO - __main__ -     cls_loss = 0.6944362824246034
05/28/2023 14:01:40 - INFO - __main__ -     eval_loss = 0.6939408977826437
05/28/2023 14:01:40 - INFO - __main__ -     global_step = 59
05/28/2023 14:01:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:40 - INFO - __main__ -     infer_time = 3.3435555555555556
05/28/2023 14:01:40 - INFO - __main__ -     loss = 0.6944362824246034

Iteration:  77%|#######6  | 60/78 [00:03<00:00, 21.55it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 22.82it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 23.97it/s][A05/28/2023 14:01:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:41 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:01:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 174.23it/s]
05/28/2023 14:01:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:41 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:41 - INFO - __main__ -    dev: eval_loss = 0.6945626537005106
05/28/2023 14:01:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:41 - INFO - __main__ -    dev: infer_time = 3.332777777777778
05/28/2023 14:01:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:41 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:41 - INFO - __main__ -     cls_loss = 0.6941182155540024
05/28/2023 14:01:41 - INFO - __main__ -     eval_loss = 0.6945626537005106
05/28/2023 14:01:41 - INFO - __main__ -     global_step = 69
05/28/2023 14:01:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:41 - INFO - __main__ -     infer_time = 3.332777777777778
05/28/2023 14:01:41 - INFO - __main__ -     loss = 0.6941182155540024

Iteration:  88%|########8 | 69/78 [00:04<00:00, 21.68it/s][A
Iteration:  92%|#########2| 72/78 [00:04<00:00, 22.65it/s][A
Iteration:  96%|#########6| 75/78 [00:04<00:00, 23.79it/s][AIteration: 100%|##########| 78/78 [00:04<00:00, 16.87it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.62s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.62s/it]
05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   w_emb: 4761432

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   p_emb: 79872

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   t_emb: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ln_emb: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 150720

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 149916

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   intermediate_numel: 150720

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   output_numel: 150228

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 150720

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 149916

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   intermediate_numel: 150720

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   output_numel: 150228

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 150720

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 149916

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   intermediate_numel: 150720

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   output_numel: 150228

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 150720

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 149916

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   intermediate_numel: 150720

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   output_numel: 150228

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   layer_numel: 1596912
05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   dense_numel: 24492
05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   emb_numel: 4841928

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   encoder_numel: 1596912

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   pooler_numel: 24492

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   all parameters: 6463332

05/28/2023 14:01:41 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:41 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 156, 'sample_intermediate_sizes': [960, 960, 960, 960], 'sample_qkv_sizes': [156, 156, 156, 156]}
parameter size = 6463332
best_acc = 0.5270758122743683
time_per_batch_infer = 3.359 ms
infer_cnt = 63
**************E*************

05/28/2023 14:01:41 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 168, 'sample_intermediate_sizes': [416, 416, 416, 416, 416], 'sample_qkv_sizes': [168, 168, 168, 168, 168]}
05/28/2023 14:01:41 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:01:41 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:01:41 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:41 - INFO - __main__ -   guid: train-0
05/28/2023 14:01:41 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:01:41 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:41 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:41 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:41 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:41 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:43 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:01:43 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:43 - INFO - __main__ -   guid: dev-0
05/28/2023 14:01:43 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:01:43 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:43 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:43 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:43 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:01:43 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:01:43 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:01:43 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:01:43 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:01:43 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:01:43 - INFO - __main__ -     Batch size = 32
05/28/2023 14:01:43 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.63it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.29it/s][A05/28/2023 14:01:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:44 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:01:44 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 168.06it/s]
05/28/2023 14:01:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:44 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:44 - INFO - __main__ -    dev: eval_loss = 0.691766189204322
05/28/2023 14:01:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:44 - INFO - __main__ -    dev: infer_time = 4.097777777777778
05/28/2023 14:01:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:44 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:44 - INFO - __main__ -     cls_loss = 0.691832787460751
05/28/2023 14:01:44 - INFO - __main__ -     eval_loss = 0.691766189204322
05/28/2023 14:01:44 - INFO - __main__ -     global_step = 9
05/28/2023 14:01:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:44 - INFO - __main__ -     infer_time = 4.097777777777778
05/28/2023 14:01:44 - INFO - __main__ -     loss = 0.691832787460751
05/28/2023 14:01:44 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.00it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.90it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.08it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.45it/s][A05/28/2023 14:01:46 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:46 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:01:46 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:46 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 168.34it/s]
05/28/2023 14:01:46 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:46 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:46 - INFO - __main__ -    dev: eval_loss = 0.6924244695239596
05/28/2023 14:01:46 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:46 - INFO - __main__ -    dev: infer_time = 4.077999999999999
05/28/2023 14:01:46 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:46 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:46 - INFO - __main__ -     cls_loss = 0.6949005440661782
05/28/2023 14:01:46 - INFO - __main__ -     eval_loss = 0.6924244695239596
05/28/2023 14:01:46 - INFO - __main__ -     global_step = 19
05/28/2023 14:01:46 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:46 - INFO - __main__ -     infer_time = 4.077999999999999
05/28/2023 14:01:46 - INFO - __main__ -     loss = 0.6949005440661782

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.48it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.80it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.95it/s][A05/28/2023 14:01:46 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:46 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:01:46 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:46 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 168.69it/s]
05/28/2023 14:01:46 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:46 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:46 - INFO - __main__ -    dev: eval_loss = 0.6959019170867072
05/28/2023 14:01:46 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:46 - INFO - __main__ -    dev: infer_time = 4.081555555555555
05/28/2023 14:01:46 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:46 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:46 - INFO - __main__ -     cls_loss = 0.6945589349187654
05/28/2023 14:01:46 - INFO - __main__ -     eval_loss = 0.6959019170867072
05/28/2023 14:01:46 - INFO - __main__ -     global_step = 29
05/28/2023 14:01:46 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:46 - INFO - __main__ -     infer_time = 4.081555555555555
05/28/2023 14:01:46 - INFO - __main__ -     loss = 0.6945589349187654

Iteration:  38%|###8      | 30/78 [00:02<00:03, 15.65it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 17.55it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 19.07it/s][A05/28/2023 14:01:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:47 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:01:47 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 168.38it/s]
05/28/2023 14:01:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:47 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:47 - INFO - __main__ -    dev: eval_loss = 0.6945109566052755
05/28/2023 14:01:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:47 - INFO - __main__ -    dev: infer_time = 4.093555555555556
05/28/2023 14:01:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:47 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:47 - INFO - __main__ -     cls_loss = 0.6948002683810699
05/28/2023 14:01:47 - INFO - __main__ -     eval_loss = 0.6945109566052755
05/28/2023 14:01:47 - INFO - __main__ -     global_step = 39
05/28/2023 14:01:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:47 - INFO - __main__ -     infer_time = 4.093555555555556
05/28/2023 14:01:47 - INFO - __main__ -     loss = 0.6948002683810699

Iteration:  50%|#####     | 39/78 [00:03<00:02, 17.86it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 19.16it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 20.45it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 21.13it/s][A05/28/2023 14:01:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:47 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:01:47 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 168.54it/s]
05/28/2023 14:01:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:47 - INFO - __main__ -    dev: acc = 0.5126353790613718
05/28/2023 14:01:47 - INFO - __main__ -    dev: eval_loss = 0.6930213570594788
05/28/2023 14:01:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:47 - INFO - __main__ -    dev: infer_time = 4.077999999999999
05/28/2023 14:01:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:47 - INFO - __main__ -     acc = 0.5126353790613718
05/28/2023 14:01:47 - INFO - __main__ -     cls_loss = 0.6943671922294461
05/28/2023 14:01:47 - INFO - __main__ -     eval_loss = 0.6930213570594788
05/28/2023 14:01:47 - INFO - __main__ -     global_step = 49
05/28/2023 14:01:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:47 - INFO - __main__ -     infer_time = 4.077999999999999
05/28/2023 14:01:47 - INFO - __main__ -     loss = 0.6943671922294461

Iteration:  65%|######5   | 51/78 [00:03<00:01, 18.91it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 20.23it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:01, 20.95it/s][A05/28/2023 14:01:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:48 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:01:48 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 168.51it/s]
05/28/2023 14:01:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:48 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:48 - INFO - __main__ -    dev: eval_loss = 0.693505671289232
05/28/2023 14:01:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:48 - INFO - __main__ -    dev: infer_time = 4.072222222222222
05/28/2023 14:01:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:48 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:48 - INFO - __main__ -     cls_loss = 0.694006290476201
05/28/2023 14:01:48 - INFO - __main__ -     eval_loss = 0.693505671289232
05/28/2023 14:01:48 - INFO - __main__ -     global_step = 59
05/28/2023 14:01:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:48 - INFO - __main__ -     infer_time = 4.072222222222222
05/28/2023 14:01:48 - INFO - __main__ -     loss = 0.694006290476201

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 18.95it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 20.02it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 21.03it/s][A05/28/2023 14:01:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:48 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:01:48 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 168.70it/s]
05/28/2023 14:01:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:48 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:48 - INFO - __main__ -    dev: eval_loss = 0.6939756671587626
05/28/2023 14:01:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:48 - INFO - __main__ -    dev: infer_time = 4.074666666666667
05/28/2023 14:01:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:48 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:48 - INFO - __main__ -     cls_loss = 0.6940635589585789
05/28/2023 14:01:48 - INFO - __main__ -     eval_loss = 0.6939756671587626
05/28/2023 14:01:48 - INFO - __main__ -     global_step = 69
05/28/2023 14:01:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:48 - INFO - __main__ -     infer_time = 4.074666666666667
05/28/2023 14:01:48 - INFO - __main__ -     loss = 0.6940635589585789

Iteration:  88%|########8 | 69/78 [00:04<00:00, 19.21it/s][A
Iteration:  92%|#########2| 72/78 [00:04<00:00, 20.20it/s][A
Iteration:  96%|#########6| 75/78 [00:04<00:00, 21.17it/s][AIteration: 100%|##########| 78/78 [00:05<00:00, 15.44it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.05s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.05s/it]
05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   w_emb: 5127696

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   p_emb: 86016

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   t_emb: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_emb: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70056

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 70392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70056

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 70392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70056

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 70392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70056

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 70392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 70056

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 70304

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   output_numel: 70392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   layer_numel: 1273000
05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   dense_numel: 28392
05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   emb_numel: 5214384

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   encoder_numel: 1273000

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   pooler_numel: 28392

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   all parameters: 6515776

05/28/2023 14:01:49 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:49 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 168, 'sample_intermediate_sizes': [416, 416, 416, 416, 416], 'sample_qkv_sizes': [168, 168, 168, 168, 168]}
parameter size = 6515776
best_acc = 0.5270758122743683
time_per_batch_infer = 4.082 ms
infer_cnt = 63
**************E*************

05/28/2023 14:01:49 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 324, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [324, 324, 324, 324]}
05/28/2023 14:01:49 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:01:49 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:01:49 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:49 - INFO - __main__ -   guid: train-0
05/28/2023 14:01:49 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:01:49 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:49 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:49 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:50 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:01:50 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:50 - INFO - __main__ -   guid: dev-0
05/28/2023 14:01:50 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:01:50 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:50 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:50 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:50 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:50 - INFO - __main__ -   label_id: 1
05/28/2023 14:01:50 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:01:50 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:01:51 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:01:51 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:01:51 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:01:51 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:01:51 - INFO - __main__ -     Batch size = 32
05/28/2023 14:01:51 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.33it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.58it/s][A05/28/2023 14:01:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:51 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:01:51 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 135.71it/s]
05/28/2023 14:01:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:51 - INFO - __main__ -    dev: acc = 0.49458483754512633
05/28/2023 14:01:51 - INFO - __main__ -    dev: eval_loss = 0.693123459815979
05/28/2023 14:01:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:51 - INFO - __main__ -    dev: infer_time = 3.387333333333333
05/28/2023 14:01:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:51 - INFO - __main__ -     acc = 0.49458483754512633
05/28/2023 14:01:51 - INFO - __main__ -     cls_loss = 0.6993783579932319
05/28/2023 14:01:51 - INFO - __main__ -     eval_loss = 0.693123459815979
05/28/2023 14:01:51 - INFO - __main__ -     global_step = 9
05/28/2023 14:01:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:51 - INFO - __main__ -     infer_time = 3.387333333333333
05/28/2023 14:01:51 - INFO - __main__ -     loss = 0.6993783579932319
05/28/2023 14:01:51 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.89it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.75it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.87it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.14it/s][A05/28/2023 14:01:53 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:53 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:01:53 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:53 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 135.37it/s]
05/28/2023 14:01:53 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:53 - INFO - __main__ -    dev: acc = 0.48375451263537905
05/28/2023 14:01:53 - INFO - __main__ -    dev: eval_loss = 0.6934547689225938
05/28/2023 14:01:53 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:53 - INFO - __main__ -    dev: infer_time = 3.472111111111111
05/28/2023 14:01:53 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:53 - INFO - __main__ -     acc = 0.48375451263537905
05/28/2023 14:01:53 - INFO - __main__ -     cls_loss = 0.6999223295011019
05/28/2023 14:01:53 - INFO - __main__ -     eval_loss = 0.6934547689225938
05/28/2023 14:01:53 - INFO - __main__ -     global_step = 19
05/28/2023 14:01:53 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:53 - INFO - __main__ -     infer_time = 3.472111111111111
05/28/2023 14:01:53 - INFO - __main__ -     loss = 0.6999223295011019

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.25it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 13.43it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.45it/s][A05/28/2023 14:01:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:54 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:01:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 135.67it/s]
05/28/2023 14:01:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:54 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:01:54 - INFO - __main__ -    dev: eval_loss = 0.6931803822517395
05/28/2023 14:01:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:54 - INFO - __main__ -    dev: infer_time = 3.425888888888889
05/28/2023 14:01:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:54 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:01:54 - INFO - __main__ -     cls_loss = 0.6977876609769361
05/28/2023 14:01:54 - INFO - __main__ -     eval_loss = 0.6931803822517395
05/28/2023 14:01:54 - INFO - __main__ -     global_step = 29
05/28/2023 14:01:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:54 - INFO - __main__ -     infer_time = 3.425888888888889
05/28/2023 14:01:54 - INFO - __main__ -     loss = 0.6977876609769361
05/28/2023 14:01:54 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:09,  5.02it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:06,  6.60it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:04,  8.43it/s][A05/28/2023 14:01:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:56 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:01:56 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 135.38it/s]
05/28/2023 14:01:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:56 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:01:56 - INFO - __main__ -    dev: eval_loss = 0.691504074467553
05/28/2023 14:01:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:56 - INFO - __main__ -    dev: infer_time = 3.424666666666667
05/28/2023 14:01:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:56 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:01:56 - INFO - __main__ -     cls_loss = 0.698139644586123
05/28/2023 14:01:56 - INFO - __main__ -     eval_loss = 0.691504074467553
05/28/2023 14:01:56 - INFO - __main__ -     global_step = 39
05/28/2023 14:01:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:56 - INFO - __main__ -     infer_time = 3.424666666666667
05/28/2023 14:01:56 - INFO - __main__ -     loss = 0.698139644586123
05/28/2023 14:01:56 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:05<00:09,  4.29it/s][A
Iteration:  54%|#####3    | 42/78 [00:06<00:06,  5.67it/s][A
Iteration:  58%|#####7    | 45/78 [00:06<00:04,  7.33it/s][A
Iteration:  62%|######1   | 48/78 [00:06<00:03,  9.23it/s][A05/28/2023 14:01:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:57 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:01:57 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 135.16it/s]
05/28/2023 14:01:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:57 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:01:57 - INFO - __main__ -    dev: eval_loss = 0.6970346172650655
05/28/2023 14:01:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:57 - INFO - __main__ -    dev: infer_time = 3.440666666666667
05/28/2023 14:01:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:57 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:01:57 - INFO - __main__ -     cls_loss = 0.6976448273172184
05/28/2023 14:01:57 - INFO - __main__ -     eval_loss = 0.6970346172650655
05/28/2023 14:01:57 - INFO - __main__ -     global_step = 49
05/28/2023 14:01:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:57 - INFO - __main__ -     infer_time = 3.440666666666667
05/28/2023 14:01:57 - INFO - __main__ -     loss = 0.6976448273172184

Iteration:  64%|######4   | 50/78 [00:06<00:02,  9.69it/s][A
Iteration:  68%|######7   | 53/78 [00:06<00:02, 11.97it/s][A
Iteration:  72%|#######1  | 56/78 [00:06<00:01, 14.10it/s][A05/28/2023 14:01:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:58 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:01:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 135.71it/s]
05/28/2023 14:01:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:58 - INFO - __main__ -    dev: acc = 0.48375451263537905
05/28/2023 14:01:58 - INFO - __main__ -    dev: eval_loss = 0.6933600438965691
05/28/2023 14:01:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:58 - INFO - __main__ -    dev: infer_time = 3.4443333333333337
05/28/2023 14:01:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:58 - INFO - __main__ -     acc = 0.48375451263537905
05/28/2023 14:01:58 - INFO - __main__ -     cls_loss = 0.6973965491278696
05/28/2023 14:01:58 - INFO - __main__ -     eval_loss = 0.6933600438965691
05/28/2023 14:01:58 - INFO - __main__ -     global_step = 59
05/28/2023 14:01:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:58 - INFO - __main__ -     infer_time = 3.4443333333333337
05/28/2023 14:01:58 - INFO - __main__ -     loss = 0.6973965491278696

Iteration:  76%|#######5  | 59/78 [00:06<00:01, 14.32it/s][A
Iteration:  79%|#######9  | 62/78 [00:07<00:00, 16.14it/s][A
Iteration:  83%|########3 | 65/78 [00:07<00:00, 17.80it/s][A
Iteration:  87%|########7 | 68/78 [00:07<00:00, 19.16it/s][A05/28/2023 14:01:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:01:58 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:01:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:01:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 135.62it/s]
05/28/2023 14:01:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:01:58 - INFO - __main__ -    dev: acc = 0.4548736462093863
05/28/2023 14:01:58 - INFO - __main__ -    dev: eval_loss = 0.6937445733282301
05/28/2023 14:01:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:01:58 - INFO - __main__ -    dev: infer_time = 3.432666666666667
05/28/2023 14:01:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:01:58 - INFO - __main__ -     acc = 0.4548736462093863
05/28/2023 14:01:58 - INFO - __main__ -     cls_loss = 0.6965503459391387
05/28/2023 14:01:58 - INFO - __main__ -     eval_loss = 0.6937445733282301
05/28/2023 14:01:58 - INFO - __main__ -     global_step = 69
05/28/2023 14:01:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:01:58 - INFO - __main__ -     infer_time = 3.432666666666667
05/28/2023 14:01:58 - INFO - __main__ -     loss = 0.6965503459391387

Iteration:  91%|#########1| 71/78 [00:07<00:00, 17.37it/s][A
Iteration:  95%|#########4| 74/78 [00:07<00:00, 18.77it/s][A
Iteration:  99%|#########8| 77/78 [00:07<00:00, 19.93it/s][AIteration: 100%|##########| 78/78 [00:07<00:00,  9.99it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.81s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.81s/it]
05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   w_emb: 9889128

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   p_emb: 165888

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   t_emb: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ln_emb: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 145600

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 145476

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   intermediate_numel: 145600

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   output_numel: 146124

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 145600

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 145476

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   intermediate_numel: 145600

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   output_numel: 146124

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 145600

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 145476

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   intermediate_numel: 145600

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   output_numel: 146124

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 145600

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 145476

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   intermediate_numel: 145600

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   output_numel: 146124

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   layer_numel: 2854288
05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   dense_numel: 105300
05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   emb_numel: 10056312

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   encoder_numel: 2854288

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   pooler_numel: 105300

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   all parameters: 13015900

05/28/2023 14:01:59 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:01:59 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 324, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [324, 324, 324, 324]}
parameter size = 13015900
best_acc = 0.5306859205776173
time_per_batch_infer = 3.433 ms
infer_cnt = 63
**************E*************

05/28/2023 14:01:59 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [640, 640, 640, 640], 'sample_qkv_sizes': [288, 288, 288, 288]}
05/28/2023 14:01:59 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:01:59 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:01:59 - INFO - __main__ -   *** Example ***
05/28/2023 14:01:59 - INFO - __main__ -   guid: train-0
05/28/2023 14:01:59 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:01:59 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:59 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:59 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:01:59 - INFO - __main__ -   label: not_entailment
05/28/2023 14:01:59 - INFO - __main__ -   label_id: 1
05/28/2023 14:02:01 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:02:01 - INFO - __main__ -   *** Example ***
05/28/2023 14:02:01 - INFO - __main__ -   guid: dev-0
05/28/2023 14:02:01 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:02:01 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:01 - INFO - __main__ -   label: not_entailment
05/28/2023 14:02:01 - INFO - __main__ -   label_id: 1
05/28/2023 14:02:01 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:02:01 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:02:01 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:02:01 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:02:01 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:02:01 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:02:01 - INFO - __main__ -     Batch size = 32
05/28/2023 14:02:01 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.69it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.19it/s][A05/28/2023 14:02:02 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:02 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:02:02 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:02 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 145.49it/s]
05/28/2023 14:02:02 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:02 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:02:02 - INFO - __main__ -    dev: eval_loss = 0.7002607650227017
05/28/2023 14:02:02 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:02 - INFO - __main__ -    dev: infer_time = 3.3546666666666667
05/28/2023 14:02:02 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:02 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:02:02 - INFO - __main__ -     cls_loss = 0.6943755745887756
05/28/2023 14:02:02 - INFO - __main__ -     eval_loss = 0.7002607650227017
05/28/2023 14:02:02 - INFO - __main__ -     global_step = 9
05/28/2023 14:02:02 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:02 - INFO - __main__ -     infer_time = 3.3546666666666667
05/28/2023 14:02:02 - INFO - __main__ -     loss = 0.6943755745887756
05/28/2023 14:02:02 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.81it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.67it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.84it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.21it/s][A05/28/2023 14:02:04 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:04 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:02:04 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:04 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 145.74it/s]
05/28/2023 14:02:04 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:04 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:02:04 - INFO - __main__ -    dev: eval_loss = 0.6909541818830702
05/28/2023 14:02:04 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:04 - INFO - __main__ -    dev: infer_time = 3.335444444444444
05/28/2023 14:02:04 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:04 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:02:04 - INFO - __main__ -     cls_loss = 0.6975274838899311
05/28/2023 14:02:04 - INFO - __main__ -     eval_loss = 0.6909541818830702
05/28/2023 14:02:04 - INFO - __main__ -     global_step = 19
05/28/2023 14:02:04 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:04 - INFO - __main__ -     infer_time = 3.335444444444444
05/28/2023 14:02:04 - INFO - __main__ -     loss = 0.6975274838899311
05/28/2023 14:02:04 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.37it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.90it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.74it/s][A05/28/2023 14:02:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:05 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:02:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 145.73it/s]
05/28/2023 14:02:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:05 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:02:05 - INFO - __main__ -    dev: eval_loss = 0.6972284449471368
05/28/2023 14:02:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:05 - INFO - __main__ -    dev: infer_time = 3.3169999999999997
05/28/2023 14:02:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:05 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:02:05 - INFO - __main__ -     cls_loss = 0.6972927985520199
05/28/2023 14:02:05 - INFO - __main__ -     eval_loss = 0.6972284449471368
05/28/2023 14:02:05 - INFO - __main__ -     global_step = 29
05/28/2023 14:02:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:05 - INFO - __main__ -     infer_time = 3.3169999999999997
05/28/2023 14:02:05 - INFO - __main__ -     loss = 0.6972927985520199

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.56it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.89it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 13.22it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 15.34it/s][A05/28/2023 14:02:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:06 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:02:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 145.89it/s]
05/28/2023 14:02:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:06 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:02:06 - INFO - __main__ -    dev: eval_loss = 0.6916185617446899
05/28/2023 14:02:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:06 - INFO - __main__ -    dev: infer_time = 3.3206666666666664
05/28/2023 14:02:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:06 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:02:06 - INFO - __main__ -     cls_loss = 0.6971478553918692
05/28/2023 14:02:06 - INFO - __main__ -     eval_loss = 0.6916185617446899
05/28/2023 14:02:06 - INFO - __main__ -     global_step = 39
05/28/2023 14:02:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:06 - INFO - __main__ -     infer_time = 3.3206666666666664
05/28/2023 14:02:06 - INFO - __main__ -     loss = 0.6971478553918692

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 15.26it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:01, 17.20it/s][A
Iteration:  60%|######    | 47/78 [00:04<00:01, 18.93it/s][A05/28/2023 14:02:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:06 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:02:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 145.94it/s]
05/28/2023 14:02:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:06 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:02:06 - INFO - __main__ -    dev: eval_loss = 0.6917310754458109
05/28/2023 14:02:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:06 - INFO - __main__ -    dev: infer_time = 3.302888888888889
05/28/2023 14:02:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:06 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:02:06 - INFO - __main__ -     cls_loss = 0.6954371734541289
05/28/2023 14:02:06 - INFO - __main__ -     eval_loss = 0.6917310754458109
05/28/2023 14:02:06 - INFO - __main__ -     global_step = 49
05/28/2023 14:02:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:06 - INFO - __main__ -     infer_time = 3.302888888888889
05/28/2023 14:02:06 - INFO - __main__ -     loss = 0.6954371734541289

Iteration:  64%|######4   | 50/78 [00:05<00:01, 17.76it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 19.19it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 20.52it/s][A05/28/2023 14:02:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:07 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:02:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 146.19it/s]
05/28/2023 14:02:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:02:07 - INFO - __main__ -    dev: eval_loss = 0.6908356481128268
05/28/2023 14:02:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:07 - INFO - __main__ -    dev: infer_time = 3.310888888888889
05/28/2023 14:02:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:02:07 - INFO - __main__ -     cls_loss = 0.6959603600582834
05/28/2023 14:02:07 - INFO - __main__ -     eval_loss = 0.6908356481128268
05/28/2023 14:02:07 - INFO - __main__ -     global_step = 59
05/28/2023 14:02:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:07 - INFO - __main__ -     infer_time = 3.310888888888889
05/28/2023 14:02:07 - INFO - __main__ -     loss = 0.6959603600582834

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 18.67it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:00, 19.50it/s][A
Iteration:  83%|########3 | 65/78 [00:05<00:00, 20.77it/s][A
Iteration:  87%|########7 | 68/78 [00:05<00:00, 21.76it/s][A05/28/2023 14:02:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:07 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:02:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 146.16it/s]
05/28/2023 14:02:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:07 - INFO - __main__ -    dev: acc = 0.555956678700361
05/28/2023 14:02:07 - INFO - __main__ -    dev: eval_loss = 0.6924137671788534
05/28/2023 14:02:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:07 - INFO - __main__ -    dev: infer_time = 3.313666666666667
05/28/2023 14:02:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:07 - INFO - __main__ -     acc = 0.555956678700361
05/28/2023 14:02:07 - INFO - __main__ -     cls_loss = 0.6958490532377491
05/28/2023 14:02:07 - INFO - __main__ -     eval_loss = 0.6924137671788534
05/28/2023 14:02:07 - INFO - __main__ -     global_step = 69
05/28/2023 14:02:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:07 - INFO - __main__ -     infer_time = 3.313666666666667
05/28/2023 14:02:07 - INFO - __main__ -     loss = 0.6958490532377491
05/28/2023 14:02:07 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  91%|#########1| 71/78 [00:07<00:01,  5.33it/s][A
Iteration:  95%|#########4| 74/78 [00:07<00:00,  6.93it/s][A
Iteration:  99%|#########8| 77/78 [00:07<00:00,  8.82it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.06it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.75s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.75s/it]
05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   w_emb: 8790336

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   p_emb: 147456

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   t_emb: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ln_emb: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 184960

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 184608

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 184960

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   output_numel: 185184

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 184960

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 184608

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 184960

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   output_numel: 185184

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 184960

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 184608

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 184960

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   output_numel: 185184

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 184960

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 184608

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 184960

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   output_numel: 185184

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   layer_numel: 2814592
05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   dense_numel: 83232
05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   emb_numel: 8938944

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   encoder_numel: 2814592

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   pooler_numel: 83232

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   all parameters: 11836768

05/28/2023 14:02:09 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:02:09 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [640, 640, 640, 640], 'sample_qkv_sizes': [288, 288, 288, 288]}
parameter size = 11836768
best_acc = 0.555956678700361
time_per_batch_infer = 3.322 ms
infer_cnt = 63
**************E*************

>>> Starting Search of EE Iteration 1 ...
Namespace(arch_perfs_file='../output/kd_ptq_10/subbert.results', candidate_file='/n/home00/lbailey/bigger_and_faster/cands/kd_ptq_10.cands', ckpt_path='/n/home00/lbailey/bigger_and_faster/conf_datasets/lat_predictor_quant.pt', feature_dim=4, feature_norm=[564, 5, 1024, 564], gen_size=10, head_num_space=[1, 12], hidden_dim=2000, hidden_layer_num=3, hidden_size_space=[144, 528], intermediate_size_space=[128, 1024], lat_norm=200, latency_constraint=10.0, layer_num_space=[1, 5], method='Evolved', model='KD', output_file='../cands/1st_generation_kd_ptq_10.evo.cands', qkv_size_space=[144, 528])
Size of candidates: 999
Size of fast candidates: 98
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_qkv_sizes': [324, 324, 324, 324]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
new_arch_latency: 10.514776408672333
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [432, 432, 432, 432]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [224, 224, 224, 224], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
new_arch_latency: 11.02142482995987
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [432, 432, 432, 432]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 444, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [444, 444, 444, 444]}
new_arch_latency: 11.16565316915512
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [432, 432, 432, 432]}
after mutation, the new arch is : {'sample_layer_num': 5, 'sample_hidden_size': 420, 'sample_intermediate_sizes': [256, 256, 256, 256, 256], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [420, 420, 420, 420, 420]}
new_arch_latency: 13.424289226531982
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [432, 432, 432, 432]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [128, 128, 128, 128], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
new_arch_latency: 10.788284242153168
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.365137457847595
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_qkv_sizes': [348, 348, 348, 348]}
after mutation, the new arch is : {'sample_layer_num': 5, 'sample_hidden_size': 360, 'sample_intermediate_sizes': [608, 608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [360, 360, 360, 360, 360]}
new_arch_latency: 13.505500555038452
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_qkv_sizes': [348, 348, 348, 348]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}
new_arch_latency: 10.93536913394928
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
new_arch_latency: 9.419119358062744
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
new_arch_latency: 10.203070938587189
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.365137457847595
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.331052005290985
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [416, 416, 416, 416], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.135745465755463
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.503265261650085
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.331052005290985
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [544, 544, 544], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [252, 252, 252]}
new_arch_latency: 7.313479483127594
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 5, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264, 264]}
new_arch_latency: 11.322836577892303
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
new_arch_latency: 9.12020206451416
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 276, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_qkv_sizes': [276, 276, 276, 276]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
new_arch_latency: 9.419119358062744
old arch: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 204, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_qkv_sizes': [204, 204, 204, 204, 204]}
after mutation, the new arch is : {'sample_layer_num': 5, 'sample_hidden_size': 204, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [204, 204, 204, 204, 204]}
new_arch_latency: 10.237343609333038
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_qkv_sizes': [324, 324, 324, 324]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 312, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [312, 312, 312, 312]}
new_arch_latency: 10.34969687461853
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [448, 448, 448], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [264, 264, 264]}
new_arch_latency: 7.2220578789711
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
new_arch_latency: 9.12020206451416
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.443767368793488
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.443767368793488
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [264, 264, 264]}
new_arch_latency: 7.363875210285187
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.443767368793488
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.443767368793488
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.443767368793488
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
new_arch_latency: 9.021583199501038
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.331052005290985
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.331052005290985
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.443767368793488
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.331052005290985
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [416, 416, 416, 416], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.135745465755463
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.331052005290985
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
new_arch_latency: 9.643405675888062
>>> Starting Evaluation of EE Iteration 2 ...
05/28/2023 14:02:31 - INFO - __main__ -   device: cuda n_gpu: 1
05/28/2023 14:02:31 - INFO - __main__ -   task_lis: ['rte']
05/28/2023 14:02:31 - INFO - __main__ -   data_dir_lis: ['/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/']
05/28/2023 14:02:31 - INFO - transformer.tokenization -   loading vocabulary file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3/vocab.txt
05/28/2023 14:02:32 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:02:32 - INFO - __main__ -   subbert_configs: [{'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [512, 512, 512], 'sample_qkv_sizes': [456, 456, 456]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [492, 492, 492]}, {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [128, 128, 128, 128], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}, {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 216, 'sample_intermediate_sizes': [448, 448, 448, 448, 448], 'sample_qkv_sizes': [216, 216, 216, 216, 216]}, {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 156, 'sample_intermediate_sizes': [896, 896, 896, 896], 'sample_qkv_sizes': [156, 156, 156, 156]}, {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 228, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_qkv_sizes': [228, 228, 228, 228]}, {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 396, 'sample_intermediate_sizes': [704, 704, 704], 'sample_qkv_sizes': [396, 396, 396]}, {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [928, 928, 928, 928], 'sample_qkv_sizes': [264, 264, 264, 264]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [352, 352, 352, 352], 'sample_qkv_sizes': [252, 252, 252, 252]}, {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [416, 416, 416, 416], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 528, 'sample_intermediate_sizes': [640, 640, 640], 'sample_qkv_sizes': [528, 528, 528]}, {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}, {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}, {'sample_layer_num': 5, 'sample_hidden_size': 204, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [204, 204, 204, 204, 204]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [736, 736, 736, 736], 'sample_qkv_sizes': [252, 252, 252, 252]}, {'sample_layer_num': 4, 'sample_hidden_size': 312, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [312, 312, 312, 312]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [576, 576, 576], 'sample_qkv_sizes': [456, 456, 456]}, {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}, {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 360, 'sample_intermediate_sizes': [928, 928, 928], 'sample_qkv_sizes': [360, 360, 360]}]
05/28/2023 14:02:32 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
05/28/2023 14:02:32 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:02:32 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:02:32 - INFO - __main__ -   *** Example ***
05/28/2023 14:02:32 - INFO - __main__ -   guid: train-0
05/28/2023 14:02:32 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:02:32 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:32 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:32 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:32 - INFO - __main__ -   label: not_entailment
05/28/2023 14:02:32 - INFO - __main__ -   label_id: 1
05/28/2023 14:02:33 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:02:33 - INFO - __main__ -   *** Example ***
05/28/2023 14:02:33 - INFO - __main__ -   guid: dev-0
05/28/2023 14:02:33 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:02:33 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:33 - INFO - __main__ -   label: not_entailment
05/28/2023 14:02:33 - INFO - __main__ -   label_id: 1
05/28/2023 14:02:33 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:02:33 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:02:34 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:02:37 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:02:37 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:02:37 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:02:37 - INFO - __main__ -     Batch size = 32
05/28/2023 14:02:37 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A/n/home00/lbailey/bigger_and_faster/transformer/optimization.py:248: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  next_m.mul_(beta1).add_(1 - beta1, grad)

Iteration:   3%|2         | 2/78 [00:00<00:05, 14.66it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 18.78it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 20.33it/s][A05/28/2023 14:02:38 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:38 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:02:38 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:38 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 129.72it/s]
05/28/2023 14:02:38 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:38 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:02:38 - INFO - __main__ -    dev: eval_loss = 0.7128724720742967
05/28/2023 14:02:38 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:38 - INFO - __main__ -    dev: infer_time = 3.5446666666666666
05/28/2023 14:02:38 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:38 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:02:38 - INFO - __main__ -     cls_loss = 0.6982901626163058
05/28/2023 14:02:38 - INFO - __main__ -     eval_loss = 0.7128724720742967
05/28/2023 14:02:38 - INFO - __main__ -     global_step = 9
05/28/2023 14:02:38 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:38 - INFO - __main__ -     infer_time = 3.5446666666666666
05/28/2023 14:02:38 - INFO - __main__ -     loss = 0.6982901626163058
05/28/2023 14:02:38 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:01<00:16,  4.19it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:10,  5.99it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:07,  8.01it/s][A05/28/2023 14:02:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:39 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:02:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 129.93it/s]
05/28/2023 14:02:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:40 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:02:40 - INFO - __main__ -    dev: eval_loss = 0.7147975100411309
05/28/2023 14:02:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:40 - INFO - __main__ -    dev: infer_time = 3.5298888888888893
05/28/2023 14:02:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:40 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:02:40 - INFO - __main__ -     cls_loss = 0.7111953120482596
05/28/2023 14:02:40 - INFO - __main__ -     eval_loss = 0.7147975100411309
05/28/2023 14:02:40 - INFO - __main__ -     global_step = 19
05/28/2023 14:02:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:40 - INFO - __main__ -     infer_time = 3.5298888888888893
05/28/2023 14:02:40 - INFO - __main__ -     loss = 0.7111953120482596

Iteration:  24%|##4       | 19/78 [00:02<00:06,  8.72it/s][A
Iteration:  28%|##8       | 22/78 [00:02<00:05, 11.01it/s][A
Iteration:  32%|###2      | 25/78 [00:02<00:04, 13.17it/s][A
Iteration:  36%|###5      | 28/78 [00:02<00:03, 15.19it/s][A05/28/2023 14:02:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:40 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:02:40 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 129.93it/s]
05/28/2023 14:02:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:40 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:02:40 - INFO - __main__ -    dev: eval_loss = 0.6944478419091966
05/28/2023 14:02:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:40 - INFO - __main__ -    dev: infer_time = 3.5265555555555554
05/28/2023 14:02:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:40 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:02:40 - INFO - __main__ -     cls_loss = 0.7058020028574713
05/28/2023 14:02:40 - INFO - __main__ -     eval_loss = 0.6944478419091966
05/28/2023 14:02:40 - INFO - __main__ -     global_step = 29
05/28/2023 14:02:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:40 - INFO - __main__ -     infer_time = 3.5265555555555554
05/28/2023 14:02:40 - INFO - __main__ -     loss = 0.7058020028574713

Iteration:  40%|###9      | 31/78 [00:02<00:03, 14.79it/s][A
Iteration:  44%|####3     | 34/78 [00:03<00:02, 16.45it/s][A
Iteration:  47%|####7     | 37/78 [00:03<00:02, 17.88it/s][A05/28/2023 14:02:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:41 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:02:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.12it/s]
05/28/2023 14:02:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:41 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 14:02:41 - INFO - __main__ -    dev: eval_loss = 0.6932982669936286
05/28/2023 14:02:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:41 - INFO - __main__ -    dev: infer_time = 3.5135555555555555
05/28/2023 14:02:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:41 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 14:02:41 - INFO - __main__ -     cls_loss = 0.7024475840421823
05/28/2023 14:02:41 - INFO - __main__ -     eval_loss = 0.6932982669936286
05/28/2023 14:02:41 - INFO - __main__ -     global_step = 39
05/28/2023 14:02:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:41 - INFO - __main__ -     infer_time = 3.5135555555555555
05/28/2023 14:02:41 - INFO - __main__ -     loss = 0.7024475840421823

Iteration:  51%|#####1    | 40/78 [00:03<00:02, 16.38it/s][A
Iteration:  55%|#####5    | 43/78 [00:03<00:01, 17.85it/s][A
Iteration:  59%|#####8    | 46/78 [00:03<00:01, 18.83it/s][A05/28/2023 14:02:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:41 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:02:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 129.76it/s]
05/28/2023 14:02:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:41 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 14:02:41 - INFO - __main__ -    dev: eval_loss = 0.6927360428704156
05/28/2023 14:02:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:41 - INFO - __main__ -    dev: infer_time = 3.5276666666666667
05/28/2023 14:02:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:41 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 14:02:41 - INFO - __main__ -     cls_loss = 0.700451736547509
05/28/2023 14:02:41 - INFO - __main__ -     eval_loss = 0.6927360428704156
05/28/2023 14:02:41 - INFO - __main__ -     global_step = 49
05/28/2023 14:02:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:41 - INFO - __main__ -     infer_time = 3.5276666666666667
05/28/2023 14:02:41 - INFO - __main__ -     loss = 0.700451736547509
05/28/2023 14:02:41 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  63%|######2   | 49/78 [00:05<00:05,  5.28it/s][A
Iteration:  67%|######6   | 52/78 [00:05<00:03,  6.80it/s][A
Iteration:  71%|#######   | 55/78 [00:05<00:02,  8.59it/s][A
Iteration:  74%|#######4  | 58/78 [00:05<00:01, 10.50it/s][A05/28/2023 14:02:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:43 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:02:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 129.89it/s]
05/28/2023 14:02:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:43 - INFO - __main__ -    dev: acc = 0.5126353790613718
05/28/2023 14:02:43 - INFO - __main__ -    dev: eval_loss = 0.6925204594930013
05/28/2023 14:02:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:43 - INFO - __main__ -    dev: infer_time = 3.5203333333333333
05/28/2023 14:02:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:43 - INFO - __main__ -     acc = 0.5126353790613718
05/28/2023 14:02:43 - INFO - __main__ -     cls_loss = 0.6991552377151231
05/28/2023 14:02:43 - INFO - __main__ -     eval_loss = 0.6925204594930013
05/28/2023 14:02:43 - INFO - __main__ -     global_step = 59
05/28/2023 14:02:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:43 - INFO - __main__ -     infer_time = 3.5203333333333333
05/28/2023 14:02:43 - INFO - __main__ -     loss = 0.6991552377151231

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 10.82it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:01, 12.91it/s][A
Iteration:  85%|########4 | 66/78 [00:06<00:00, 14.77it/s][A05/28/2023 14:02:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:43 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:02:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.02it/s]
05/28/2023 14:02:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:44 - INFO - __main__ -    dev: acc = 0.5523465703971119
05/28/2023 14:02:44 - INFO - __main__ -    dev: eval_loss = 0.6914646426836649
05/28/2023 14:02:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:44 - INFO - __main__ -    dev: infer_time = 3.5060000000000002
05/28/2023 14:02:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:44 - INFO - __main__ -     acc = 0.5523465703971119
05/28/2023 14:02:44 - INFO - __main__ -     cls_loss = 0.6980729172195214
05/28/2023 14:02:44 - INFO - __main__ -     eval_loss = 0.6914646426836649
05/28/2023 14:02:44 - INFO - __main__ -     global_step = 69
05/28/2023 14:02:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:44 - INFO - __main__ -     infer_time = 3.5060000000000002
05/28/2023 14:02:44 - INFO - __main__ -     loss = 0.6980729172195214
05/28/2023 14:02:44 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  88%|########8 | 69/78 [00:07<00:01,  4.92it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:00,  6.45it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00,  8.22it/s][AIteration: 100%|##########| 78/78 [00:08<00:00,  9.73it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.02s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.02s/it]
05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   w_emb: 9889128

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   p_emb: 165888

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   t_emb: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ln_emb: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 187200

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 186948

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 187200

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   output_numel: 187596

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 187200

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 186948

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 187200

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   output_numel: 187596

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 187200

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 186948

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 187200

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   output_numel: 187596

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 187200

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 186948

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 187200

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   output_numel: 187596

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   layer_numel: 3186576
05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   dense_numel: 105300
05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   emb_numel: 10056312

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   encoder_numel: 3186576

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   pooler_numel: 105300

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   all parameters: 13348188

05/28/2023 14:02:45 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:02:45 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
parameter size = 13348188
best_acc = 0.5523465703971119
time_per_batch_infer = 3.524 ms
infer_cnt = 63
**************E*************

05/28/2023 14:02:45 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [512, 512, 512], 'sample_qkv_sizes': [456, 456, 456]}
05/28/2023 14:02:45 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:02:45 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:02:45 - INFO - __main__ -   *** Example ***
05/28/2023 14:02:45 - INFO - __main__ -   guid: train-0
05/28/2023 14:02:45 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:02:45 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:45 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:45 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:45 - INFO - __main__ -   label: not_entailment
05/28/2023 14:02:45 - INFO - __main__ -   label_id: 1
05/28/2023 14:02:47 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:02:47 - INFO - __main__ -   *** Example ***
05/28/2023 14:02:47 - INFO - __main__ -   guid: dev-0
05/28/2023 14:02:47 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:02:47 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:47 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:47 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:47 - INFO - __main__ -   label: not_entailment
05/28/2023 14:02:47 - INFO - __main__ -   label_id: 1
05/28/2023 14:02:47 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:02:47 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:02:48 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:02:48 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:02:48 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:02:48 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:02:48 - INFO - __main__ -     Batch size = 32
05/28/2023 14:02:48 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.88it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.02it/s][A05/28/2023 14:02:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:48 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:02:48 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.58it/s]
05/28/2023 14:02:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:48 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:02:48 - INFO - __main__ -    dev: eval_loss = 0.691701180405087
05/28/2023 14:02:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:48 - INFO - __main__ -    dev: infer_time = 2.8232222222222223
05/28/2023 14:02:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:48 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:02:48 - INFO - __main__ -     cls_loss = 0.6956196427345276
05/28/2023 14:02:48 - INFO - __main__ -     eval_loss = 0.691701180405087
05/28/2023 14:02:48 - INFO - __main__ -     global_step = 9
05/28/2023 14:02:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:48 - INFO - __main__ -     infer_time = 2.8232222222222223
05/28/2023 14:02:48 - INFO - __main__ -     loss = 0.6956196427345276
05/28/2023 14:02:48 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.98it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.94it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.19it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.62it/s][A05/28/2023 14:02:50 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:50 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:02:50 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:50 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.14it/s]
05/28/2023 14:02:50 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:50 - INFO - __main__ -    dev: acc = 0.5451263537906137
05/28/2023 14:02:50 - INFO - __main__ -    dev: eval_loss = 0.6910386814011468
05/28/2023 14:02:50 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:50 - INFO - __main__ -    dev: infer_time = 2.829
05/28/2023 14:02:50 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:50 - INFO - __main__ -     acc = 0.5451263537906137
05/28/2023 14:02:50 - INFO - __main__ -     cls_loss = 0.6993097004137541
05/28/2023 14:02:50 - INFO - __main__ -     eval_loss = 0.6910386814011468
05/28/2023 14:02:50 - INFO - __main__ -     global_step = 19
05/28/2023 14:02:50 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:50 - INFO - __main__ -     infer_time = 2.829
05/28/2023 14:02:50 - INFO - __main__ -     loss = 0.6993097004137541
05/28/2023 14:02:50 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.44it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:08,  6.02it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.89it/s][A05/28/2023 14:02:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:52 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:02:52 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.28it/s]
05/28/2023 14:02:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:52 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:02:52 - INFO - __main__ -    dev: eval_loss = 0.6964522335264418
05/28/2023 14:02:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:52 - INFO - __main__ -    dev: infer_time = 2.7993333333333332
05/28/2023 14:02:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:52 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:02:52 - INFO - __main__ -     cls_loss = 0.6939963513407214
05/28/2023 14:02:52 - INFO - __main__ -     eval_loss = 0.6964522335264418
05/28/2023 14:02:52 - INFO - __main__ -     global_step = 29
05/28/2023 14:02:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:52 - INFO - __main__ -     infer_time = 2.7993333333333332
05/28/2023 14:02:52 - INFO - __main__ -     loss = 0.6939963513407214

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.21it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.42it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.68it/s][A05/28/2023 14:02:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:52 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:02:52 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.44it/s]
05/28/2023 14:02:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:52 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:02:52 - INFO - __main__ -    dev: eval_loss = 0.6880336801211039
05/28/2023 14:02:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:52 - INFO - __main__ -    dev: infer_time = 2.7948888888888894
05/28/2023 14:02:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:52 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:02:52 - INFO - __main__ -     cls_loss = 0.6942104926476111
05/28/2023 14:02:52 - INFO - __main__ -     eval_loss = 0.6880336801211039
05/28/2023 14:02:52 - INFO - __main__ -     global_step = 39
05/28/2023 14:02:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:52 - INFO - __main__ -     infer_time = 2.7948888888888894
05/28/2023 14:02:52 - INFO - __main__ -     loss = 0.6942104926476111

Iteration:  50%|#####     | 39/78 [00:04<00:02, 14.13it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 16.11it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 17.96it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 19.61it/s][A05/28/2023 14:02:53 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:53 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:02:53 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:53 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.62it/s]
05/28/2023 14:02:53 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:53 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:02:53 - INFO - __main__ -    dev: eval_loss = 0.7022690441873338
05/28/2023 14:02:53 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:53 - INFO - __main__ -    dev: infer_time = 2.800111111111111
05/28/2023 14:02:53 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:53 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:02:53 - INFO - __main__ -     cls_loss = 0.6932821894178585
05/28/2023 14:02:53 - INFO - __main__ -     eval_loss = 0.7022690441873338
05/28/2023 14:02:53 - INFO - __main__ -     global_step = 49
05/28/2023 14:02:53 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:53 - INFO - __main__ -     infer_time = 2.800111111111111
05/28/2023 14:02:53 - INFO - __main__ -     loss = 0.6932821894178585

Iteration:  65%|######5   | 51/78 [00:05<00:01, 17.87it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 19.52it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 20.86it/s][A05/28/2023 14:02:53 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:53 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:02:53 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:53 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.30it/s]
05/28/2023 14:02:53 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:53 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:02:53 - INFO - __main__ -    dev: eval_loss = 0.6987452838155959
05/28/2023 14:02:53 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:53 - INFO - __main__ -    dev: infer_time = 2.7975555555555554
05/28/2023 14:02:53 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:53 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:02:53 - INFO - __main__ -     cls_loss = 0.6929353364443375
05/28/2023 14:02:53 - INFO - __main__ -     eval_loss = 0.6987452838155959
05/28/2023 14:02:53 - INFO - __main__ -     global_step = 59
05/28/2023 14:02:53 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:53 - INFO - __main__ -     infer_time = 2.7975555555555554
05/28/2023 14:02:53 - INFO - __main__ -     loss = 0.6929353364443375

Iteration:  77%|#######6  | 60/78 [00:05<00:00, 18.58it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 20.12it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 21.35it/s][A05/28/2023 14:02:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:54 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:02:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.27it/s]
05/28/2023 14:02:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:54 - INFO - __main__ -    dev: acc = 0.5451263537906137
05/28/2023 14:02:54 - INFO - __main__ -    dev: eval_loss = 0.6909202006128099
05/28/2023 14:02:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:54 - INFO - __main__ -    dev: infer_time = 2.8096666666666668
05/28/2023 14:02:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:54 - INFO - __main__ -     acc = 0.5451263537906137
05/28/2023 14:02:54 - INFO - __main__ -     cls_loss = 0.6936452095059381
05/28/2023 14:02:54 - INFO - __main__ -     eval_loss = 0.6909202006128099
05/28/2023 14:02:54 - INFO - __main__ -     global_step = 69
05/28/2023 14:02:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:54 - INFO - __main__ -     infer_time = 2.8096666666666668
05/28/2023 14:02:54 - INFO - __main__ -     loss = 0.6936452095059381

Iteration:  88%|########8 | 69/78 [00:05<00:00, 19.05it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 20.27it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 21.47it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.46it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.26s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.26s/it]
05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   w_emb: 13918032

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   p_emb: 233472

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   t_emb: 912

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   ln_emb: 912

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 233984

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 233928

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   intermediate_numel: 233984

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   output_numel: 234840

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 233984

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 233928

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   intermediate_numel: 233984

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   output_numel: 234840

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 233984

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 233928

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   intermediate_numel: 233984

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   output_numel: 234840

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   layer_numel: 3909912
05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   dense_numel: 208392
05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   emb_numel: 14153328

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   encoder_numel: 3909912

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   pooler_numel: 208392

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   all parameters: 18271632

05/28/2023 14:02:54 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:02:54 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [512, 512, 512], 'sample_qkv_sizes': [456, 456, 456]}
parameter size = 18271632
best_acc = 0.5451263537906137
time_per_batch_infer = 2.808 ms
infer_cnt = 63
**************E*************

05/28/2023 14:02:54 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [492, 492, 492]}
05/28/2023 14:02:54 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:02:54 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:02:54 - INFO - __main__ -   *** Example ***
05/28/2023 14:02:54 - INFO - __main__ -   guid: train-0
05/28/2023 14:02:54 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:02:54 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:54 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:54 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:54 - INFO - __main__ -   label: not_entailment
05/28/2023 14:02:54 - INFO - __main__ -   label_id: 1
05/28/2023 14:02:56 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:02:56 - INFO - __main__ -   *** Example ***
05/28/2023 14:02:56 - INFO - __main__ -   guid: dev-0
05/28/2023 14:02:56 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:02:56 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:56 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:56 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:02:56 - INFO - __main__ -   label: not_entailment
05/28/2023 14:02:56 - INFO - __main__ -   label_id: 1
05/28/2023 14:02:56 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:02:56 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:02:57 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:02:57 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:02:57 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:02:57 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:02:57 - INFO - __main__ -     Batch size = 32
05/28/2023 14:02:57 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.94it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.99it/s][A05/28/2023 14:02:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:57 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:02:57 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.27it/s]
05/28/2023 14:02:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:57 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:02:57 - INFO - __main__ -    dev: eval_loss = 0.6912609272532992
05/28/2023 14:02:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:57 - INFO - __main__ -    dev: infer_time = 2.8088888888888888
05/28/2023 14:02:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:57 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:02:57 - INFO - __main__ -     cls_loss = 0.7069068087471856
05/28/2023 14:02:57 - INFO - __main__ -     eval_loss = 0.6912609272532992
05/28/2023 14:02:57 - INFO - __main__ -     global_step = 9
05/28/2023 14:02:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:57 - INFO - __main__ -     infer_time = 2.8088888888888888
05/28/2023 14:02:57 - INFO - __main__ -     loss = 0.7069068087471856
05/28/2023 14:02:57 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.88it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.72it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.79it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.00it/s][A05/28/2023 14:02:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:59 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:02:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.24it/s]
05/28/2023 14:02:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:59 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:02:59 - INFO - __main__ -    dev: eval_loss = 0.696112248632643
05/28/2023 14:02:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:59 - INFO - __main__ -    dev: infer_time = 2.8406666666666665
05/28/2023 14:02:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:59 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:02:59 - INFO - __main__ -     cls_loss = 0.7054637890113028
05/28/2023 14:02:59 - INFO - __main__ -     eval_loss = 0.696112248632643
05/28/2023 14:02:59 - INFO - __main__ -     global_step = 19
05/28/2023 14:02:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:59 - INFO - __main__ -     infer_time = 2.8406666666666665
05/28/2023 14:02:59 - INFO - __main__ -     loss = 0.7054637890113028

Iteration:  26%|##5       | 20/78 [00:02<00:05, 10.23it/s][A
Iteration:  29%|##9       | 23/78 [00:02<00:04, 12.56it/s][A
Iteration:  33%|###3      | 26/78 [00:02<00:03, 14.64it/s][A05/28/2023 14:02:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:02:59 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:02:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:02:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.39it/s]
05/28/2023 14:02:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:02:59 - INFO - __main__ -    dev: acc = 0.5487364620938628
05/28/2023 14:02:59 - INFO - __main__ -    dev: eval_loss = 0.6921253866619534
05/28/2023 14:02:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:02:59 - INFO - __main__ -    dev: infer_time = 2.8308888888888886
05/28/2023 14:02:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:02:59 - INFO - __main__ -     acc = 0.5487364620938628
05/28/2023 14:02:59 - INFO - __main__ -     cls_loss = 0.7026816791501539
05/28/2023 14:02:59 - INFO - __main__ -     eval_loss = 0.6921253866619534
05/28/2023 14:02:59 - INFO - __main__ -     global_step = 29
05/28/2023 14:02:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:02:59 - INFO - __main__ -     infer_time = 2.8308888888888886
05/28/2023 14:02:59 - INFO - __main__ -     loss = 0.7026816791501539
05/28/2023 14:02:59 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  37%|###7      | 29/78 [00:04<00:10,  4.82it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:07,  6.35it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:05,  8.16it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:03, 10.08it/s][A05/28/2023 14:03:01 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:01 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:03:01 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:01 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.04it/s]
05/28/2023 14:03:01 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:01 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:01 - INFO - __main__ -    dev: eval_loss = 0.6907623410224915
05/28/2023 14:03:01 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:01 - INFO - __main__ -    dev: infer_time = 2.853888888888889
05/28/2023 14:03:01 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:01 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:01 - INFO - __main__ -     cls_loss = 0.7007466233693637
05/28/2023 14:03:01 - INFO - __main__ -     eval_loss = 0.6907623410224915
05/28/2023 14:03:01 - INFO - __main__ -     global_step = 39
05/28/2023 14:03:01 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:01 - INFO - __main__ -     infer_time = 2.853888888888889
05/28/2023 14:03:01 - INFO - __main__ -     loss = 0.7007466233693637

Iteration:  51%|#####1    | 40/78 [00:04<00:03, 10.27it/s][A
Iteration:  55%|#####5    | 43/78 [00:04<00:02, 12.43it/s][A
Iteration:  59%|#####8    | 46/78 [00:05<00:02, 14.46it/s][A05/28/2023 14:03:02 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:02 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:03:02 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:02 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.33it/s]
05/28/2023 14:03:02 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:02 - INFO - __main__ -    dev: acc = 0.5451263537906137
05/28/2023 14:03:02 - INFO - __main__ -    dev: eval_loss = 0.6919441488054063
05/28/2023 14:03:02 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:02 - INFO - __main__ -    dev: infer_time = 2.820222222222222
05/28/2023 14:03:02 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:02 - INFO - __main__ -     acc = 0.5451263537906137
05/28/2023 14:03:02 - INFO - __main__ -     cls_loss = 0.6992225428016818
05/28/2023 14:03:02 - INFO - __main__ -     eval_loss = 0.6919441488054063
05/28/2023 14:03:02 - INFO - __main__ -     global_step = 49
05/28/2023 14:03:02 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:02 - INFO - __main__ -     infer_time = 2.820222222222222
05/28/2023 14:03:02 - INFO - __main__ -     loss = 0.6992225428016818

Iteration:  63%|######2   | 49/78 [00:05<00:02, 14.14it/s][A
Iteration:  67%|######6   | 52/78 [00:05<00:01, 15.76it/s][A
Iteration:  71%|#######   | 55/78 [00:05<00:01, 17.27it/s][A
Iteration:  74%|#######4  | 58/78 [00:05<00:01, 18.47it/s][A05/28/2023 14:03:02 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:02 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:03:02 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:02 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.34it/s]
05/28/2023 14:03:02 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:02 - INFO - __main__ -    dev: acc = 0.48014440433212996
05/28/2023 14:03:02 - INFO - __main__ -    dev: eval_loss = 0.6931263274616666
05/28/2023 14:03:02 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:02 - INFO - __main__ -    dev: infer_time = 2.802666666666667
05/28/2023 14:03:02 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:02 - INFO - __main__ -     acc = 0.48014440433212996
05/28/2023 14:03:02 - INFO - __main__ -     cls_loss = 0.6983764313035092
05/28/2023 14:03:02 - INFO - __main__ -     eval_loss = 0.6931263274616666
05/28/2023 14:03:02 - INFO - __main__ -     global_step = 59
05/28/2023 14:03:02 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:02 - INFO - __main__ -     infer_time = 2.802666666666667
05/28/2023 14:03:02 - INFO - __main__ -     loss = 0.6983764313035092

Iteration:  78%|#######8  | 61/78 [00:05<00:01, 16.52it/s][A
Iteration:  82%|########2 | 64/78 [00:06<00:00, 17.88it/s][A
Iteration:  86%|########5 | 67/78 [00:06<00:00, 19.00it/s][A05/28/2023 14:03:03 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:03 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:03:03 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:03 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.74it/s]
05/28/2023 14:03:03 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:03 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:03 - INFO - __main__ -    dev: eval_loss = 0.6940955320994059
05/28/2023 14:03:03 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:03 - INFO - __main__ -    dev: infer_time = 2.8356666666666666
05/28/2023 14:03:03 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:03 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:03 - INFO - __main__ -     cls_loss = 0.6976538360982701
05/28/2023 14:03:03 - INFO - __main__ -     eval_loss = 0.6940955320994059
05/28/2023 14:03:03 - INFO - __main__ -     global_step = 69
05/28/2023 14:03:03 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:03 - INFO - __main__ -     infer_time = 2.8356666666666666
05/28/2023 14:03:03 - INFO - __main__ -     loss = 0.6976538360982701

Iteration:  90%|########9 | 70/78 [00:06<00:00, 16.77it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 18.12it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 19.06it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.59it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.73s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.73s/it]
05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   w_emb: 15016824

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   p_emb: 251904

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   t_emb: 984

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   ln_emb: 984

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 425952

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 425580

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   intermediate_numel: 425952

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   output_numel: 426564

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 425952

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 425580

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   intermediate_numel: 425952

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   output_numel: 426564

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 425952

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 425580

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   intermediate_numel: 425952

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   output_numel: 426564

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   layer_numel: 5471172
05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   dense_numel: 242556
05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   emb_numel: 15270696

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   encoder_numel: 5471172

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   pooler_numel: 242556

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   all parameters: 20984424

05/28/2023 14:03:03 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:03 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [492, 492, 492]}
parameter size = 20984424
best_acc = 0.5487364620938628
time_per_batch_infer = 2.828 ms
infer_cnt = 63
**************E*************

05/28/2023 14:03:03 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [128, 128, 128, 128], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
05/28/2023 14:03:03 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:03:03 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:03:03 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:03 - INFO - __main__ -   guid: train-0
05/28/2023 14:03:03 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:03:03 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:03 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:03 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:03 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:03 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:05 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:03:05 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:05 - INFO - __main__ -   guid: dev-0
05/28/2023 14:03:05 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:03:05 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:05 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:05 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:05 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:05 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:05 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:03:05 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:03:06 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:03:06 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:03:06 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:03:06 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:03:06 - INFO - __main__ -     Batch size = 32
05/28/2023 14:03:06 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   3%|2         | 2/78 [00:00<00:03, 19.96it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 20.78it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 21.02it/s][A05/28/2023 14:03:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:06 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:03:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.17it/s]
05/28/2023 14:03:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:06 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:06 - INFO - __main__ -    dev: eval_loss = 0.7048803567886353
05/28/2023 14:03:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:06 - INFO - __main__ -    dev: infer_time = 3.5672222222222225
05/28/2023 14:03:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:06 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:06 - INFO - __main__ -     cls_loss = 0.6928101314438714
05/28/2023 14:03:06 - INFO - __main__ -     eval_loss = 0.7048803567886353
05/28/2023 14:03:06 - INFO - __main__ -     global_step = 9
05/28/2023 14:03:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:06 - INFO - __main__ -     infer_time = 3.5672222222222225
05/28/2023 14:03:06 - INFO - __main__ -     loss = 0.6928101314438714
05/28/2023 14:03:06 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:01<00:15,  4.22it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:10,  6.01it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:07,  8.02it/s][A05/28/2023 14:03:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:08 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:03:08 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.40it/s]
05/28/2023 14:03:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:08 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:08 - INFO - __main__ -    dev: eval_loss = 0.7041481799549527
05/28/2023 14:03:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:08 - INFO - __main__ -    dev: infer_time = 3.5345555555555555
05/28/2023 14:03:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:08 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:08 - INFO - __main__ -     cls_loss = 0.6934227692453485
05/28/2023 14:03:08 - INFO - __main__ -     eval_loss = 0.7041481799549527
05/28/2023 14:03:08 - INFO - __main__ -     global_step = 19
05/28/2023 14:03:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:08 - INFO - __main__ -     infer_time = 3.5345555555555555
05/28/2023 14:03:08 - INFO - __main__ -     loss = 0.6934227692453485

Iteration:  24%|##4       | 19/78 [00:02<00:06,  8.63it/s][A
Iteration:  28%|##8       | 22/78 [00:02<00:05, 10.86it/s][A
Iteration:  32%|###2      | 25/78 [00:02<00:04, 13.04it/s][A
Iteration:  36%|###5      | 28/78 [00:02<00:03, 14.99it/s][A05/28/2023 14:03:09 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:09 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:03:09 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:09 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.41it/s]
05/28/2023 14:03:09 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:09 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:09 - INFO - __main__ -    dev: eval_loss = 0.6941571566793654
05/28/2023 14:03:09 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:09 - INFO - __main__ -    dev: infer_time = 3.528666666666666
05/28/2023 14:03:09 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:09 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:09 - INFO - __main__ -     cls_loss = 0.6929017634227358
05/28/2023 14:03:09 - INFO - __main__ -     eval_loss = 0.6941571566793654
05/28/2023 14:03:09 - INFO - __main__ -     global_step = 29
05/28/2023 14:03:09 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:09 - INFO - __main__ -     infer_time = 3.528666666666666
05/28/2023 14:03:09 - INFO - __main__ -     loss = 0.6929017634227358
05/28/2023 14:03:09 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:10,  4.49it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:07,  6.09it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:05,  7.94it/s][A05/28/2023 14:03:11 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:11 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:03:11 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:11 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.38it/s]
05/28/2023 14:03:11 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:11 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 14:03:11 - INFO - __main__ -    dev: eval_loss = 0.6921499305301242
05/28/2023 14:03:11 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:11 - INFO - __main__ -    dev: infer_time = 3.544888888888889
05/28/2023 14:03:11 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:11 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 14:03:11 - INFO - __main__ -     cls_loss = 0.6964222131631314
05/28/2023 14:03:11 - INFO - __main__ -     eval_loss = 0.6921499305301242
05/28/2023 14:03:11 - INFO - __main__ -     global_step = 39
05/28/2023 14:03:11 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:11 - INFO - __main__ -     infer_time = 3.544888888888889
05/28/2023 14:03:11 - INFO - __main__ -     loss = 0.6964222131631314
05/28/2023 14:03:11 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:06<00:09,  4.07it/s][A
Iteration:  54%|#####3    | 42/78 [00:06<00:06,  5.42it/s][A
Iteration:  58%|#####7    | 45/78 [00:06<00:04,  7.03it/s][A
Iteration:  62%|######1   | 48/78 [00:06<00:03,  8.83it/s][A05/28/2023 14:03:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:12 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:03:12 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.35it/s]
05/28/2023 14:03:12 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:12 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:12 - INFO - __main__ -    dev: eval_loss = 0.6961139904128181
05/28/2023 14:03:12 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:12 - INFO - __main__ -    dev: infer_time = 3.5475555555555554
05/28/2023 14:03:12 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:12 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:12 - INFO - __main__ -     cls_loss = 0.6973554467668339
05/28/2023 14:03:12 - INFO - __main__ -     eval_loss = 0.6961139904128181
05/28/2023 14:03:12 - INFO - __main__ -     global_step = 49
05/28/2023 14:03:12 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:12 - INFO - __main__ -     infer_time = 3.5475555555555554
05/28/2023 14:03:12 - INFO - __main__ -     loss = 0.6973554467668339

Iteration:  64%|######4   | 50/78 [00:06<00:03,  9.27it/s][A
Iteration:  68%|######7   | 53/78 [00:06<00:02, 11.35it/s][A
Iteration:  72%|#######1  | 56/78 [00:06<00:01, 13.38it/s][A05/28/2023 14:03:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:13 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:03:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.44it/s]
05/28/2023 14:03:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:13 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:13 - INFO - __main__ -    dev: eval_loss = 0.6903657184706794
05/28/2023 14:03:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:13 - INFO - __main__ -    dev: infer_time = 3.5397777777777777
05/28/2023 14:03:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:13 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:13 - INFO - __main__ -     cls_loss = 0.6967235884423983
05/28/2023 14:03:13 - INFO - __main__ -     eval_loss = 0.6903657184706794
05/28/2023 14:03:13 - INFO - __main__ -     global_step = 59
05/28/2023 14:03:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:13 - INFO - __main__ -     infer_time = 3.5397777777777777
05/28/2023 14:03:13 - INFO - __main__ -     loss = 0.6967235884423983

Iteration:  76%|#######5  | 59/78 [00:07<00:01, 13.54it/s][A
Iteration:  79%|#######9  | 62/78 [00:07<00:01, 15.27it/s][A
Iteration:  83%|########3 | 65/78 [00:07<00:00, 16.73it/s][A
Iteration:  87%|########7 | 68/78 [00:07<00:00, 17.94it/s][A05/28/2023 14:03:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:13 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:03:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.55it/s]
05/28/2023 14:03:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:14 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:14 - INFO - __main__ -    dev: eval_loss = 0.6912894580099318
05/28/2023 14:03:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:14 - INFO - __main__ -    dev: infer_time = 3.539555555555556
05/28/2023 14:03:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:14 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:14 - INFO - __main__ -     cls_loss = 0.6960061840389086
05/28/2023 14:03:14 - INFO - __main__ -     eval_loss = 0.6912894580099318
05/28/2023 14:03:14 - INFO - __main__ -     global_step = 69
05/28/2023 14:03:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:14 - INFO - __main__ -     infer_time = 3.539555555555556
05/28/2023 14:03:14 - INFO - __main__ -     loss = 0.6960061840389086

Iteration:  91%|#########1| 71/78 [00:07<00:00, 16.45it/s][A
Iteration:  95%|#########4| 74/78 [00:07<00:00, 17.71it/s][A
Iteration:  99%|#########8| 77/78 [00:08<00:00, 18.44it/s][AIteration: 100%|##########| 78/78 [00:08<00:00,  9.68it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.06s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.06s/it]
05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   w_emb: 13185504

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   p_emb: 221184

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   t_emb: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ln_emb: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 55424

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 55728

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   intermediate_numel: 55424

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   output_numel: 56592

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 55424

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 55728

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   intermediate_numel: 55424

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   output_numel: 56592

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 55424

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 55728

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   intermediate_numel: 55424

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   output_numel: 56592

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 55424

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 55728

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   intermediate_numel: 55424

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   output_numel: 56592

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   layer_numel: 3444416
05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   dense_numel: 187056
05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   emb_numel: 13408416

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   encoder_numel: 3444416

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   pooler_numel: 187056

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   all parameters: 17039888

05/28/2023 14:03:14 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:14 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [128, 128, 128, 128], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
parameter size = 17039888
best_acc = 0.5342960288808665
time_per_batch_infer = 3.543 ms
infer_cnt = 63
**************E*************

05/28/2023 14:03:14 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 216, 'sample_intermediate_sizes': [448, 448, 448, 448, 448], 'sample_qkv_sizes': [216, 216, 216, 216, 216]}
05/28/2023 14:03:14 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:03:14 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:03:14 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:14 - INFO - __main__ -   guid: train-0
05/28/2023 14:03:14 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:03:14 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:14 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:14 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:14 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:14 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:16 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:03:16 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:16 - INFO - __main__ -   guid: dev-0
05/28/2023 14:03:16 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:03:16 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:16 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:16 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:16 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:16 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:16 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:03:16 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:03:16 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:03:17 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:03:17 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:03:17 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:03:17 - INFO - __main__ -     Batch size = 32
05/28/2023 14:03:17 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.30it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.73it/s][A05/28/2023 14:03:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:17 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:03:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 153.37it/s]
05/28/2023 14:03:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:17 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:17 - INFO - __main__ -    dev: eval_loss = 0.6935496992535062
05/28/2023 14:03:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:17 - INFO - __main__ -    dev: infer_time = 4.063666666666666
05/28/2023 14:03:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:17 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:17 - INFO - __main__ -     cls_loss = 0.6956345637639364
05/28/2023 14:03:17 - INFO - __main__ -     eval_loss = 0.6935496992535062
05/28/2023 14:03:17 - INFO - __main__ -     global_step = 9
05/28/2023 14:03:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:17 - INFO - __main__ -     infer_time = 4.063666666666666
05/28/2023 14:03:17 - INFO - __main__ -     loss = 0.6956345637639364
05/28/2023 14:03:17 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.70it/s][A
Iteration:  15%|#5        | 12/78 [00:02<00:12,  5.49it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.53it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.67it/s][A05/28/2023 14:03:19 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:19 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:03:19 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:19 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 152.58it/s]
05/28/2023 14:03:19 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:19 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:19 - INFO - __main__ -    dev: eval_loss = 0.6962882081667582
05/28/2023 14:03:19 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:19 - INFO - __main__ -    dev: infer_time = 4.111999999999999
05/28/2023 14:03:19 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:19 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:19 - INFO - __main__ -     cls_loss = 0.6965748981425637
05/28/2023 14:03:19 - INFO - __main__ -     eval_loss = 0.6962882081667582
05/28/2023 14:03:19 - INFO - __main__ -     global_step = 19
05/28/2023 14:03:19 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:19 - INFO - __main__ -     infer_time = 4.111999999999999
05/28/2023 14:03:19 - INFO - __main__ -     loss = 0.6965748981425637
05/28/2023 14:03:19 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:15,  3.84it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.36it/s][A
Iteration:  33%|###3      | 26/78 [00:04<00:07,  7.17it/s][A05/28/2023 14:03:21 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:21 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:03:21 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:21 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 153.00it/s]
05/28/2023 14:03:21 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:21 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:21 - INFO - __main__ -    dev: eval_loss = 0.6976972487237718
05/28/2023 14:03:21 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:21 - INFO - __main__ -    dev: infer_time = 4.082888888888888
05/28/2023 14:03:21 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:21 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:21 - INFO - __main__ -     cls_loss = 0.6980840091047615
05/28/2023 14:03:21 - INFO - __main__ -     eval_loss = 0.6976972487237718
05/28/2023 14:03:21 - INFO - __main__ -     global_step = 29
05/28/2023 14:03:21 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:21 - INFO - __main__ -     infer_time = 4.082888888888888
05/28/2023 14:03:21 - INFO - __main__ -     loss = 0.6980840091047615

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.63it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.56it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.65it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.64it/s][A05/28/2023 14:03:21 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:21 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:03:21 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:21 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 152.53it/s]
05/28/2023 14:03:21 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:21 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:21 - INFO - __main__ -    dev: eval_loss = 0.69651288456387
05/28/2023 14:03:21 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:21 - INFO - __main__ -    dev: infer_time = 4.061555555555555
05/28/2023 14:03:21 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:21 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:21 - INFO - __main__ -     cls_loss = 0.6987207302680383
05/28/2023 14:03:21 - INFO - __main__ -     eval_loss = 0.69651288456387
05/28/2023 14:03:21 - INFO - __main__ -     global_step = 39
05/28/2023 14:03:21 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:21 - INFO - __main__ -     infer_time = 4.061555555555555
05/28/2023 14:03:21 - INFO - __main__ -     loss = 0.6987207302680383

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.66it/s][A
Iteration:  56%|#####6    | 44/78 [00:05<00:02, 16.44it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.92it/s][A05/28/2023 14:03:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:22 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:03:22 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 153.48it/s]
05/28/2023 14:03:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:22 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:22 - INFO - __main__ -    dev: eval_loss = 0.6922165751457214
05/28/2023 14:03:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:22 - INFO - __main__ -    dev: infer_time = 4.041777777777778
05/28/2023 14:03:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:22 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:22 - INFO - __main__ -     cls_loss = 0.6979726711098029
05/28/2023 14:03:22 - INFO - __main__ -     eval_loss = 0.6922165751457214
05/28/2023 14:03:22 - INFO - __main__ -     global_step = 49
05/28/2023 14:03:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:22 - INFO - __main__ -     infer_time = 4.041777777777778
05/28/2023 14:03:22 - INFO - __main__ -     loss = 0.6979726711098029

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.92it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 18.20it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 19.29it/s][A05/28/2023 14:03:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:22 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:03:22 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 153.50it/s]
05/28/2023 14:03:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:22 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:22 - INFO - __main__ -    dev: eval_loss = 0.6920375161700778
05/28/2023 14:03:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:22 - INFO - __main__ -    dev: infer_time = 4.070555555555556
05/28/2023 14:03:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:22 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:22 - INFO - __main__ -     cls_loss = 0.697170601052753
05/28/2023 14:03:22 - INFO - __main__ -     eval_loss = 0.6920375161700778
05/28/2023 14:03:22 - INFO - __main__ -     global_step = 59
05/28/2023 14:03:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:22 - INFO - __main__ -     infer_time = 4.070555555555556
05/28/2023 14:03:22 - INFO - __main__ -     loss = 0.697170601052753

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 17.74it/s][A
Iteration:  78%|#######8  | 61/78 [00:05<00:00, 18.19it/s][A
Iteration:  82%|########2 | 64/78 [00:06<00:00, 19.31it/s][A
Iteration:  86%|########5 | 67/78 [00:06<00:00, 20.03it/s][A05/28/2023 14:03:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:23 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:03:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 153.46it/s]
05/28/2023 14:03:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:23 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:23 - INFO - __main__ -    dev: eval_loss = 0.6922461854086982
05/28/2023 14:03:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:23 - INFO - __main__ -    dev: infer_time = 4.049444444444445
05/28/2023 14:03:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:23 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:23 - INFO - __main__ -     cls_loss = 0.6966593775196351
05/28/2023 14:03:23 - INFO - __main__ -     eval_loss = 0.6922461854086982
05/28/2023 14:03:23 - INFO - __main__ -     global_step = 69
05/28/2023 14:03:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:23 - INFO - __main__ -     infer_time = 4.049444444444445
05/28/2023 14:03:23 - INFO - __main__ -     loss = 0.6966593775196351

Iteration:  90%|########9 | 70/78 [00:06<00:00, 18.03it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 19.24it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 20.17it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.65it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.70s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.70s/it]
05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   w_emb: 6592752

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   p_emb: 110592

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   t_emb: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_emb: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 96984

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   intermediate_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 97416

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 96984

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   intermediate_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 97416

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 96984

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   intermediate_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 97416

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 96984

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   intermediate_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 97416

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 96984

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   intermediate_numel: 97216

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   output_numel: 97416

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   layer_numel: 1912760
05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   dense_numel: 46872
05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   emb_numel: 6704208

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   encoder_numel: 1912760

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   pooler_numel: 46872

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   all parameters: 8663840

05/28/2023 14:03:23 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:23 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 216, 'sample_intermediate_sizes': [448, 448, 448, 448, 448], 'sample_qkv_sizes': [216, 216, 216, 216, 216]}
parameter size = 8663840
best_acc = 0.5270758122743683
time_per_batch_infer = 4.069 ms
infer_cnt = 63
**************E*************

05/28/2023 14:03:23 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
05/28/2023 14:03:23 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:03:23 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:03:23 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:23 - INFO - __main__ -   guid: train-0
05/28/2023 14:03:23 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:03:23 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:23 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:23 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:23 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:23 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:25 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:03:25 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:25 - INFO - __main__ -   guid: dev-0
05/28/2023 14:03:25 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:03:25 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:25 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:25 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:25 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:03:25 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:03:26 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:03:26 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:03:26 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:03:26 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:03:26 - INFO - __main__ -     Batch size = 32
05/28/2023 14:03:26 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.04it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.43it/s][A05/28/2023 14:03:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:26 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:03:26 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.05it/s]
05/28/2023 14:03:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:26 - INFO - __main__ -    dev: acc = 0.49097472924187724
05/28/2023 14:03:26 - INFO - __main__ -    dev: eval_loss = 0.6924199528164334
05/28/2023 14:03:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:26 - INFO - __main__ -    dev: infer_time = 2.753888888888889
05/28/2023 14:03:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:26 - INFO - __main__ -     acc = 0.49097472924187724
05/28/2023 14:03:26 - INFO - __main__ -     cls_loss = 0.695440411567688
05/28/2023 14:03:26 - INFO - __main__ -     eval_loss = 0.6924199528164334
05/28/2023 14:03:26 - INFO - __main__ -     global_step = 9
05/28/2023 14:03:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:26 - INFO - __main__ -     infer_time = 2.753888888888889
05/28/2023 14:03:26 - INFO - __main__ -     loss = 0.695440411567688
05/28/2023 14:03:26 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.92it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.80it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  7.90it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.15it/s][A05/28/2023 14:03:28 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:28 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:03:28 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:28 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.73it/s]
05/28/2023 14:03:28 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:28 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:28 - INFO - __main__ -    dev: eval_loss = 0.6921616858906217
05/28/2023 14:03:28 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:28 - INFO - __main__ -    dev: infer_time = 2.8510000000000004
05/28/2023 14:03:28 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:28 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:28 - INFO - __main__ -     cls_loss = 0.6965441484200326
05/28/2023 14:03:28 - INFO - __main__ -     eval_loss = 0.6921616858906217
05/28/2023 14:03:28 - INFO - __main__ -     global_step = 19
05/28/2023 14:03:28 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:28 - INFO - __main__ -     infer_time = 2.8510000000000004
05/28/2023 14:03:28 - INFO - __main__ -     loss = 0.6965441484200326
05/28/2023 14:03:28 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.36it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.87it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.65it/s][A05/28/2023 14:03:30 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:30 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:03:30 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:30 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.98it/s]
05/28/2023 14:03:30 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:30 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:30 - INFO - __main__ -    dev: eval_loss = 0.6976762016614279
05/28/2023 14:03:30 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:30 - INFO - __main__ -    dev: infer_time = 2.755666666666667
05/28/2023 14:03:30 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:30 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:30 - INFO - __main__ -     cls_loss = 0.6974560166227406
05/28/2023 14:03:30 - INFO - __main__ -     eval_loss = 0.6976762016614279
05/28/2023 14:03:30 - INFO - __main__ -     global_step = 29
05/28/2023 14:03:30 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:30 - INFO - __main__ -     infer_time = 2.755666666666667
05/28/2023 14:03:30 - INFO - __main__ -     loss = 0.6974560166227406

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.27it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.46it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.67it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.75it/s][A05/28/2023 14:03:30 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:30 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:03:30 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:30 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.14it/s]
05/28/2023 14:03:30 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:30 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:30 - INFO - __main__ -    dev: eval_loss = 0.6923188103569878
05/28/2023 14:03:30 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:30 - INFO - __main__ -    dev: infer_time = 2.75
05/28/2023 14:03:30 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:30 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:30 - INFO - __main__ -     cls_loss = 0.6971223812836868
05/28/2023 14:03:30 - INFO - __main__ -     eval_loss = 0.6923188103569878
05/28/2023 14:03:30 - INFO - __main__ -     global_step = 39
05/28/2023 14:03:30 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:30 - INFO - __main__ -     infer_time = 2.75
05/28/2023 14:03:30 - INFO - __main__ -     loss = 0.6971223812836868

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.43it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.21it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.73it/s][A05/28/2023 14:03:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:31 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:03:31 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.99it/s]
05/28/2023 14:03:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:31 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:31 - INFO - __main__ -    dev: eval_loss = 0.700734105375078
05/28/2023 14:03:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:31 - INFO - __main__ -    dev: infer_time = 2.761444444444445
05/28/2023 14:03:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:31 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:31 - INFO - __main__ -     cls_loss = 0.6978008686279764
05/28/2023 14:03:31 - INFO - __main__ -     eval_loss = 0.700734105375078
05/28/2023 14:03:31 - INFO - __main__ -     global_step = 49
05/28/2023 14:03:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:31 - INFO - __main__ -     infer_time = 2.761444444444445
05/28/2023 14:03:31 - INFO - __main__ -     loss = 0.6978008686279764

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.04it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 17.62it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 18.90it/s][A05/28/2023 14:03:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:31 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:03:31 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.98it/s]
05/28/2023 14:03:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:31 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:31 - INFO - __main__ -    dev: eval_loss = 0.7082415686713325
05/28/2023 14:03:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:31 - INFO - __main__ -    dev: infer_time = 2.757666666666667
05/28/2023 14:03:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:31 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:31 - INFO - __main__ -     cls_loss = 0.696473637879905
05/28/2023 14:03:31 - INFO - __main__ -     eval_loss = 0.7082415686713325
05/28/2023 14:03:31 - INFO - __main__ -     global_step = 59
05/28/2023 14:03:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:31 - INFO - __main__ -     infer_time = 2.757666666666667
05/28/2023 14:03:31 - INFO - __main__ -     loss = 0.696473637879905

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 16.94it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:00, 18.32it/s][A
Iteration:  83%|########3 | 65/78 [00:05<00:00, 19.47it/s][A
Iteration:  87%|########7 | 68/78 [00:06<00:00, 20.31it/s][A05/28/2023 14:03:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:32 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:03:32 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.36it/s]
05/28/2023 14:03:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:32 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:32 - INFO - __main__ -    dev: eval_loss = 0.6978691021601359
05/28/2023 14:03:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:32 - INFO - __main__ -    dev: infer_time = 2.738222222222222
05/28/2023 14:03:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:32 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:32 - INFO - __main__ -     cls_loss = 0.6971840003262395
05/28/2023 14:03:32 - INFO - __main__ -     eval_loss = 0.6978691021601359
05/28/2023 14:03:32 - INFO - __main__ -     global_step = 69
05/28/2023 14:03:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:32 - INFO - __main__ -     infer_time = 2.738222222222222
05/28/2023 14:03:32 - INFO - __main__ -     loss = 0.6971840003262395

Iteration:  91%|#########1| 71/78 [00:06<00:00, 17.68it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 18.81it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 19.84it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.82it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.60s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.60s/it]
05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   w_emb: 14284296

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   p_emb: 239616

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   t_emb: 936

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   ln_emb: 936

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 405216

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 404820

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   intermediate_numel: 405216

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   output_numel: 405756

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 405216

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 404820

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   intermediate_numel: 405216

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   output_numel: 405756

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 405216

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 404820

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   intermediate_numel: 405216

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   output_numel: 405756

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   layer_numel: 5069628
05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   dense_numel: 219492
05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   emb_numel: 14525784

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   encoder_numel: 5069628

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   pooler_numel: 219492

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   all parameters: 19814904

05/28/2023 14:03:32 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:32 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
parameter size = 19814904
best_acc = 0.5270758122743683
time_per_batch_infer = 2.767 ms
infer_cnt = 63
**************E*************

05/28/2023 14:03:32 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 156, 'sample_intermediate_sizes': [896, 896, 896, 896], 'sample_qkv_sizes': [156, 156, 156, 156]}
05/28/2023 14:03:32 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:03:32 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:03:32 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:32 - INFO - __main__ -   guid: train-0
05/28/2023 14:03:32 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:03:32 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:32 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:32 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:32 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:32 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:34 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:03:34 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:34 - INFO - __main__ -   guid: dev-0
05/28/2023 14:03:34 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:03:34 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:34 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:34 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:34 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:34 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:34 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:03:34 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:03:35 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:03:35 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:03:35 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:03:35 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:03:35 - INFO - __main__ -     Batch size = 32
05/28/2023 14:03:35 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:02, 25.26it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 26.54it/s][A05/28/2023 14:03:35 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:35 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:03:35 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:35 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 178.01it/s]
05/28/2023 14:03:35 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:35 - INFO - __main__ -    dev: acc = 0.5054151624548736
05/28/2023 14:03:35 - INFO - __main__ -    dev: eval_loss = 0.6930421657032437
05/28/2023 14:03:35 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:35 - INFO - __main__ -    dev: infer_time = 3.379666666666667
05/28/2023 14:03:35 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:35 - INFO - __main__ -     acc = 0.5054151624548736
05/28/2023 14:03:35 - INFO - __main__ -     cls_loss = 0.6937467654546102
05/28/2023 14:03:35 - INFO - __main__ -     eval_loss = 0.6930421657032437
05/28/2023 14:03:35 - INFO - __main__ -     global_step = 9
05/28/2023 14:03:35 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:35 - INFO - __main__ -     infer_time = 3.379666666666667
05/28/2023 14:03:35 - INFO - __main__ -     loss = 0.6937467654546102
05/28/2023 14:03:35 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.89it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.82it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.11it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.60it/s][A05/28/2023 14:03:37 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:37 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:03:37 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:37 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 177.79it/s]
05/28/2023 14:03:37 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:37 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:37 - INFO - __main__ -    dev: eval_loss = 0.6916383769777086
05/28/2023 14:03:37 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:37 - INFO - __main__ -    dev: infer_time = 3.384888888888889
05/28/2023 14:03:37 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:37 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:37 - INFO - __main__ -     cls_loss = 0.6937312797496193
05/28/2023 14:03:37 - INFO - __main__ -     eval_loss = 0.6916383769777086
05/28/2023 14:03:37 - INFO - __main__ -     global_step = 19
05/28/2023 14:03:37 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:37 - INFO - __main__ -     infer_time = 3.384888888888889
05/28/2023 14:03:37 - INFO - __main__ -     loss = 0.6937312797496193
05/28/2023 14:03:37 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.47it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:08,  6.08it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  8.01it/s][A05/28/2023 14:03:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:39 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:03:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 177.32it/s]
05/28/2023 14:03:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:39 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:39 - INFO - __main__ -    dev: eval_loss = 0.6945418583022224
05/28/2023 14:03:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:39 - INFO - __main__ -    dev: infer_time = 3.4090000000000003
05/28/2023 14:03:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:39 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:39 - INFO - __main__ -     cls_loss = 0.6939885328555929
05/28/2023 14:03:39 - INFO - __main__ -     eval_loss = 0.6945418583022224
05/28/2023 14:03:39 - INFO - __main__ -     global_step = 29
05/28/2023 14:03:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:39 - INFO - __main__ -     infer_time = 3.4090000000000003
05/28/2023 14:03:39 - INFO - __main__ -     loss = 0.6939885328555929

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.56it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.93it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:02, 14.41it/s][A05/28/2023 14:03:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:39 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:03:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 177.93it/s]
05/28/2023 14:03:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:39 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:39 - INFO - __main__ -    dev: eval_loss = 0.6945130162768893
05/28/2023 14:03:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:39 - INFO - __main__ -    dev: infer_time = 3.3726666666666665
05/28/2023 14:03:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:39 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:39 - INFO - __main__ -     cls_loss = 0.6940421645457928
05/28/2023 14:03:39 - INFO - __main__ -     eval_loss = 0.6945130162768893
05/28/2023 14:03:39 - INFO - __main__ -     global_step = 39
05/28/2023 14:03:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:39 - INFO - __main__ -     infer_time = 3.3726666666666665
05/28/2023 14:03:39 - INFO - __main__ -     loss = 0.6940421645457928

Iteration:  50%|#####     | 39/78 [00:04<00:02, 15.30it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 17.48it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 19.54it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 21.40it/s][A05/28/2023 14:03:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:40 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:03:40 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 178.16it/s]
05/28/2023 14:03:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:40 - INFO - __main__ -    dev: acc = 0.5451263537906137
05/28/2023 14:03:40 - INFO - __main__ -    dev: eval_loss = 0.6928572787178887
05/28/2023 14:03:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:40 - INFO - __main__ -    dev: infer_time = 3.378111111111111
05/28/2023 14:03:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:40 - INFO - __main__ -     acc = 0.5451263537906137
05/28/2023 14:03:40 - INFO - __main__ -     cls_loss = 0.6936391032471949
05/28/2023 14:03:40 - INFO - __main__ -     eval_loss = 0.6928572787178887
05/28/2023 14:03:40 - INFO - __main__ -     global_step = 49
05/28/2023 14:03:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:40 - INFO - __main__ -     infer_time = 3.378111111111111
05/28/2023 14:03:40 - INFO - __main__ -     loss = 0.6936391032471949
05/28/2023 14:03:40 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  65%|######5   | 51/78 [00:06<00:05,  5.39it/s][A
Iteration:  69%|######9   | 54/78 [00:06<00:03,  7.10it/s][A
Iteration:  73%|#######3  | 57/78 [00:06<00:02,  9.11it/s][A05/28/2023 14:03:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:41 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:03:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 177.51it/s]
05/28/2023 14:03:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:41 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:41 - INFO - __main__ -    dev: eval_loss = 0.6926782131195068
05/28/2023 14:03:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:41 - INFO - __main__ -    dev: infer_time = 3.361555555555556
05/28/2023 14:03:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:41 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:41 - INFO - __main__ -     cls_loss = 0.6935838945841385
05/28/2023 14:03:41 - INFO - __main__ -     eval_loss = 0.6926782131195068
05/28/2023 14:03:41 - INFO - __main__ -     global_step = 59
05/28/2023 14:03:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:41 - INFO - __main__ -     infer_time = 3.361555555555556
05/28/2023 14:03:41 - INFO - __main__ -     loss = 0.6935838945841385

Iteration:  77%|#######6  | 60/78 [00:06<00:01, 10.47it/s][A
Iteration:  81%|########  | 63/78 [00:06<00:01, 12.80it/s][A
Iteration:  85%|########4 | 66/78 [00:06<00:00, 15.11it/s][A05/28/2023 14:03:42 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:42 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:03:42 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:42 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 177.75it/s]
05/28/2023 14:03:42 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:42 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:03:42 - INFO - __main__ -    dev: eval_loss = 0.6925577521324158
05/28/2023 14:03:42 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:42 - INFO - __main__ -    dev: infer_time = 3.3779999999999997
05/28/2023 14:03:42 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:42 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:03:42 - INFO - __main__ -     cls_loss = 0.6932648878166641
05/28/2023 14:03:42 - INFO - __main__ -     eval_loss = 0.6925577521324158
05/28/2023 14:03:42 - INFO - __main__ -     global_step = 69
05/28/2023 14:03:42 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:42 - INFO - __main__ -     infer_time = 3.3779999999999997
05/28/2023 14:03:42 - INFO - __main__ -     loss = 0.6932648878166641

Iteration:  88%|########8 | 69/78 [00:07<00:00, 15.81it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:00, 17.78it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00, 19.45it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.57it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.38s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.38s/it]
05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   w_emb: 4761432

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   p_emb: 79872

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   t_emb: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ln_emb: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 140672

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 139932

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 140672

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   output_numel: 140244

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 140672

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 139932

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 140672

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   output_numel: 140244

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 140672

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 139932

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 140672

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   output_numel: 140244

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   query_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   key_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   value_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   self_numel: 73476

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   output_numel: 24804

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 140672

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 139932

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ln_numel: 312

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   attention_numel: 98280

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 140672

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   output_numel: 140244

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   layer_numel: 1516784
05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   dense_numel: 24492
05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   emb_numel: 4841928

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   encoder_numel: 1516784

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   pooler_numel: 24492

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   all parameters: 6383204

05/28/2023 14:03:42 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:42 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 156, 'sample_intermediate_sizes': [896, 896, 896, 896], 'sample_qkv_sizes': [156, 156, 156, 156]}
parameter size = 6383204
best_acc = 0.5451263537906137
time_per_batch_infer = 3.381 ms
infer_cnt = 63
**************E*************

05/28/2023 14:03:42 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}
05/28/2023 14:03:42 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:03:42 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:03:42 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:42 - INFO - __main__ -   guid: train-0
05/28/2023 14:03:42 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:03:42 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:42 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:42 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:44 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:03:44 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:44 - INFO - __main__ -   guid: dev-0
05/28/2023 14:03:44 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:03:44 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:44 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:44 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:44 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:03:44 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:03:45 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:03:45 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:03:45 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:03:45 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:03:45 - INFO - __main__ -     Batch size = 32
05/28/2023 14:03:45 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.09it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.78it/s][A05/28/2023 14:03:45 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:45 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:03:45 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:45 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.82it/s]
05/28/2023 14:03:45 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:45 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:45 - INFO - __main__ -    dev: eval_loss = 0.6956890490319994
05/28/2023 14:03:45 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:45 - INFO - __main__ -    dev: infer_time = 3.5028888888888883
05/28/2023 14:03:45 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:45 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:45 - INFO - __main__ -     cls_loss = 0.7031824721230401
05/28/2023 14:03:45 - INFO - __main__ -     eval_loss = 0.6956890490319994
05/28/2023 14:03:45 - INFO - __main__ -     global_step = 9
05/28/2023 14:03:45 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:45 - INFO - __main__ -     infer_time = 3.5028888888888883
05/28/2023 14:03:45 - INFO - __main__ -     loss = 0.7031824721230401
05/28/2023 14:03:45 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.91it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.75it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.82it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.97it/s][A05/28/2023 14:03:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:47 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:03:47 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.31it/s]
05/28/2023 14:03:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:47 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:47 - INFO - __main__ -    dev: eval_loss = 0.6924029654926724
05/28/2023 14:03:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:47 - INFO - __main__ -    dev: infer_time = 3.559444444444444
05/28/2023 14:03:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:47 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:47 - INFO - __main__ -     cls_loss = 0.7019779431192499
05/28/2023 14:03:47 - INFO - __main__ -     eval_loss = 0.6924029654926724
05/28/2023 14:03:47 - INFO - __main__ -     global_step = 19
05/28/2023 14:03:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:47 - INFO - __main__ -     infer_time = 3.559444444444444
05/28/2023 14:03:47 - INFO - __main__ -     loss = 0.7019779431192499
05/28/2023 14:03:47 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:14,  3.91it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.44it/s][A
Iteration:  33%|###3      | 26/78 [00:03<00:07,  7.22it/s][A05/28/2023 14:03:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:49 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:03:49 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.77it/s]
05/28/2023 14:03:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:49 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:49 - INFO - __main__ -    dev: eval_loss = 0.6915307707256741
05/28/2023 14:03:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:49 - INFO - __main__ -    dev: infer_time = 3.553777777777778
05/28/2023 14:03:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:49 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:49 - INFO - __main__ -     cls_loss = 0.7001164946062811
05/28/2023 14:03:49 - INFO - __main__ -     eval_loss = 0.6915307707256741
05/28/2023 14:03:49 - INFO - __main__ -     global_step = 29
05/28/2023 14:03:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:49 - INFO - __main__ -     infer_time = 3.553777777777778
05/28/2023 14:03:49 - INFO - __main__ -     loss = 0.7001164946062811

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.51it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.52it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.50it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.40it/s][A05/28/2023 14:03:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:49 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:03:49 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.97it/s]
05/28/2023 14:03:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:49 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:49 - INFO - __main__ -    dev: eval_loss = 0.691143618689643
05/28/2023 14:03:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:49 - INFO - __main__ -    dev: infer_time = 3.530222222222222
05/28/2023 14:03:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:49 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:49 - INFO - __main__ -     cls_loss = 0.6996718324147738
05/28/2023 14:03:49 - INFO - __main__ -     eval_loss = 0.691143618689643
05/28/2023 14:03:49 - INFO - __main__ -     global_step = 39
05/28/2023 14:03:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:49 - INFO - __main__ -     infer_time = 3.530222222222222
05/28/2023 14:03:49 - INFO - __main__ -     loss = 0.6996718324147738

Iteration:  51%|#####1    | 40/78 [00:04<00:02, 13.71it/s][A
Iteration:  55%|#####5    | 43/78 [00:04<00:02, 15.60it/s][A
Iteration:  59%|#####8    | 46/78 [00:05<00:01, 17.14it/s][A05/28/2023 14:03:50 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:50 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:03:50 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:50 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.05it/s]
05/28/2023 14:03:50 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:50 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:50 - INFO - __main__ -    dev: eval_loss = 0.6983974840905931
05/28/2023 14:03:50 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:50 - INFO - __main__ -    dev: infer_time = 3.544777777777778
05/28/2023 14:03:50 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:50 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:50 - INFO - __main__ -     cls_loss = 0.6987539177038231
05/28/2023 14:03:50 - INFO - __main__ -     eval_loss = 0.6983974840905931
05/28/2023 14:03:50 - INFO - __main__ -     global_step = 49
05/28/2023 14:03:50 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:50 - INFO - __main__ -     infer_time = 3.544777777777778
05/28/2023 14:03:50 - INFO - __main__ -     loss = 0.6987539177038231

Iteration:  63%|######2   | 49/78 [00:05<00:01, 16.00it/s][A
Iteration:  67%|######6   | 52/78 [00:05<00:01, 17.37it/s][A
Iteration:  71%|#######   | 55/78 [00:05<00:01, 18.54it/s][A
Iteration:  74%|#######4  | 58/78 [00:05<00:01, 19.35it/s][A05/28/2023 14:03:50 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:50 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:03:50 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:50 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.96it/s]
05/28/2023 14:03:50 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:50 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:50 - INFO - __main__ -    dev: eval_loss = 0.6951493687099881
05/28/2023 14:03:50 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:50 - INFO - __main__ -    dev: infer_time = 3.544888888888889
05/28/2023 14:03:50 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:50 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:50 - INFO - __main__ -     cls_loss = 0.6981020984003099
05/28/2023 14:03:50 - INFO - __main__ -     eval_loss = 0.6951493687099881
05/28/2023 14:03:50 - INFO - __main__ -     global_step = 59
05/28/2023 14:03:50 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:50 - INFO - __main__ -     infer_time = 3.544888888888889
05/28/2023 14:03:50 - INFO - __main__ -     loss = 0.6981020984003099

Iteration:  78%|#######8  | 61/78 [00:05<00:00, 17.09it/s][A
Iteration:  82%|########2 | 64/78 [00:06<00:00, 18.28it/s][A
Iteration:  86%|########5 | 67/78 [00:06<00:00, 19.20it/s][A05/28/2023 14:03:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:51 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:03:51 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 125.13it/s]
05/28/2023 14:03:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:51 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 14:03:51 - INFO - __main__ -    dev: eval_loss = 0.6934301654497782
05/28/2023 14:03:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:51 - INFO - __main__ -    dev: infer_time = 3.5274444444444444
05/28/2023 14:03:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:51 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 14:03:51 - INFO - __main__ -     cls_loss = 0.697385897670967
05/28/2023 14:03:51 - INFO - __main__ -     eval_loss = 0.6934301654497782
05/28/2023 14:03:51 - INFO - __main__ -     global_step = 69
05/28/2023 14:03:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:51 - INFO - __main__ -     infer_time = 3.5274444444444444
05/28/2023 14:03:51 - INFO - __main__ -     loss = 0.697385897670967

Iteration:  90%|########9 | 70/78 [00:06<00:00, 17.17it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 18.29it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 19.29it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.62it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.71s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.71s/it]
05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   w_emb: 10621656

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   p_emb: 178176

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   t_emb: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ln_emb: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   layer_numel: 3645296
05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   dense_numel: 121452
05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   emb_numel: 10801224

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   encoder_numel: 3645296

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   pooler_numel: 121452

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   all parameters: 14567972

05/28/2023 14:03:51 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:03:51 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}
parameter size = 14567972
best_acc = 0.5270758122743683
time_per_batch_infer = 3.538 ms
infer_cnt = 63
**************E*************

05/28/2023 14:03:51 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 228, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_qkv_sizes': [228, 228, 228, 228]}
05/28/2023 14:03:51 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:03:51 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:03:51 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:51 - INFO - __main__ -   guid: train-0
05/28/2023 14:03:51 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:03:51 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:51 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:51 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:51 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:53 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:03:53 - INFO - __main__ -   *** Example ***
05/28/2023 14:03:53 - INFO - __main__ -   guid: dev-0
05/28/2023 14:03:53 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:03:53 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:53 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:53 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:03:53 - INFO - __main__ -   label: not_entailment
05/28/2023 14:03:53 - INFO - __main__ -   label_id: 1
05/28/2023 14:03:53 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:03:53 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:03:54 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:03:54 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:03:54 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:03:54 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:03:54 - INFO - __main__ -     Batch size = 32
05/28/2023 14:03:54 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.71it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.26it/s][A05/28/2023 14:03:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:54 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:03:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 170.49it/s]
05/28/2023 14:03:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:54 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:03:54 - INFO - __main__ -    dev: eval_loss = 0.6946486632029215
05/28/2023 14:03:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:54 - INFO - __main__ -    dev: infer_time = 3.365666666666667
05/28/2023 14:03:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:54 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:03:54 - INFO - __main__ -     cls_loss = 0.689636210600535
05/28/2023 14:03:54 - INFO - __main__ -     eval_loss = 0.6946486632029215
05/28/2023 14:03:54 - INFO - __main__ -     global_step = 9
05/28/2023 14:03:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:54 - INFO - __main__ -     infer_time = 3.365666666666667
05/28/2023 14:03:54 - INFO - __main__ -     loss = 0.689636210600535
05/28/2023 14:03:54 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.95it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.89it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.17it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.63it/s][A05/28/2023 14:03:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:56 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:03:56 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.65it/s]
05/28/2023 14:03:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:56 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:03:56 - INFO - __main__ -    dev: eval_loss = 0.6950036684672037
05/28/2023 14:03:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:56 - INFO - __main__ -    dev: infer_time = 3.4022222222222225
05/28/2023 14:03:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:56 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:03:56 - INFO - __main__ -     cls_loss = 0.6984247188819083
05/28/2023 14:03:56 - INFO - __main__ -     eval_loss = 0.6950036684672037
05/28/2023 14:03:56 - INFO - __main__ -     global_step = 19
05/28/2023 14:03:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:56 - INFO - __main__ -     infer_time = 3.4022222222222225
05/28/2023 14:03:56 - INFO - __main__ -     loss = 0.6984247188819083

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.88it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.98it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.40it/s][A05/28/2023 14:03:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:57 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:03:57 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 170.29it/s]
05/28/2023 14:03:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:57 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:57 - INFO - __main__ -    dev: eval_loss = 0.7005801730685763
05/28/2023 14:03:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:57 - INFO - __main__ -    dev: infer_time = 3.3785555555555558
05/28/2023 14:03:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:57 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:57 - INFO - __main__ -     cls_loss = 0.6973875082772354
05/28/2023 14:03:57 - INFO - __main__ -     eval_loss = 0.7005801730685763
05/28/2023 14:03:57 - INFO - __main__ -     global_step = 29
05/28/2023 14:03:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:57 - INFO - __main__ -     infer_time = 3.3785555555555558
05/28/2023 14:03:57 - INFO - __main__ -     loss = 0.6973875082772354

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.56it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 18.65it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 20.40it/s][A05/28/2023 14:03:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:57 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:03:57 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.60it/s]
05/28/2023 14:03:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:57 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:57 - INFO - __main__ -    dev: eval_loss = 0.6981752514839172
05/28/2023 14:03:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:57 - INFO - __main__ -    dev: infer_time = 3.401888888888889
05/28/2023 14:03:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:57 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:57 - INFO - __main__ -     cls_loss = 0.6953792495605273
05/28/2023 14:03:57 - INFO - __main__ -     eval_loss = 0.6981752514839172
05/28/2023 14:03:57 - INFO - __main__ -     global_step = 39
05/28/2023 14:03:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:57 - INFO - __main__ -     infer_time = 3.401888888888889
05/28/2023 14:03:57 - INFO - __main__ -     loss = 0.6953792495605273

Iteration:  50%|#####     | 39/78 [00:03<00:02, 19.35it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 20.72it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 22.08it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 23.14it/s][A05/28/2023 14:03:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:57 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:03:57 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 170.05it/s]
05/28/2023 14:03:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:58 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:58 - INFO - __main__ -    dev: eval_loss = 0.698155575328403
05/28/2023 14:03:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:58 - INFO - __main__ -    dev: infer_time = 3.4227777777777773
05/28/2023 14:03:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:58 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:58 - INFO - __main__ -     cls_loss = 0.6949569814059199
05/28/2023 14:03:58 - INFO - __main__ -     eval_loss = 0.698155575328403
05/28/2023 14:03:58 - INFO - __main__ -     global_step = 49
05/28/2023 14:03:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:58 - INFO - __main__ -     infer_time = 3.4227777777777773
05/28/2023 14:03:58 - INFO - __main__ -     loss = 0.6949569814059199

Iteration:  65%|######5   | 51/78 [00:03<00:01, 20.62it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 22.05it/s][A
Iteration:  73%|#######3  | 57/78 [00:03<00:00, 22.93it/s][A05/28/2023 14:03:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:58 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:03:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.81it/s]
05/28/2023 14:03:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:58 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:03:58 - INFO - __main__ -    dev: eval_loss = 0.6939302881558737
05/28/2023 14:03:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:58 - INFO - __main__ -    dev: infer_time = 3.397777777777778
05/28/2023 14:03:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:58 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:03:58 - INFO - __main__ -     cls_loss = 0.6954017625016681
05/28/2023 14:03:58 - INFO - __main__ -     eval_loss = 0.6939302881558737
05/28/2023 14:03:58 - INFO - __main__ -     global_step = 59
05/28/2023 14:03:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:58 - INFO - __main__ -     infer_time = 3.397777777777778
05/28/2023 14:03:58 - INFO - __main__ -     loss = 0.6954017625016681

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 20.64it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 22.09it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 22.85it/s][A05/28/2023 14:03:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:03:58 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:03:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:03:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 170.13it/s]
05/28/2023 14:03:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:03:58 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:03:58 - INFO - __main__ -    dev: eval_loss = 0.6918570200602213
05/28/2023 14:03:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:03:58 - INFO - __main__ -    dev: infer_time = 3.3966666666666665
05/28/2023 14:03:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:03:58 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:03:58 - INFO - __main__ -     cls_loss = 0.6951831864274066
05/28/2023 14:03:58 - INFO - __main__ -     eval_loss = 0.6918570200602213
05/28/2023 14:03:58 - INFO - __main__ -     global_step = 69
05/28/2023 14:03:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:03:58 - INFO - __main__ -     infer_time = 3.3966666666666665
05/28/2023 14:03:58 - INFO - __main__ -     loss = 0.6951831864274066
05/28/2023 14:03:58 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  88%|########8 | 69/78 [00:05<00:01,  5.50it/s][A
Iteration:  92%|#########2| 72/78 [00:05<00:00,  7.15it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00,  9.14it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.68it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.15s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.15s/it]
05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   w_emb: 6959016

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   p_emb: 116736

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   t_emb: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ln_emb: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   query_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   key_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   value_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ln_numel: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   self_numel: 156636

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   output_numel: 52668

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 109920

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 109668

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ln_numel: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   attention_numel: 209304

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 109920

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   output_numel: 110124

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   query_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   key_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   value_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ln_numel: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   self_numel: 156636

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   output_numel: 52668

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 109920

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 109668

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ln_numel: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   attention_numel: 209304

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 109920

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   output_numel: 110124

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   query_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   key_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   value_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ln_numel: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   self_numel: 156636

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   output_numel: 52668

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 109920

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 109668

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ln_numel: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   attention_numel: 209304

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 109920

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   output_numel: 110124

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   query_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   key_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   value_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ln_numel: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   self_numel: 156636

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   output_numel: 52668

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 109920

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 109668

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ln_numel: 456

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   attention_numel: 209304

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 109920

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   output_numel: 110124

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   layer_numel: 1717392
05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   dense_numel: 52212
05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   emb_numel: 7076664

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   encoder_numel: 1717392

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   pooler_numel: 52212

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   all parameters: 8846268

05/28/2023 14:04:00 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:00 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 228, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_qkv_sizes': [228, 228, 228, 228]}
parameter size = 8846268
best_acc = 0.5306859205776173
time_per_batch_infer = 3.395 ms
infer_cnt = 63
**************E*************

05/28/2023 14:04:00 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
05/28/2023 14:04:00 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:04:00 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:04:00 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:00 - INFO - __main__ -   guid: train-0
05/28/2023 14:04:00 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:04:00 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:00 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:00 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:02 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:04:02 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:02 - INFO - __main__ -   guid: dev-0
05/28/2023 14:04:02 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:04:02 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:02 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:02 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:02 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:02 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:04:02 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:04:03 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:04:03 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:04:03 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:04:03 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:04:03 - INFO - __main__ -     Batch size = 32
05/28/2023 14:04:03 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.98it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.55it/s][A05/28/2023 14:04:03 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:03 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:04:03 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:03 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 159.98it/s]
05/28/2023 14:04:03 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:03 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:03 - INFO - __main__ -    dev: eval_loss = 0.6941867536968656
05/28/2023 14:04:03 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:03 - INFO - __main__ -    dev: infer_time = 3.3761111111111117
05/28/2023 14:04:03 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:03 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:03 - INFO - __main__ -     cls_loss = 0.6915773948033651
05/28/2023 14:04:03 - INFO - __main__ -     eval_loss = 0.6941867536968656
05/28/2023 14:04:03 - INFO - __main__ -     global_step = 9
05/28/2023 14:04:03 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:03 - INFO - __main__ -     infer_time = 3.3761111111111117
05/28/2023 14:04:03 - INFO - __main__ -     loss = 0.6915773948033651
05/28/2023 14:04:03 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.03it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.97it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.22it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.64it/s][A05/28/2023 14:04:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:05 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:04:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 158.68it/s]
05/28/2023 14:04:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:05 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:04:05 - INFO - __main__ -    dev: eval_loss = 0.69301970799764
05/28/2023 14:04:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:05 - INFO - __main__ -    dev: infer_time = 3.550666666666666
05/28/2023 14:04:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:05 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:04:05 - INFO - __main__ -     cls_loss = 0.6949960558038009
05/28/2023 14:04:05 - INFO - __main__ -     eval_loss = 0.69301970799764
05/28/2023 14:04:05 - INFO - __main__ -     global_step = 19
05/28/2023 14:04:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:05 - INFO - __main__ -     infer_time = 3.550666666666666
05/28/2023 14:04:05 - INFO - __main__ -     loss = 0.6949960558038009

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.90it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.27it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.45it/s][A05/28/2023 14:04:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:05 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:04:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 159.99it/s]
05/28/2023 14:04:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:05 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:04:05 - INFO - __main__ -    dev: eval_loss = 0.695773270395067
05/28/2023 14:04:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:05 - INFO - __main__ -    dev: infer_time = 3.416444444444444
05/28/2023 14:04:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:05 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:04:05 - INFO - __main__ -     cls_loss = 0.6944430182720053
05/28/2023 14:04:05 - INFO - __main__ -     eval_loss = 0.695773270395067
05/28/2023 14:04:05 - INFO - __main__ -     global_step = 29
05/28/2023 14:04:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:05 - INFO - __main__ -     infer_time = 3.416444444444444
05/28/2023 14:04:05 - INFO - __main__ -     loss = 0.6944430182720053

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.35it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 18.08it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 19.71it/s][A05/28/2023 14:04:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:06 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:04:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 159.41it/s]
05/28/2023 14:04:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:06 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:06 - INFO - __main__ -    dev: eval_loss = 0.6918668945630392
05/28/2023 14:04:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:06 - INFO - __main__ -    dev: infer_time = 3.4526666666666666
05/28/2023 14:04:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:06 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:06 - INFO - __main__ -     cls_loss = 0.6942854584791721
05/28/2023 14:04:06 - INFO - __main__ -     eval_loss = 0.6918668945630392
05/28/2023 14:04:06 - INFO - __main__ -     global_step = 39
05/28/2023 14:04:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:06 - INFO - __main__ -     infer_time = 3.4526666666666666
05/28/2023 14:04:06 - INFO - __main__ -     loss = 0.6942854584791721

Iteration:  50%|#####     | 39/78 [00:03<00:02, 18.51it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 19.79it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 21.04it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 22.04it/s][A05/28/2023 14:04:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:06 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:04:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 160.26it/s]
05/28/2023 14:04:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:06 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:06 - INFO - __main__ -    dev: eval_loss = 0.691891610622406
05/28/2023 14:04:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:06 - INFO - __main__ -    dev: infer_time = 3.390888888888889
05/28/2023 14:04:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:06 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:06 - INFO - __main__ -     cls_loss = 0.693746495003603
05/28/2023 14:04:06 - INFO - __main__ -     eval_loss = 0.691891610622406
05/28/2023 14:04:06 - INFO - __main__ -     global_step = 49
05/28/2023 14:04:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:06 - INFO - __main__ -     infer_time = 3.390888888888889
05/28/2023 14:04:06 - INFO - __main__ -     loss = 0.693746495003603

Iteration:  65%|######5   | 51/78 [00:03<00:01, 19.64it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 20.95it/s][A
Iteration:  73%|#######3  | 57/78 [00:03<00:00, 22.00it/s][A05/28/2023 14:04:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:07 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:04:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 160.06it/s]
05/28/2023 14:04:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:07 - INFO - __main__ -    dev: eval_loss = 0.691845264699724
05/28/2023 14:04:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:07 - INFO - __main__ -    dev: infer_time = 3.3921111111111113
05/28/2023 14:04:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:07 - INFO - __main__ -     cls_loss = 0.693873255939807
05/28/2023 14:04:07 - INFO - __main__ -     eval_loss = 0.691845264699724
05/28/2023 14:04:07 - INFO - __main__ -     global_step = 59
05/28/2023 14:04:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:07 - INFO - __main__ -     infer_time = 3.3921111111111113
05/28/2023 14:04:07 - INFO - __main__ -     loss = 0.693873255939807

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 19.59it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 20.90it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 21.44it/s][A05/28/2023 14:04:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:07 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:04:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 160.20it/s]
05/28/2023 14:04:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:07 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:04:07 - INFO - __main__ -    dev: eval_loss = 0.6922658615642123
05/28/2023 14:04:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:07 - INFO - __main__ -    dev: infer_time = 3.3965555555555556
05/28/2023 14:04:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:07 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:04:07 - INFO - __main__ -     cls_loss = 0.6947111556495446
05/28/2023 14:04:07 - INFO - __main__ -     eval_loss = 0.6922658615642123
05/28/2023 14:04:07 - INFO - __main__ -     global_step = 69
05/28/2023 14:04:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:07 - INFO - __main__ -     infer_time = 3.3965555555555556
05/28/2023 14:04:07 - INFO - __main__ -     loss = 0.6947111556495446

Iteration:  88%|########8 | 69/78 [00:04<00:00, 19.56it/s][A
Iteration:  92%|#########2| 72/78 [00:04<00:00, 20.56it/s][A
Iteration:  96%|#########6| 75/78 [00:04<00:00, 21.69it/s][AIteration: 100%|##########| 78/78 [00:04<00:00, 15.86it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.92s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.92s/it]
05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   w_emb: 8424072

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   p_emb: 141312

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   t_emb: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ln_emb: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 124096

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 123924

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 124096

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   output_numel: 124476

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 124096

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 123924

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 124096

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   output_numel: 124476

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 124096

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 123924

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 124096

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   output_numel: 124476

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 124096

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 123924

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   intermediate_numel: 124096

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   output_numel: 124476

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   layer_numel: 2219728
05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   dense_numel: 76452
05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   emb_numel: 8566488

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   encoder_numel: 2219728

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   pooler_numel: 76452

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   all parameters: 10862668

05/28/2023 14:04:07 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:07 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
parameter size = 10862668
best_acc = 0.5270758122743683
time_per_batch_infer = 3.425 ms
infer_cnt = 63
**************E*************

05/28/2023 14:04:07 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 396, 'sample_intermediate_sizes': [704, 704, 704], 'sample_qkv_sizes': [396, 396, 396]}
05/28/2023 14:04:07 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:04:08 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:04:08 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:08 - INFO - __main__ -   guid: train-0
05/28/2023 14:04:08 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:04:08 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:08 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:08 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:08 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:08 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:09 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:04:09 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:09 - INFO - __main__ -   guid: dev-0
05/28/2023 14:04:09 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:04:09 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:09 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:09 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:09 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:09 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:04:09 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:04:10 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:04:10 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:04:10 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:04:10 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:04:10 - INFO - __main__ -     Batch size = 32
05/28/2023 14:04:10 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.75it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.65it/s][A05/28/2023 14:04:10 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:10 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:04:10 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:10 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.52it/s]
05/28/2023 14:04:10 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:10 - INFO - __main__ -    dev: acc = 0.5487364620938628
05/28/2023 14:04:10 - INFO - __main__ -    dev: eval_loss = 0.6907140281465318
05/28/2023 14:04:10 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:10 - INFO - __main__ -    dev: infer_time = 2.8005555555555555
05/28/2023 14:04:10 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:10 - INFO - __main__ -     acc = 0.5487364620938628
05/28/2023 14:04:10 - INFO - __main__ -     cls_loss = 0.6986837651994493
05/28/2023 14:04:10 - INFO - __main__ -     eval_loss = 0.6907140281465318
05/28/2023 14:04:10 - INFO - __main__ -     global_step = 9
05/28/2023 14:04:10 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:10 - INFO - __main__ -     infer_time = 2.8005555555555555
05/28/2023 14:04:10 - INFO - __main__ -     loss = 0.6986837651994493
05/28/2023 14:04:10 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.01it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.98it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.25it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.68it/s][A05/28/2023 14:04:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:12 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:04:12 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.21it/s]
05/28/2023 14:04:12 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:12 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:12 - INFO - __main__ -    dev: eval_loss = 0.6921621759732565
05/28/2023 14:04:12 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:12 - INFO - __main__ -    dev: infer_time = 2.7874444444444446
05/28/2023 14:04:12 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:12 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:12 - INFO - __main__ -     cls_loss = 0.6993340096975628
05/28/2023 14:04:12 - INFO - __main__ -     eval_loss = 0.6921621759732565
05/28/2023 14:04:12 - INFO - __main__ -     global_step = 19
05/28/2023 14:04:12 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:12 - INFO - __main__ -     infer_time = 2.7874444444444446
05/28/2023 14:04:12 - INFO - __main__ -     loss = 0.6993340096975628

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.68it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.97it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.93it/s][A05/28/2023 14:04:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:13 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:04:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.58it/s]
05/28/2023 14:04:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:13 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:04:13 - INFO - __main__ -    dev: eval_loss = 0.6911162734031677
05/28/2023 14:04:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:13 - INFO - __main__ -    dev: infer_time = 2.8016666666666663
05/28/2023 14:04:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:13 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:04:13 - INFO - __main__ -     cls_loss = 0.6997755210975121
05/28/2023 14:04:13 - INFO - __main__ -     eval_loss = 0.6911162734031677
05/28/2023 14:04:13 - INFO - __main__ -     global_step = 29
05/28/2023 14:04:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:13 - INFO - __main__ -     infer_time = 2.8016666666666663
05/28/2023 14:04:13 - INFO - __main__ -     loss = 0.6997755210975121

Iteration:  38%|###8      | 30/78 [00:02<00:03, 15.70it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 17.78it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 19.46it/s][A05/28/2023 14:04:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:13 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:04:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.38it/s]
05/28/2023 14:04:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:13 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:13 - INFO - __main__ -    dev: eval_loss = 0.6995500458611382
05/28/2023 14:04:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:13 - INFO - __main__ -    dev: infer_time = 2.821444444444444
05/28/2023 14:04:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:13 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:13 - INFO - __main__ -     cls_loss = 0.6979336708019941
05/28/2023 14:04:13 - INFO - __main__ -     eval_loss = 0.6995500458611382
05/28/2023 14:04:13 - INFO - __main__ -     global_step = 39
05/28/2023 14:04:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:13 - INFO - __main__ -     infer_time = 2.821444444444444
05/28/2023 14:04:13 - INFO - __main__ -     loss = 0.6979336708019941

Iteration:  50%|#####     | 39/78 [00:03<00:02, 17.94it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 19.38it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 20.75it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 21.86it/s][A05/28/2023 14:04:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:14 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:04:14 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.26it/s]
05/28/2023 14:04:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:14 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:14 - INFO - __main__ -    dev: eval_loss = 0.6990653607580397
05/28/2023 14:04:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:14 - INFO - __main__ -    dev: infer_time = 2.8356666666666666
05/28/2023 14:04:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:14 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:14 - INFO - __main__ -     cls_loss = 0.6974129907938899
05/28/2023 14:04:14 - INFO - __main__ -     eval_loss = 0.6990653607580397
05/28/2023 14:04:14 - INFO - __main__ -     global_step = 49
05/28/2023 14:04:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:14 - INFO - __main__ -     infer_time = 2.8356666666666666
05/28/2023 14:04:14 - INFO - __main__ -     loss = 0.6974129907938899

Iteration:  65%|######5   | 51/78 [00:03<00:01, 19.16it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 20.52it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:00, 21.63it/s][A05/28/2023 14:04:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:14 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:04:14 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.66it/s]
05/28/2023 14:04:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:14 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:14 - INFO - __main__ -    dev: eval_loss = 0.6948625246683756
05/28/2023 14:04:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:14 - INFO - __main__ -    dev: infer_time = 2.800111111111111
05/28/2023 14:04:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:14 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:14 - INFO - __main__ -     cls_loss = 0.6962145405300593
05/28/2023 14:04:14 - INFO - __main__ -     eval_loss = 0.6948625246683756
05/28/2023 14:04:14 - INFO - __main__ -     global_step = 59
05/28/2023 14:04:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:14 - INFO - __main__ -     infer_time = 2.800111111111111
05/28/2023 14:04:14 - INFO - __main__ -     loss = 0.6962145405300593

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 19.19it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 20.42it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 21.54it/s][A05/28/2023 14:04:15 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:15 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:04:15 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:15 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.77it/s]
05/28/2023 14:04:15 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:15 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:04:15 - INFO - __main__ -    dev: eval_loss = 0.6927909784846835
05/28/2023 14:04:15 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:15 - INFO - __main__ -    dev: infer_time = 2.8032222222222227
05/28/2023 14:04:15 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:15 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:04:15 - INFO - __main__ -     cls_loss = 0.6956452571827433
05/28/2023 14:04:15 - INFO - __main__ -     eval_loss = 0.6927909784846835
05/28/2023 14:04:15 - INFO - __main__ -     global_step = 69
05/28/2023 14:04:15 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:15 - INFO - __main__ -     infer_time = 2.8032222222222227
05/28/2023 14:04:15 - INFO - __main__ -     loss = 0.6956452571827433

Iteration:  88%|########8 | 69/78 [00:04<00:00, 19.22it/s][A
Iteration:  92%|#########2| 72/78 [00:04<00:00, 20.65it/s][A
Iteration:  96%|#########6| 75/78 [00:04<00:00, 21.87it/s][AIteration: 100%|##########| 78/78 [00:04<00:00, 15.71it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.97s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.97s/it]
05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   w_emb: 12086712

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   p_emb: 202752

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   t_emb: 792

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   ln_emb: 792

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   query_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   key_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   value_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   ln_numel: 792

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   self_numel: 471636

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   output_numel: 158004

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 279488

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 279180

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   ln_numel: 792

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   attention_numel: 629640

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 279488

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   output_numel: 279972

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   query_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   key_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   value_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   ln_numel: 792

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   self_numel: 471636

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   output_numel: 158004

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 279488

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 279180

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   ln_numel: 792

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   attention_numel: 629640

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 279488

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   output_numel: 279972

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   query_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   key_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   value_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   ln_numel: 792

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   self_numel: 471636

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   output_numel: 158004

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 279488

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 279180

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   ln_numel: 792

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   attention_numel: 629640

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 279488

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   output_numel: 279972

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   layer_numel: 3567300
05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   dense_numel: 157212
05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   emb_numel: 12291048

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   encoder_numel: 3567300

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   pooler_numel: 157212

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   all parameters: 16015560

05/28/2023 14:04:15 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:15 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 396, 'sample_intermediate_sizes': [704, 704, 704], 'sample_qkv_sizes': [396, 396, 396]}
parameter size = 16015560
best_acc = 0.5487364620938628
time_per_batch_infer = 2.807 ms
infer_cnt = 63
**************E*************

05/28/2023 14:04:15 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
05/28/2023 14:04:15 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:04:15 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:04:15 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:15 - INFO - __main__ -   guid: train-0
05/28/2023 14:04:15 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:04:15 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:15 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:15 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:15 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:15 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:17 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:04:17 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:17 - INFO - __main__ -   guid: dev-0
05/28/2023 14:04:17 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:04:17 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:17 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:17 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:17 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:17 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:04:17 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:04:17 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:04:17 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:04:17 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:04:17 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:04:17 - INFO - __main__ -     Batch size = 32
05/28/2023 14:04:17 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.39it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.67it/s][A05/28/2023 14:04:18 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:18 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:04:18 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:18 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.42it/s]
05/28/2023 14:04:18 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:18 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:18 - INFO - __main__ -    dev: eval_loss = 0.6978583137194315
05/28/2023 14:04:18 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:18 - INFO - __main__ -    dev: infer_time = 2.8048888888888888
05/28/2023 14:04:18 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:18 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:18 - INFO - __main__ -     cls_loss = 0.6938945518599616
05/28/2023 14:04:18 - INFO - __main__ -     eval_loss = 0.6978583137194315
05/28/2023 14:04:18 - INFO - __main__ -     global_step = 9
05/28/2023 14:04:18 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:18 - INFO - __main__ -     infer_time = 2.8048888888888888
05/28/2023 14:04:18 - INFO - __main__ -     loss = 0.6938945518599616
05/28/2023 14:04:18 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.91it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.81it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  7.93it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.19it/s][A05/28/2023 14:04:20 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:20 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:04:20 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:20 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.16it/s]
05/28/2023 14:04:20 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:20 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:20 - INFO - __main__ -    dev: eval_loss = 0.7046529584460788
05/28/2023 14:04:20 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:20 - INFO - __main__ -    dev: infer_time = 2.8110000000000004
05/28/2023 14:04:20 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:20 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:20 - INFO - __main__ -     cls_loss = 0.7053512617161399
05/28/2023 14:04:20 - INFO - __main__ -     eval_loss = 0.7046529584460788
05/28/2023 14:04:20 - INFO - __main__ -     global_step = 19
05/28/2023 14:04:20 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:20 - INFO - __main__ -     infer_time = 2.8110000000000004
05/28/2023 14:04:20 - INFO - __main__ -     loss = 0.7053512617161399

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.00it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 13.16it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.15it/s][A05/28/2023 14:04:20 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:20 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:04:20 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:20 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.73it/s]
05/28/2023 14:04:20 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:20 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:20 - INFO - __main__ -    dev: eval_loss = 0.6919073727395799
05/28/2023 14:04:20 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:20 - INFO - __main__ -    dev: infer_time = 2.7804444444444445
05/28/2023 14:04:20 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:20 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:20 - INFO - __main__ -     cls_loss = 0.7020721353333572
05/28/2023 14:04:20 - INFO - __main__ -     eval_loss = 0.6919073727395799
05/28/2023 14:04:20 - INFO - __main__ -     global_step = 29
05/28/2023 14:04:20 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:20 - INFO - __main__ -     infer_time = 2.7804444444444445
05/28/2023 14:04:20 - INFO - __main__ -     loss = 0.7020721353333572

Iteration:  38%|###8      | 30/78 [00:02<00:03, 14.68it/s][A
Iteration:  42%|####2     | 33/78 [00:03<00:02, 16.48it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 17.98it/s][A05/28/2023 14:04:21 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:21 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:04:21 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:21 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.65it/s]
05/28/2023 14:04:21 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:21 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:04:21 - INFO - __main__ -    dev: eval_loss = 0.6902199453777738
05/28/2023 14:04:21 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:21 - INFO - __main__ -    dev: infer_time = 2.802888888888889
05/28/2023 14:04:21 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:21 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:04:21 - INFO - __main__ -     cls_loss = 0.6997680281981443
05/28/2023 14:04:21 - INFO - __main__ -     eval_loss = 0.6902199453777738
05/28/2023 14:04:21 - INFO - __main__ -     global_step = 39
05/28/2023 14:04:21 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:21 - INFO - __main__ -     infer_time = 2.802888888888889
05/28/2023 14:04:21 - INFO - __main__ -     loss = 0.6997680281981443

Iteration:  50%|#####     | 39/78 [00:03<00:02, 16.52it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 18.04it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 19.24it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 20.21it/s][A05/28/2023 14:04:21 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:21 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:04:21 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:21 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.51it/s]
05/28/2023 14:04:21 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:21 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:04:21 - INFO - __main__ -    dev: eval_loss = 0.6921590632862515
05/28/2023 14:04:21 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:21 - INFO - __main__ -    dev: infer_time = 2.7957777777777775
05/28/2023 14:04:21 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:21 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:04:21 - INFO - __main__ -     cls_loss = 0.6984724548398232
05/28/2023 14:04:21 - INFO - __main__ -     eval_loss = 0.6921590632862515
05/28/2023 14:04:21 - INFO - __main__ -     global_step = 49
05/28/2023 14:04:21 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:21 - INFO - __main__ -     infer_time = 2.7957777777777775
05/28/2023 14:04:21 - INFO - __main__ -     loss = 0.6984724548398232
05/28/2023 14:04:21 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  65%|######5   | 51/78 [00:05<00:05,  5.37it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:03,  6.97it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:02,  8.77it/s][A05/28/2023 14:04:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:23 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:04:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.43it/s]
05/28/2023 14:04:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:23 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:23 - INFO - __main__ -    dev: eval_loss = 0.6993287801742554
05/28/2023 14:04:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:23 - INFO - __main__ -    dev: infer_time = 2.7831111111111113
05/28/2023 14:04:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:23 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:23 - INFO - __main__ -     cls_loss = 0.6971161951452999
05/28/2023 14:04:23 - INFO - __main__ -     eval_loss = 0.6993287801742554
05/28/2023 14:04:23 - INFO - __main__ -     global_step = 59
05/28/2023 14:04:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:23 - INFO - __main__ -     infer_time = 2.7831111111111113
05/28/2023 14:04:23 - INFO - __main__ -     loss = 0.6971161951452999

Iteration:  76%|#######5  | 59/78 [00:05<00:02,  9.26it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:01, 11.42it/s][A
Iteration:  83%|########3 | 65/78 [00:05<00:00, 13.55it/s][A
Iteration:  87%|########7 | 68/78 [00:06<00:00, 15.48it/s][A05/28/2023 14:04:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:24 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:04:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 118.02it/s]
05/28/2023 14:04:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:24 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:24 - INFO - __main__ -    dev: eval_loss = 0.7029474245177375
05/28/2023 14:04:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:24 - INFO - __main__ -    dev: infer_time = 2.741777777777778
05/28/2023 14:04:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:24 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:24 - INFO - __main__ -     cls_loss = 0.6961949657702792
05/28/2023 14:04:24 - INFO - __main__ -     eval_loss = 0.7029474245177375
05/28/2023 14:04:24 - INFO - __main__ -     global_step = 69
05/28/2023 14:04:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:24 - INFO - __main__ -     infer_time = 2.741777777777778
05/28/2023 14:04:24 - INFO - __main__ -     loss = 0.6961949657702792

Iteration:  91%|#########1| 71/78 [00:06<00:00, 15.01it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 16.78it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 18.24it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.82it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.60s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.60s/it]
05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   w_emb: 13918032

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   p_emb: 233472

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   t_emb: 912

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   ln_emb: 912

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 394848

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 394440

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   intermediate_numel: 394848

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   output_numel: 395352

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 394848

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 394440

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   intermediate_numel: 394848

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   output_numel: 395352

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 394848

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 394440

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   intermediate_numel: 394848

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   output_numel: 395352

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   layer_numel: 4874040
05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   dense_numel: 208392
05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   emb_numel: 14153328

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   encoder_numel: 4874040

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   pooler_numel: 208392

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   all parameters: 19235760

05/28/2023 14:04:24 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:24 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
parameter size = 19235760
best_acc = 0.5306859205776173
time_per_batch_infer = 2.789 ms
infer_cnt = 63
**************E*************

05/28/2023 14:04:24 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [928, 928, 928, 928], 'sample_qkv_sizes': [264, 264, 264, 264]}
05/28/2023 14:04:24 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:04:24 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:04:24 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:24 - INFO - __main__ -   guid: train-0
05/28/2023 14:04:24 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:04:24 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:24 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:24 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:24 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:24 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:26 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:04:26 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:26 - INFO - __main__ -   guid: dev-0
05/28/2023 14:04:26 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:04:26 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:26 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:26 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:26 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:26 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:26 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:04:26 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:04:27 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:04:27 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:04:27 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:04:27 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:04:27 - INFO - __main__ -     Batch size = 32
05/28/2023 14:04:27 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.05it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.75it/s][A05/28/2023 14:04:27 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:27 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:04:27 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:27 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.37it/s]
05/28/2023 14:04:27 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:27 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:27 - INFO - __main__ -    dev: eval_loss = 0.6962689624892341
05/28/2023 14:04:27 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:27 - INFO - __main__ -    dev: infer_time = 3.3707777777777777
05/28/2023 14:04:27 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:27 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:27 - INFO - __main__ -     cls_loss = 0.6951283150249057
05/28/2023 14:04:27 - INFO - __main__ -     eval_loss = 0.6962689624892341
05/28/2023 14:04:27 - INFO - __main__ -     global_step = 9
05/28/2023 14:04:27 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:27 - INFO - __main__ -     infer_time = 3.3707777777777777
05/28/2023 14:04:27 - INFO - __main__ -     loss = 0.6951283150249057
05/28/2023 14:04:27 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.83it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.66it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.73it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.95it/s][A05/28/2023 14:04:29 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:29 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:04:29 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:29 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.05it/s]
05/28/2023 14:04:29 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:29 - INFO - __main__ -    dev: acc = 0.5090252707581228
05/28/2023 14:04:29 - INFO - __main__ -    dev: eval_loss = 0.6928321189350553
05/28/2023 14:04:29 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:29 - INFO - __main__ -    dev: infer_time = 3.4083333333333337
05/28/2023 14:04:29 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:29 - INFO - __main__ -     acc = 0.5090252707581228
05/28/2023 14:04:29 - INFO - __main__ -     cls_loss = 0.6965783834457397
05/28/2023 14:04:29 - INFO - __main__ -     eval_loss = 0.6928321189350553
05/28/2023 14:04:29 - INFO - __main__ -     global_step = 19
05/28/2023 14:04:29 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:29 - INFO - __main__ -     infer_time = 3.4083333333333337
05/28/2023 14:04:29 - INFO - __main__ -     loss = 0.6965783834457397
05/28/2023 14:04:29 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:15,  3.77it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.27it/s][A
Iteration:  33%|###3      | 26/78 [00:04<00:07,  7.06it/s][A05/28/2023 14:04:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:31 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:04:31 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.16it/s]
05/28/2023 14:04:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:31 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:31 - INFO - __main__ -    dev: eval_loss = 0.6974670158492194
05/28/2023 14:04:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:31 - INFO - __main__ -    dev: infer_time = 3.4065555555555553
05/28/2023 14:04:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:31 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:31 - INFO - __main__ -     cls_loss = 0.695365821493083
05/28/2023 14:04:31 - INFO - __main__ -     eval_loss = 0.6974670158492194
05/28/2023 14:04:31 - INFO - __main__ -     global_step = 29
05/28/2023 14:04:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:31 - INFO - __main__ -     infer_time = 3.4065555555555553
05/28/2023 14:04:31 - INFO - __main__ -     loss = 0.695365821493083

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.39it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.41it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.49it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.49it/s][A05/28/2023 14:04:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:31 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:04:31 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.44it/s]
05/28/2023 14:04:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:31 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:31 - INFO - __main__ -    dev: eval_loss = 0.6938551796807183
05/28/2023 14:04:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:31 - INFO - __main__ -    dev: infer_time = 3.376
05/28/2023 14:04:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:31 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:31 - INFO - __main__ -     cls_loss = 0.6948591776383228
05/28/2023 14:04:31 - INFO - __main__ -     eval_loss = 0.6938551796807183
05/28/2023 14:04:31 - INFO - __main__ -     global_step = 39
05/28/2023 14:04:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:31 - INFO - __main__ -     infer_time = 3.376
05/28/2023 14:04:31 - INFO - __main__ -     loss = 0.6948591776383228

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.24it/s][A
Iteration:  56%|#####6    | 44/78 [00:05<00:02, 16.03it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.55it/s][A05/28/2023 14:04:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:32 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:04:32 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.35it/s]
05/28/2023 14:04:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:32 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:32 - INFO - __main__ -    dev: eval_loss = 0.6907986601193746
05/28/2023 14:04:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:32 - INFO - __main__ -    dev: infer_time = 3.3664444444444444
05/28/2023 14:04:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:32 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:32 - INFO - __main__ -     cls_loss = 0.6939975947749858
05/28/2023 14:04:32 - INFO - __main__ -     eval_loss = 0.6907986601193746
05/28/2023 14:04:32 - INFO - __main__ -     global_step = 49
05/28/2023 14:04:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:32 - INFO - __main__ -     infer_time = 3.3664444444444444
05/28/2023 14:04:32 - INFO - __main__ -     loss = 0.6939975947749858
05/28/2023 14:04:32 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  64%|######4   | 50/78 [00:06<00:05,  5.01it/s][A
Iteration:  68%|######7   | 53/78 [00:06<00:03,  6.52it/s][A
Iteration:  72%|#######1  | 56/78 [00:07<00:02,  8.29it/s][A05/28/2023 14:04:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:34 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:04:34 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.21it/s]
05/28/2023 14:04:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:34 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:34 - INFO - __main__ -    dev: eval_loss = 0.6918063428666856
05/28/2023 14:04:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:34 - INFO - __main__ -    dev: infer_time = 3.398444444444445
05/28/2023 14:04:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:34 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:34 - INFO - __main__ -     cls_loss = 0.6933706433086072
05/28/2023 14:04:34 - INFO - __main__ -     eval_loss = 0.6918063428666856
05/28/2023 14:04:34 - INFO - __main__ -     global_step = 59
05/28/2023 14:04:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:34 - INFO - __main__ -     infer_time = 3.398444444444445
05/28/2023 14:04:34 - INFO - __main__ -     loss = 0.6933706433086072

Iteration:  76%|#######5  | 59/78 [00:07<00:02,  9.45it/s][A
Iteration:  79%|#######9  | 62/78 [00:07<00:01, 11.38it/s][A
Iteration:  83%|########3 | 65/78 [00:07<00:00, 13.38it/s][A
Iteration:  87%|########7 | 68/78 [00:07<00:00, 15.24it/s][A05/28/2023 14:04:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:34 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:04:34 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.33it/s]
05/28/2023 14:04:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:34 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:04:34 - INFO - __main__ -    dev: eval_loss = 0.6903306312031217
05/28/2023 14:04:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:34 - INFO - __main__ -    dev: infer_time = 3.402222222222222
05/28/2023 14:04:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:34 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:04:34 - INFO - __main__ -     cls_loss = 0.6940973746603813
05/28/2023 14:04:34 - INFO - __main__ -     eval_loss = 0.6903306312031217
05/28/2023 14:04:34 - INFO - __main__ -     global_step = 69
05/28/2023 14:04:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:34 - INFO - __main__ -     infer_time = 3.402222222222222
05/28/2023 14:04:34 - INFO - __main__ -     loss = 0.6940973746603813
05/28/2023 14:04:34 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  91%|#########1| 71/78 [00:09<00:01,  4.79it/s][A
Iteration:  95%|#########4| 74/78 [00:09<00:00,  6.24it/s][A
Iteration:  99%|#########8| 77/78 [00:09<00:00,  7.95it/s][AIteration: 100%|##########| 78/78 [00:09<00:00,  8.17it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.54s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.54s/it]
05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   w_emb: 8057808

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   p_emb: 135168

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   t_emb: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ln_emb: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 245920

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 245256

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 245920

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   output_numel: 245784

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 245920

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 245256

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 245920

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   output_numel: 245784

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 245920

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 245256

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 245920

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   output_numel: 245784

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 245920

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 245256

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 245920

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   output_numel: 245784

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   layer_numel: 3088288
05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   dense_numel: 69960
05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   emb_numel: 8194032

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   encoder_numel: 3088288

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   pooler_numel: 69960

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   all parameters: 11352280

05/28/2023 14:04:36 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:36 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [928, 928, 928, 928], 'sample_qkv_sizes': [264, 264, 264, 264]}
parameter size = 11352280
best_acc = 0.5306859205776173
time_per_batch_infer = 3.390 ms
infer_cnt = 63
**************E*************

05/28/2023 14:04:36 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [352, 352, 352, 352], 'sample_qkv_sizes': [252, 252, 252, 252]}
05/28/2023 14:04:36 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:04:36 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:04:36 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:36 - INFO - __main__ -   guid: train-0
05/28/2023 14:04:36 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:04:36 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:36 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:36 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:38 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:04:38 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:38 - INFO - __main__ -   guid: dev-0
05/28/2023 14:04:38 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:04:38 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:38 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:38 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:38 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:38 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:04:38 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:04:39 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:04:39 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:04:39 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:04:39 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:04:39 - INFO - __main__ -     Batch size = 32
05/28/2023 14:04:39 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 24.28it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 25.87it/s][A05/28/2023 14:04:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:39 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:04:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 170.84it/s]
05/28/2023 14:04:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:39 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:04:39 - INFO - __main__ -    dev: eval_loss = 0.6916846301820543
05/28/2023 14:04:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:39 - INFO - __main__ -    dev: infer_time = 3.3824444444444444
05/28/2023 14:04:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:39 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:04:39 - INFO - __main__ -     cls_loss = 0.6937178240882026
05/28/2023 14:04:39 - INFO - __main__ -     eval_loss = 0.6916846301820543
05/28/2023 14:04:39 - INFO - __main__ -     global_step = 9
05/28/2023 14:04:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:39 - INFO - __main__ -     infer_time = 3.3824444444444444
05/28/2023 14:04:39 - INFO - __main__ -     loss = 0.6937178240882026
05/28/2023 14:04:39 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:16,  4.10it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:10,  6.10it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.44it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.98it/s][A05/28/2023 14:04:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:41 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:04:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.06it/s]
05/28/2023 14:04:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:41 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:04:41 - INFO - __main__ -    dev: eval_loss = 0.6919482350349426
05/28/2023 14:04:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:41 - INFO - __main__ -    dev: infer_time = 3.4135555555555555
05/28/2023 14:04:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:41 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:04:41 - INFO - __main__ -     cls_loss = 0.6948356534305372
05/28/2023 14:04:41 - INFO - __main__ -     eval_loss = 0.6919482350349426
05/28/2023 14:04:41 - INFO - __main__ -     global_step = 19
05/28/2023 14:04:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:41 - INFO - __main__ -     infer_time = 3.4135555555555555
05/28/2023 14:04:41 - INFO - __main__ -     loss = 0.6948356534305372
05/28/2023 14:04:41 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.49it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:08,  6.09it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  8.02it/s][A05/28/2023 14:04:42 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:42 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:04:42 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:42 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 172.56it/s]
05/28/2023 14:04:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:43 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:43 - INFO - __main__ -    dev: eval_loss = 0.691006428665585
05/28/2023 14:04:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:43 - INFO - __main__ -    dev: infer_time = 3.4847777777777775
05/28/2023 14:04:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:43 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:43 - INFO - __main__ -     cls_loss = 0.6941942798680273
05/28/2023 14:04:43 - INFO - __main__ -     eval_loss = 0.691006428665585
05/28/2023 14:04:43 - INFO - __main__ -     global_step = 29
05/28/2023 14:04:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:43 - INFO - __main__ -     infer_time = 3.4847777777777775
05/28/2023 14:04:43 - INFO - __main__ -     loss = 0.6941942798680273

Iteration:  38%|###8      | 30/78 [00:03<00:05,  9.56it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.88it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:02, 14.27it/s][A05/28/2023 14:04:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:43 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:04:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.11it/s]
05/28/2023 14:04:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:43 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:43 - INFO - __main__ -    dev: eval_loss = 0.6923935413360596
05/28/2023 14:04:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:43 - INFO - __main__ -    dev: infer_time = 3.464666666666667
05/28/2023 14:04:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:43 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:43 - INFO - __main__ -     cls_loss = 0.6938856305220188
05/28/2023 14:04:43 - INFO - __main__ -     eval_loss = 0.6923935413360596
05/28/2023 14:04:43 - INFO - __main__ -     global_step = 39
05/28/2023 14:04:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:43 - INFO - __main__ -     infer_time = 3.464666666666667
05/28/2023 14:04:43 - INFO - __main__ -     loss = 0.6938856305220188

Iteration:  50%|#####     | 39/78 [00:04<00:02, 15.07it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 17.26it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 19.30it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 20.98it/s][A05/28/2023 14:04:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:43 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:04:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.26it/s]
05/28/2023 14:04:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:43 - INFO - __main__ -    dev: acc = 0.48014440433212996
05/28/2023 14:04:43 - INFO - __main__ -    dev: eval_loss = 0.6937014924155341
05/28/2023 14:04:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:43 - INFO - __main__ -    dev: infer_time = 3.4582222222222225
05/28/2023 14:04:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:43 - INFO - __main__ -     acc = 0.48014440433212996
05/28/2023 14:04:43 - INFO - __main__ -     cls_loss = 0.6938912868499756
05/28/2023 14:04:43 - INFO - __main__ -     eval_loss = 0.6937014924155341
05/28/2023 14:04:43 - INFO - __main__ -     global_step = 49
05/28/2023 14:04:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:43 - INFO - __main__ -     infer_time = 3.4582222222222225
05/28/2023 14:04:43 - INFO - __main__ -     loss = 0.6938912868499756

Iteration:  65%|######5   | 51/78 [00:04<00:01, 19.55it/s][A
Iteration:  69%|######9   | 54/78 [00:04<00:01, 21.23it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:00, 22.49it/s][A05/28/2023 14:04:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:44 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:04:44 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.08it/s]
05/28/2023 14:04:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:44 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:44 - INFO - __main__ -    dev: eval_loss = 0.7001434167226156
05/28/2023 14:04:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:44 - INFO - __main__ -    dev: infer_time = 3.4603333333333333
05/28/2023 14:04:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:44 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:44 - INFO - __main__ -     cls_loss = 0.693168277457609
05/28/2023 14:04:44 - INFO - __main__ -     eval_loss = 0.7001434167226156
05/28/2023 14:04:44 - INFO - __main__ -     global_step = 59
05/28/2023 14:04:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:44 - INFO - __main__ -     infer_time = 3.4603333333333333
05/28/2023 14:04:44 - INFO - __main__ -     loss = 0.693168277457609

Iteration:  77%|#######6  | 60/78 [00:05<00:00, 20.49it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 21.75it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 23.00it/s][A05/28/2023 14:04:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:44 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:04:44 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.23it/s]
05/28/2023 14:04:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:44 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:44 - INFO - __main__ -    dev: eval_loss = 0.7009867760870192
05/28/2023 14:04:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:44 - INFO - __main__ -    dev: infer_time = 3.452888888888889
05/28/2023 14:04:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:44 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:44 - INFO - __main__ -     cls_loss = 0.6932228440823762
05/28/2023 14:04:44 - INFO - __main__ -     eval_loss = 0.7009867760870192
05/28/2023 14:04:44 - INFO - __main__ -     global_step = 69
05/28/2023 14:04:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:44 - INFO - __main__ -     infer_time = 3.452888888888889
05/28/2023 14:04:44 - INFO - __main__ -     loss = 0.6932228440823762

Iteration:  88%|########8 | 69/78 [00:05<00:00, 21.02it/s][A
Iteration:  92%|#########2| 72/78 [00:05<00:00, 22.10it/s][A
Iteration:  96%|#########6| 75/78 [00:05<00:00, 23.22it/s][AIteration: 100%|##########| 78/78 [00:05<00:00, 13.05it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.98s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.98s/it]
05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   w_emb: 7691544

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   p_emb: 129024

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   t_emb: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ln_emb: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 89056

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 88956

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 89056

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   output_numel: 89460

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 89056

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 88956

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 89056

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   output_numel: 89460

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 89056

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 88956

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 89056

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   output_numel: 89460

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 89056

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 88956

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 89056

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   output_numel: 89460

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   layer_numel: 1736176
05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   dense_numel: 63756
05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   emb_numel: 7821576

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   encoder_numel: 1736176

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   pooler_numel: 63756

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   all parameters: 9621508

05/28/2023 14:04:45 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:45 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [352, 352, 352, 352], 'sample_qkv_sizes': [252, 252, 252, 252]}
parameter size = 9621508
best_acc = 0.5379061371841155
time_per_batch_infer = 3.445 ms
infer_cnt = 63
**************E*************

05/28/2023 14:04:45 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [416, 416, 416, 416], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
05/28/2023 14:04:45 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:04:45 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:04:45 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:45 - INFO - __main__ -   guid: train-0
05/28/2023 14:04:45 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:04:45 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:45 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:45 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:45 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:45 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:46 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:04:46 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:46 - INFO - __main__ -   guid: dev-0
05/28/2023 14:04:46 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:04:46 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:46 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:46 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:46 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:46 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:47 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:04:47 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:04:47 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:04:47 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:04:47 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:04:47 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:04:47 - INFO - __main__ -     Batch size = 32
05/28/2023 14:04:47 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.07it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.64it/s][A05/28/2023 14:04:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:48 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:04:48 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 161.62it/s]
05/28/2023 14:04:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:48 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:48 - INFO - __main__ -    dev: eval_loss = 0.7114454772737291
05/28/2023 14:04:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:48 - INFO - __main__ -    dev: infer_time = 3.432777777777778
05/28/2023 14:04:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:48 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:48 - INFO - __main__ -     cls_loss = 0.6926782263649834
05/28/2023 14:04:48 - INFO - __main__ -     eval_loss = 0.7114454772737291
05/28/2023 14:04:48 - INFO - __main__ -     global_step = 9
05/28/2023 14:04:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:48 - INFO - __main__ -     infer_time = 3.432777777777778
05/28/2023 14:04:48 - INFO - __main__ -     loss = 0.6926782263649834
05/28/2023 14:04:48 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.00it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.92it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.16it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.58it/s][A05/28/2023 14:04:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:49 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:04:49 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 161.65it/s]
05/28/2023 14:04:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:49 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:49 - INFO - __main__ -    dev: eval_loss = 0.6933309435844421
05/28/2023 14:04:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:49 - INFO - __main__ -    dev: infer_time = 3.397777777777778
05/28/2023 14:04:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:49 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:49 - INFO - __main__ -     cls_loss = 0.6988251805305481
05/28/2023 14:04:49 - INFO - __main__ -     eval_loss = 0.6933309435844421
05/28/2023 14:04:49 - INFO - __main__ -     global_step = 19
05/28/2023 14:04:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:49 - INFO - __main__ -     infer_time = 3.397777777777778
05/28/2023 14:04:49 - INFO - __main__ -     loss = 0.6988251805305481
05/28/2023 14:04:49 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.46it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:08,  6.04it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.91it/s][A05/28/2023 14:04:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:51 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:04:51 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 161.15it/s]
05/28/2023 14:04:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:51 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:04:51 - INFO - __main__ -    dev: eval_loss = 0.692537804444631
05/28/2023 14:04:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:51 - INFO - __main__ -    dev: infer_time = 3.449
05/28/2023 14:04:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:51 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:04:51 - INFO - __main__ -     cls_loss = 0.6961509297633993
05/28/2023 14:04:51 - INFO - __main__ -     eval_loss = 0.692537804444631
05/28/2023 14:04:51 - INFO - __main__ -     global_step = 29
05/28/2023 14:04:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:51 - INFO - __main__ -     infer_time = 3.449
05/28/2023 14:04:51 - INFO - __main__ -     loss = 0.6961509297633993

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.40it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.63it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.90it/s][A05/28/2023 14:04:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:52 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:04:52 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 162.22it/s]
05/28/2023 14:04:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:52 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:04:52 - INFO - __main__ -    dev: eval_loss = 0.6931523349550035
05/28/2023 14:04:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:52 - INFO - __main__ -    dev: infer_time = 3.392777777777778
05/28/2023 14:04:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:52 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:04:52 - INFO - __main__ -     cls_loss = 0.696372159016438
05/28/2023 14:04:52 - INFO - __main__ -     eval_loss = 0.6931523349550035
05/28/2023 14:04:52 - INFO - __main__ -     global_step = 39
05/28/2023 14:04:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:52 - INFO - __main__ -     infer_time = 3.392777777777778
05/28/2023 14:04:52 - INFO - __main__ -     loss = 0.696372159016438

Iteration:  50%|#####     | 39/78 [00:04<00:02, 14.56it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 16.66it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 18.51it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 20.04it/s][A05/28/2023 14:04:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:52 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:04:52 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 162.17it/s]
05/28/2023 14:04:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:52 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:04:52 - INFO - __main__ -    dev: eval_loss = 0.6939787136183845
05/28/2023 14:04:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:52 - INFO - __main__ -    dev: infer_time = 3.4243333333333332
05/28/2023 14:04:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:52 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:04:52 - INFO - __main__ -     cls_loss = 0.6960789074703139
05/28/2023 14:04:52 - INFO - __main__ -     eval_loss = 0.6939787136183845
05/28/2023 14:04:52 - INFO - __main__ -     global_step = 49
05/28/2023 14:04:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:52 - INFO - __main__ -     infer_time = 3.4243333333333332
05/28/2023 14:04:52 - INFO - __main__ -     loss = 0.6960789074703139

Iteration:  65%|######5   | 51/78 [00:04<00:01, 18.78it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 20.29it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:00, 21.44it/s][A05/28/2023 14:04:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:52 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:04:52 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 162.10it/s]
05/28/2023 14:04:53 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:53 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:04:53 - INFO - __main__ -    dev: eval_loss = 0.6922779281934103
05/28/2023 14:04:53 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:53 - INFO - __main__ -    dev: infer_time = 3.401888888888889
05/28/2023 14:04:53 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:53 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:04:53 - INFO - __main__ -     cls_loss = 0.695436917119107
05/28/2023 14:04:53 - INFO - __main__ -     eval_loss = 0.6922779281934103
05/28/2023 14:04:53 - INFO - __main__ -     global_step = 59
05/28/2023 14:04:53 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:53 - INFO - __main__ -     infer_time = 3.401888888888889
05/28/2023 14:04:53 - INFO - __main__ -     loss = 0.695436917119107
05/28/2023 14:04:53 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  77%|#######6  | 60/78 [00:06<00:03,  5.37it/s][A
Iteration:  81%|########  | 63/78 [00:06<00:02,  7.02it/s][A
Iteration:  85%|########4 | 66/78 [00:07<00:01,  8.96it/s][A05/28/2023 14:04:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:54 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:04:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 161.73it/s]
05/28/2023 14:04:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:54 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:04:54 - INFO - __main__ -    dev: eval_loss = 0.6924392845895555
05/28/2023 14:04:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:54 - INFO - __main__ -    dev: infer_time = 3.417333333333333
05/28/2023 14:04:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:54 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:04:54 - INFO - __main__ -     cls_loss = 0.6951182596925376
05/28/2023 14:04:54 - INFO - __main__ -     eval_loss = 0.6924392845895555
05/28/2023 14:04:54 - INFO - __main__ -     global_step = 69
05/28/2023 14:04:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:54 - INFO - __main__ -     infer_time = 3.417333333333333
05/28/2023 14:04:54 - INFO - __main__ -     loss = 0.6951182596925376

Iteration:  88%|########8 | 69/78 [00:07<00:00, 10.36it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:00, 12.57it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00, 14.78it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.38it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.51s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.51s/it]
05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   w_emb: 8057808

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   p_emb: 135168

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   t_emb: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ln_emb: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 110240

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 110088

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   intermediate_numel: 110240

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   output_numel: 110616

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 110240

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 110088

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   intermediate_numel: 110240

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   output_numel: 110616

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 110240

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 110088

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   intermediate_numel: 110240

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   output_numel: 110616

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 110240

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 110088

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   intermediate_numel: 110240

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   output_numel: 110616

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   layer_numel: 2004896
05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   dense_numel: 69960
05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   emb_numel: 8194032

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   encoder_numel: 2004896

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   pooler_numel: 69960

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   all parameters: 10268888

05/28/2023 14:04:55 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:04:55 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [416, 416, 416, 416], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
parameter size = 10268888
best_acc = 0.5306859205776173
time_per_batch_infer = 3.417 ms
infer_cnt = 63
**************E*************

05/28/2023 14:04:55 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 528, 'sample_intermediate_sizes': [640, 640, 640], 'sample_qkv_sizes': [528, 528, 528]}
05/28/2023 14:04:55 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:04:55 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:04:55 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:55 - INFO - __main__ -   guid: train-0
05/28/2023 14:04:55 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:04:55 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:55 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:55 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:55 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:55 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:56 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:04:56 - INFO - __main__ -   *** Example ***
05/28/2023 14:04:56 - INFO - __main__ -   guid: dev-0
05/28/2023 14:04:56 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:04:56 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:56 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:56 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:04:56 - INFO - __main__ -   label: not_entailment
05/28/2023 14:04:56 - INFO - __main__ -   label_id: 1
05/28/2023 14:04:57 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:04:57 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:04:57 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:04:57 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:04:57 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:04:57 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:04:57 - INFO - __main__ -     Batch size = 32
05/28/2023 14:04:57 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.82it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.35it/s][A05/28/2023 14:04:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:58 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:04:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.57it/s]
05/28/2023 14:04:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:04:58 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:04:58 - INFO - __main__ -    dev: eval_loss = 0.6918191048834059
05/28/2023 14:04:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:04:58 - INFO - __main__ -    dev: infer_time = 2.6865555555555556
05/28/2023 14:04:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:04:58 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:04:58 - INFO - __main__ -     cls_loss = 0.7010318769348992
05/28/2023 14:04:58 - INFO - __main__ -     eval_loss = 0.6918191048834059
05/28/2023 14:04:58 - INFO - __main__ -     global_step = 9
05/28/2023 14:04:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:04:58 - INFO - __main__ -     infer_time = 2.6865555555555556
05/28/2023 14:04:58 - INFO - __main__ -     loss = 0.7010318769348992
05/28/2023 14:04:58 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.85it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.70it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.80it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.05it/s][A05/28/2023 14:04:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:04:59 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:04:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:04:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.46it/s]
05/28/2023 14:05:00 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:00 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:00 - INFO - __main__ -    dev: eval_loss = 0.6913492878278097
05/28/2023 14:05:00 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:00 - INFO - __main__ -    dev: infer_time = 2.6897777777777776
05/28/2023 14:05:00 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:00 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:00 - INFO - __main__ -     cls_loss = 0.6990539117863304
05/28/2023 14:05:00 - INFO - __main__ -     eval_loss = 0.6913492878278097
05/28/2023 14:05:00 - INFO - __main__ -     global_step = 19
05/28/2023 14:05:00 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:00 - INFO - __main__ -     infer_time = 2.6897777777777776
05/28/2023 14:05:00 - INFO - __main__ -     loss = 0.6990539117863304

Iteration:  27%|##6       | 21/78 [00:02<00:05, 10.97it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 13.12it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.13it/s][A05/28/2023 14:05:00 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:00 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:05:00 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:00 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.43it/s]
05/28/2023 14:05:00 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:00 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:00 - INFO - __main__ -    dev: eval_loss = 0.6916051904360453
05/28/2023 14:05:00 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:00 - INFO - __main__ -    dev: infer_time = 2.7238888888888884
05/28/2023 14:05:00 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:00 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:00 - INFO - __main__ -     cls_loss = 0.6979040359628612
05/28/2023 14:05:00 - INFO - __main__ -     eval_loss = 0.6916051904360453
05/28/2023 14:05:00 - INFO - __main__ -     global_step = 29
05/28/2023 14:05:00 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:00 - INFO - __main__ -     infer_time = 2.7238888888888884
05/28/2023 14:05:00 - INFO - __main__ -     loss = 0.6979040359628612

Iteration:  38%|###8      | 30/78 [00:02<00:03, 14.68it/s][A
Iteration:  42%|####2     | 33/78 [00:03<00:02, 16.46it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 17.92it/s][A05/28/2023 14:05:01 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:01 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:05:01 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:01 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.47it/s]
05/28/2023 14:05:01 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:01 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 14:05:01 - INFO - __main__ -    dev: eval_loss = 0.6920090582635667
05/28/2023 14:05:01 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:01 - INFO - __main__ -    dev: infer_time = 2.727444444444444
05/28/2023 14:05:01 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:01 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 14:05:01 - INFO - __main__ -     cls_loss = 0.6975208857120612
05/28/2023 14:05:01 - INFO - __main__ -     eval_loss = 0.6920090582635667
05/28/2023 14:05:01 - INFO - __main__ -     global_step = 39
05/28/2023 14:05:01 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:01 - INFO - __main__ -     infer_time = 2.727444444444444
05/28/2023 14:05:01 - INFO - __main__ -     loss = 0.6975208857120612
05/28/2023 14:05:01 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:04<00:07,  5.21it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:05,  6.75it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:03,  8.57it/s][A
Iteration:  62%|######1   | 48/78 [00:05<00:02, 10.54it/s][A05/28/2023 14:05:02 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:02 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:05:02 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:02 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.53it/s]
05/28/2023 14:05:02 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:02 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:02 - INFO - __main__ -    dev: eval_loss = 0.6959124472406175
05/28/2023 14:05:02 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:02 - INFO - __main__ -    dev: infer_time = 2.675111111111111
05/28/2023 14:05:02 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:02 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:02 - INFO - __main__ -     cls_loss = 0.6968981380365333
05/28/2023 14:05:02 - INFO - __main__ -     eval_loss = 0.6959124472406175
05/28/2023 14:05:02 - INFO - __main__ -     global_step = 49
05/28/2023 14:05:02 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:02 - INFO - __main__ -     infer_time = 2.675111111111111
05/28/2023 14:05:02 - INFO - __main__ -     loss = 0.6968981380365333

Iteration:  64%|######4   | 50/78 [00:05<00:02, 10.73it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 12.96it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 14.83it/s][A05/28/2023 14:05:03 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:03 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:05:03 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:03 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.67it/s]
05/28/2023 14:05:03 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:03 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:05:03 - INFO - __main__ -    dev: eval_loss = 0.691331062051985
05/28/2023 14:05:03 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:03 - INFO - __main__ -    dev: infer_time = 2.687111111111111
05/28/2023 14:05:03 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:03 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:05:03 - INFO - __main__ -     cls_loss = 0.69647520982613
05/28/2023 14:05:03 - INFO - __main__ -     eval_loss = 0.691331062051985
05/28/2023 14:05:03 - INFO - __main__ -     global_step = 59
05/28/2023 14:05:03 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:03 - INFO - __main__ -     infer_time = 2.687111111111111
05/28/2023 14:05:03 - INFO - __main__ -     loss = 0.69647520982613

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 14.47it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:00, 16.28it/s][A
Iteration:  83%|########3 | 65/78 [00:06<00:00, 17.75it/s][A
Iteration:  87%|########7 | 68/78 [00:06<00:00, 19.01it/s][A05/28/2023 14:05:03 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:03 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:05:03 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:03 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.65it/s]
05/28/2023 14:05:03 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:03 - INFO - __main__ -    dev: acc = 0.5451263537906137
05/28/2023 14:05:03 - INFO - __main__ -    dev: eval_loss = 0.6913174589474996
05/28/2023 14:05:03 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:03 - INFO - __main__ -    dev: infer_time = 2.696555555555556
05/28/2023 14:05:03 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:03 - INFO - __main__ -     acc = 0.5451263537906137
05/28/2023 14:05:03 - INFO - __main__ -     cls_loss = 0.6956903312517249
05/28/2023 14:05:03 - INFO - __main__ -     eval_loss = 0.6913174589474996
05/28/2023 14:05:03 - INFO - __main__ -     global_step = 69
05/28/2023 14:05:03 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:03 - INFO - __main__ -     infer_time = 2.696555555555556
05/28/2023 14:05:03 - INFO - __main__ -     loss = 0.6956903312517249
05/28/2023 14:05:03 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  91%|#########1| 71/78 [00:07<00:01,  5.25it/s][A
Iteration:  95%|#########4| 74/78 [00:07<00:00,  6.83it/s][A
Iteration:  99%|#########8| 77/78 [00:07<00:00,  8.64it/s][AIteration: 100%|##########| 78/78 [00:07<00:00,  9.80it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.96s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.96s/it]
05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   w_emb: 16115616

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   p_emb: 270336

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   t_emb: 1056

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   ln_emb: 1056

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   query_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   key_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   value_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   ln_numel: 1056

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   self_numel: 837936

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   output_numel: 280368

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 338560

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 338448

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   ln_numel: 1056

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   attention_numel: 1118304

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   intermediate_numel: 338560

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   output_numel: 339504

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   query_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   key_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   value_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   ln_numel: 1056

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   self_numel: 837936

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   output_numel: 280368

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 338560

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 338448

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   ln_numel: 1056

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   attention_numel: 1118304

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   intermediate_numel: 338560

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   output_numel: 339504

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   query_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   key_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   value_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   ln_numel: 1056

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   self_numel: 837936

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   output_numel: 280368

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 338560

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 338448

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   ln_numel: 1056

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   attention_numel: 1118304

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   intermediate_numel: 338560

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   output_numel: 339504

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   layer_numel: 5389104
05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   dense_numel: 279312
05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   emb_numel: 16388064

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   encoder_numel: 5389104

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   pooler_numel: 279312

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   all parameters: 22056480

05/28/2023 14:05:05 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:05 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 528, 'sample_intermediate_sizes': [640, 640, 640], 'sample_qkv_sizes': [528, 528, 528]}
parameter size = 22056480
best_acc = 0.5451263537906137
time_per_batch_infer = 2.698 ms
infer_cnt = 63
**************E*************

05/28/2023 14:05:05 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
05/28/2023 14:05:05 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:05:05 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:05:05 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:05 - INFO - __main__ -   guid: train-0
05/28/2023 14:05:05 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:05:05 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:05 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:05 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:05 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:05 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:07 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:05:07 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:07 - INFO - __main__ -   guid: dev-0
05/28/2023 14:05:07 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:05:07 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:07 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:07 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:07 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:07 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:07 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:05:07 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:05:08 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:05:08 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:05:08 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:05:08 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:05:08 - INFO - __main__ -     Batch size = 32
05/28/2023 14:05:08 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.40it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.71it/s][A05/28/2023 14:05:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:08 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:05:08 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.74it/s]
05/28/2023 14:05:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:08 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:08 - INFO - __main__ -    dev: eval_loss = 0.6927207310994467
05/28/2023 14:05:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:08 - INFO - __main__ -    dev: infer_time = 2.793
05/28/2023 14:05:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:08 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:08 - INFO - __main__ -     cls_loss = 0.6955466071764628
05/28/2023 14:05:08 - INFO - __main__ -     eval_loss = 0.6927207310994467
05/28/2023 14:05:08 - INFO - __main__ -     global_step = 9
05/28/2023 14:05:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:08 - INFO - __main__ -     infer_time = 2.793
05/28/2023 14:05:08 - INFO - __main__ -     loss = 0.6955466071764628
05/28/2023 14:05:08 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.64it/s][A
Iteration:  15%|#5        | 12/78 [00:02<00:12,  5.41it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.42it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.59it/s][A05/28/2023 14:05:10 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:10 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:05:10 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:10 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.85it/s]
05/28/2023 14:05:10 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:10 - INFO - __main__ -    dev: acc = 0.5667870036101083
05/28/2023 14:05:10 - INFO - __main__ -    dev: eval_loss = 0.6915707985560099
05/28/2023 14:05:10 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:10 - INFO - __main__ -    dev: infer_time = 2.800222222222222
05/28/2023 14:05:10 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:10 - INFO - __main__ -     acc = 0.5667870036101083
05/28/2023 14:05:10 - INFO - __main__ -     cls_loss = 0.6925601331811202
05/28/2023 14:05:10 - INFO - __main__ -     eval_loss = 0.6915707985560099
05/28/2023 14:05:10 - INFO - __main__ -     global_step = 19
05/28/2023 14:05:10 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:10 - INFO - __main__ -     infer_time = 2.800222222222222
05/28/2023 14:05:10 - INFO - __main__ -     loss = 0.6925601331811202
05/28/2023 14:05:10 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:15,  3.86it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.38it/s][A
Iteration:  33%|###3      | 26/78 [00:04<00:07,  7.16it/s][A05/28/2023 14:05:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:12 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:05:12 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.87it/s]
05/28/2023 14:05:12 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:12 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:12 - INFO - __main__ -    dev: eval_loss = 0.715790741973453
05/28/2023 14:05:12 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:12 - INFO - __main__ -    dev: infer_time = 2.813
05/28/2023 14:05:12 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:12 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:12 - INFO - __main__ -     cls_loss = 0.6926099575799087
05/28/2023 14:05:12 - INFO - __main__ -     eval_loss = 0.715790741973453
05/28/2023 14:05:12 - INFO - __main__ -     global_step = 29
05/28/2023 14:05:12 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:12 - INFO - __main__ -     infer_time = 2.813
05/28/2023 14:05:12 - INFO - __main__ -     loss = 0.6926099575799087

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.43it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.45it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.47it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.42it/s][A05/28/2023 14:05:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:12 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:05:12 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.98it/s]
05/28/2023 14:05:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:13 - INFO - __main__ -    dev: acc = 0.5595667870036101
05/28/2023 14:05:13 - INFO - __main__ -    dev: eval_loss = 0.6918712059656779
05/28/2023 14:05:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:13 - INFO - __main__ -    dev: infer_time = 2.8127777777777774
05/28/2023 14:05:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:13 - INFO - __main__ -     acc = 0.5595667870036101
05/28/2023 14:05:13 - INFO - __main__ -     cls_loss = 0.6976162470304049
05/28/2023 14:05:13 - INFO - __main__ -     eval_loss = 0.6918712059656779
05/28/2023 14:05:13 - INFO - __main__ -     global_step = 39
05/28/2023 14:05:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:13 - INFO - __main__ -     infer_time = 2.8127777777777774
05/28/2023 14:05:13 - INFO - __main__ -     loss = 0.6976162470304049

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.12it/s][A
Iteration:  56%|#####6    | 44/78 [00:05<00:02, 15.80it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.31it/s][A05/28/2023 14:05:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:13 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:05:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 111.94it/s]
05/28/2023 14:05:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:13 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:13 - INFO - __main__ -    dev: eval_loss = 0.6907127896944681
05/28/2023 14:05:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:13 - INFO - __main__ -    dev: infer_time = 2.803444444444444
05/28/2023 14:05:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:13 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:13 - INFO - __main__ -     cls_loss = 0.6975139501143475
05/28/2023 14:05:13 - INFO - __main__ -     eval_loss = 0.6907127896944681
05/28/2023 14:05:13 - INFO - __main__ -     global_step = 49
05/28/2023 14:05:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:13 - INFO - __main__ -     infer_time = 2.803444444444444
05/28/2023 14:05:13 - INFO - __main__ -     loss = 0.6975139501143475

Iteration:  64%|######4   | 50/78 [00:05<00:01, 15.83it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 17.37it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 18.53it/s][A05/28/2023 14:05:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:14 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:05:14 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.07it/s]
05/28/2023 14:05:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:14 - INFO - __main__ -    dev: acc = 0.5667870036101083
05/28/2023 14:05:14 - INFO - __main__ -    dev: eval_loss = 0.6920402513609992
05/28/2023 14:05:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:14 - INFO - __main__ -    dev: infer_time = 2.804777777777778
05/28/2023 14:05:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:14 - INFO - __main__ -     acc = 0.5667870036101083
05/28/2023 14:05:14 - INFO - __main__ -     cls_loss = 0.697377766593028
05/28/2023 14:05:14 - INFO - __main__ -     eval_loss = 0.6920402513609992
05/28/2023 14:05:14 - INFO - __main__ -     global_step = 59
05/28/2023 14:05:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:14 - INFO - __main__ -     infer_time = 2.804777777777778
05/28/2023 14:05:14 - INFO - __main__ -     loss = 0.697377766593028

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 16.59it/s][A
Iteration:  78%|#######8  | 61/78 [00:06<00:00, 17.23it/s][A
Iteration:  82%|########2 | 64/78 [00:06<00:00, 18.65it/s][A
Iteration:  86%|########5 | 67/78 [00:06<00:00, 19.56it/s][A05/28/2023 14:05:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:14 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:05:14 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.11it/s]
05/28/2023 14:05:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:14 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:14 - INFO - __main__ -    dev: eval_loss = 0.6953907741440667
05/28/2023 14:05:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:14 - INFO - __main__ -    dev: infer_time = 2.7963333333333336
05/28/2023 14:05:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:14 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:14 - INFO - __main__ -     cls_loss = 0.6967369343923486
05/28/2023 14:05:14 - INFO - __main__ -     eval_loss = 0.6953907741440667
05/28/2023 14:05:14 - INFO - __main__ -     global_step = 69
05/28/2023 14:05:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:14 - INFO - __main__ -     infer_time = 2.7963333333333336
05/28/2023 14:05:14 - INFO - __main__ -     loss = 0.6967369343923486

Iteration:  90%|########9 | 70/78 [00:06<00:00, 17.10it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 18.36it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 19.39it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.40it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.84s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.84s/it]
05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   w_emb: 14284296

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   p_emb: 239616

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   t_emb: 936

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   ln_emb: 936

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 435232

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 434772

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 435232

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   output_numel: 435708

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 435232

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 434772

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 435232

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   output_numel: 435708

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 435232

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 434772

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   intermediate_numel: 435232

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   output_numel: 435708

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   layer_numel: 5249532
05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   dense_numel: 219492
05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   emb_numel: 14525784

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   encoder_numel: 5249532

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   pooler_numel: 219492

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   all parameters: 19994808

05/28/2023 14:05:15 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:15 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
parameter size = 19994808
best_acc = 0.5667870036101083
time_per_batch_infer = 2.803 ms
infer_cnt = 63
**************E*************

05/28/2023 14:05:15 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
05/28/2023 14:05:15 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:05:15 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:05:15 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:15 - INFO - __main__ -   guid: train-0
05/28/2023 14:05:15 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:05:15 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:15 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:15 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:15 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:15 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:16 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:05:16 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:16 - INFO - __main__ -   guid: dev-0
05/28/2023 14:05:16 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:05:16 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:16 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:16 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:16 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:16 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:16 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:05:16 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:05:17 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:05:17 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:05:17 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:05:17 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:05:17 - INFO - __main__ -     Batch size = 32
05/28/2023 14:05:17 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.42it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.26it/s][A05/28/2023 14:05:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:17 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:05:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.00it/s]
05/28/2023 14:05:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:17 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:17 - INFO - __main__ -    dev: eval_loss = 0.7070188456111484
05/28/2023 14:05:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:17 - INFO - __main__ -    dev: infer_time = 3.3899999999999997
05/28/2023 14:05:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:17 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:17 - INFO - __main__ -     cls_loss = 0.6932077672746446
05/28/2023 14:05:17 - INFO - __main__ -     eval_loss = 0.7070188456111484
05/28/2023 14:05:17 - INFO - __main__ -     global_step = 9
05/28/2023 14:05:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:17 - INFO - __main__ -     infer_time = 3.3899999999999997
05/28/2023 14:05:17 - INFO - __main__ -     loss = 0.6932077672746446
05/28/2023 14:05:17 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.72it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.57it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.77it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.21it/s][A05/28/2023 14:05:19 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:19 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:05:19 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:19 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.00it/s]
05/28/2023 14:05:19 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:19 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:19 - INFO - __main__ -    dev: eval_loss = 0.6923440098762512
05/28/2023 14:05:19 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:19 - INFO - __main__ -    dev: infer_time = 3.342222222222222
05/28/2023 14:05:19 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:19 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:19 - INFO - __main__ -     cls_loss = 0.698967528970618
05/28/2023 14:05:19 - INFO - __main__ -     eval_loss = 0.6923440098762512
05/28/2023 14:05:19 - INFO - __main__ -     global_step = 19
05/28/2023 14:05:19 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:19 - INFO - __main__ -     infer_time = 3.342222222222222
05/28/2023 14:05:19 - INFO - __main__ -     loss = 0.698967528970618
05/28/2023 14:05:19 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.41it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  6.00it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.86it/s][A05/28/2023 14:05:21 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:21 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:05:21 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:21 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.16it/s]
05/28/2023 14:05:21 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:21 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:21 - INFO - __main__ -    dev: eval_loss = 0.6957559188206991
05/28/2023 14:05:21 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:21 - INFO - __main__ -    dev: infer_time = 3.3425555555555557
05/28/2023 14:05:21 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:21 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:21 - INFO - __main__ -     cls_loss = 0.6988831717392494
05/28/2023 14:05:21 - INFO - __main__ -     eval_loss = 0.6957559188206991
05/28/2023 14:05:21 - INFO - __main__ -     global_step = 29
05/28/2023 14:05:21 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:21 - INFO - __main__ -     infer_time = 3.3425555555555557
05/28/2023 14:05:21 - INFO - __main__ -     loss = 0.6988831717392494

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.35it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.56it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.83it/s][A05/28/2023 14:05:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:22 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:05:22 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.06it/s]
05/28/2023 14:05:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:22 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:22 - INFO - __main__ -    dev: eval_loss = 0.6981754634115431
05/28/2023 14:05:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:22 - INFO - __main__ -    dev: infer_time = 3.3606666666666665
05/28/2023 14:05:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:22 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:22 - INFO - __main__ -     cls_loss = 0.6977853362376873
05/28/2023 14:05:22 - INFO - __main__ -     eval_loss = 0.6981754634115431
05/28/2023 14:05:22 - INFO - __main__ -     global_step = 39
05/28/2023 14:05:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:22 - INFO - __main__ -     infer_time = 3.3606666666666665
05/28/2023 14:05:22 - INFO - __main__ -     loss = 0.6977853362376873

Iteration:  50%|#####     | 39/78 [00:04<00:02, 14.56it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 16.29it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 18.34it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 20.19it/s][A05/28/2023 14:05:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:22 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:05:22 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.30it/s]
05/28/2023 14:05:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:22 - INFO - __main__ -    dev: acc = 0.5523465703971119
05/28/2023 14:05:22 - INFO - __main__ -    dev: eval_loss = 0.6924606362978617
05/28/2023 14:05:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:22 - INFO - __main__ -    dev: infer_time = 3.3548888888888886
05/28/2023 14:05:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:22 - INFO - __main__ -     acc = 0.5523465703971119
05/28/2023 14:05:22 - INFO - __main__ -     cls_loss = 0.6975731010339699
05/28/2023 14:05:22 - INFO - __main__ -     eval_loss = 0.6924606362978617
05/28/2023 14:05:22 - INFO - __main__ -     global_step = 49
05/28/2023 14:05:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:22 - INFO - __main__ -     infer_time = 3.3548888888888886
05/28/2023 14:05:22 - INFO - __main__ -     loss = 0.6975731010339699
05/28/2023 14:05:22 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  65%|######5   | 51/78 [00:06<00:05,  5.22it/s][A
Iteration:  69%|######9   | 54/78 [00:06<00:03,  6.86it/s][A
Iteration:  73%|#######3  | 57/78 [00:06<00:02,  8.79it/s][A05/28/2023 14:05:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:24 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:05:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.13it/s]
05/28/2023 14:05:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:24 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:24 - INFO - __main__ -    dev: eval_loss = 0.691069503625234
05/28/2023 14:05:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:24 - INFO - __main__ -    dev: infer_time = 3.3372222222222216
05/28/2023 14:05:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:24 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:24 - INFO - __main__ -     cls_loss = 0.6966015635910681
05/28/2023 14:05:24 - INFO - __main__ -     eval_loss = 0.691069503625234
05/28/2023 14:05:24 - INFO - __main__ -     global_step = 59
05/28/2023 14:05:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:24 - INFO - __main__ -     infer_time = 3.3372222222222216
05/28/2023 14:05:24 - INFO - __main__ -     loss = 0.6966015635910681

Iteration:  77%|#######6  | 60/78 [00:06<00:01, 10.21it/s][A
Iteration:  81%|########  | 63/78 [00:07<00:01, 12.44it/s][A
Iteration:  85%|########4 | 66/78 [00:07<00:00, 14.76it/s][A05/28/2023 14:05:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:24 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:05:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.31it/s]
05/28/2023 14:05:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:24 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:24 - INFO - __main__ -    dev: eval_loss = 0.6910513374540541
05/28/2023 14:05:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:24 - INFO - __main__ -    dev: infer_time = 3.3374444444444444
05/28/2023 14:05:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:24 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:24 - INFO - __main__ -     cls_loss = 0.6962053343869637
05/28/2023 14:05:24 - INFO - __main__ -     eval_loss = 0.6910513374540541
05/28/2023 14:05:24 - INFO - __main__ -     global_step = 69
05/28/2023 14:05:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:24 - INFO - __main__ -     infer_time = 3.3374444444444444
05/28/2023 14:05:24 - INFO - __main__ -     loss = 0.6962053343869637

Iteration:  88%|########8 | 69/78 [00:07<00:00, 15.25it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:00, 17.27it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00, 19.11it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.23it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.62s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.62s/it]
05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   w_emb: 7691544

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   p_emb: 129024

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   t_emb: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ln_emb: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 121440

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 121212

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   intermediate_numel: 121440

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   output_numel: 121716

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 121440

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 121212

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   intermediate_numel: 121440

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   output_numel: 121716

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 121440

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 121212

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   intermediate_numel: 121440

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   output_numel: 121716

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 121440

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 121212

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   intermediate_numel: 121440

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   output_numel: 121716

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   layer_numel: 1994736
05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   dense_numel: 63756
05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   emb_numel: 7821576

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   encoder_numel: 1994736

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   pooler_numel: 63756

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   all parameters: 9880068

05/28/2023 14:05:25 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:25 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
parameter size = 9880068
best_acc = 0.5523465703971119
time_per_batch_infer = 3.352 ms
infer_cnt = 63
**************E*************

05/28/2023 14:05:25 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_hidden_size': 204, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [204, 204, 204, 204, 204]}
05/28/2023 14:05:25 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:05:25 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:05:25 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:25 - INFO - __main__ -   guid: train-0
05/28/2023 14:05:25 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:05:25 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:25 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:25 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:25 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:25 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:26 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:05:26 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:26 - INFO - __main__ -   guid: dev-0
05/28/2023 14:05:26 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:05:26 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:26 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:26 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:26 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:26 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:27 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:05:27 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:05:27 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:05:27 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:05:27 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:05:27 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:05:27 - INFO - __main__ -     Batch size = 32
05/28/2023 14:05:27 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   3%|2         | 2/78 [00:00<00:03, 19.33it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 21.57it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 22.07it/s][A05/28/2023 14:05:28 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:28 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:05:28 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:28 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 158.56it/s]
05/28/2023 14:05:28 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:28 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:05:28 - INFO - __main__ -    dev: eval_loss = 0.6924209793408712
05/28/2023 14:05:28 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:28 - INFO - __main__ -    dev: infer_time = 4.106888888888889
05/28/2023 14:05:28 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:28 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:05:28 - INFO - __main__ -     cls_loss = 0.6945244802369012
05/28/2023 14:05:28 - INFO - __main__ -     eval_loss = 0.6924209793408712
05/28/2023 14:05:28 - INFO - __main__ -     global_step = 9
05/28/2023 14:05:28 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:28 - INFO - __main__ -     infer_time = 4.106888888888889
05/28/2023 14:05:28 - INFO - __main__ -     loss = 0.6945244802369012
05/28/2023 14:05:28 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:01<00:15,  4.28it/s][A
Iteration:  18%|#7        | 14/78 [00:01<00:10,  6.14it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:07,  8.24it/s][A05/28/2023 14:05:29 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:29 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:05:29 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:29 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 158.26it/s]
05/28/2023 14:05:29 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:29 - INFO - __main__ -    dev: acc = 0.5126353790613718
05/28/2023 14:05:29 - INFO - __main__ -    dev: eval_loss = 0.6923060946994357
05/28/2023 14:05:29 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:29 - INFO - __main__ -    dev: infer_time = 4.092666666666667
05/28/2023 14:05:29 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:29 - INFO - __main__ -     acc = 0.5126353790613718
05/28/2023 14:05:29 - INFO - __main__ -     cls_loss = 0.6948320897001969
05/28/2023 14:05:29 - INFO - __main__ -     eval_loss = 0.6923060946994357
05/28/2023 14:05:29 - INFO - __main__ -     global_step = 19
05/28/2023 14:05:29 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:29 - INFO - __main__ -     infer_time = 4.092666666666667
05/28/2023 14:05:29 - INFO - __main__ -     loss = 0.6948320897001969

Iteration:  24%|##4       | 19/78 [00:02<00:06,  9.08it/s][A
Iteration:  28%|##8       | 22/78 [00:02<00:04, 11.42it/s][A
Iteration:  32%|###2      | 25/78 [00:02<00:03, 13.73it/s][A
Iteration:  36%|###5      | 28/78 [00:02<00:03, 15.76it/s][A05/28/2023 14:05:30 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:30 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:05:30 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:30 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 158.66it/s]
05/28/2023 14:05:30 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:30 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:05:30 - INFO - __main__ -    dev: eval_loss = 0.6929086645444235
05/28/2023 14:05:30 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:30 - INFO - __main__ -    dev: infer_time = 4.116777777777777
05/28/2023 14:05:30 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:30 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:05:30 - INFO - __main__ -     cls_loss = 0.6955232969645796
05/28/2023 14:05:30 - INFO - __main__ -     eval_loss = 0.6929086645444235
05/28/2023 14:05:30 - INFO - __main__ -     global_step = 29
05/28/2023 14:05:30 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:30 - INFO - __main__ -     infer_time = 4.116777777777777
05/28/2023 14:05:30 - INFO - __main__ -     loss = 0.6955232969645796
05/28/2023 14:05:30 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  40%|###9      | 31/78 [00:04<00:09,  4.97it/s][A
Iteration:  44%|####3     | 34/78 [00:04<00:06,  6.57it/s][A
Iteration:  47%|####7     | 37/78 [00:04<00:04,  8.44it/s][A05/28/2023 14:05:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:32 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:05:32 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 158.25it/s]
05/28/2023 14:05:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:32 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:32 - INFO - __main__ -    dev: eval_loss = 0.6952902608447604
05/28/2023 14:05:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:32 - INFO - __main__ -    dev: infer_time = 4.088666666666666
05/28/2023 14:05:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:32 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:32 - INFO - __main__ -     cls_loss = 0.6946754730664767
05/28/2023 14:05:32 - INFO - __main__ -     eval_loss = 0.6952902608447604
05/28/2023 14:05:32 - INFO - __main__ -     global_step = 39
05/28/2023 14:05:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:32 - INFO - __main__ -     infer_time = 4.088666666666666
05/28/2023 14:05:32 - INFO - __main__ -     loss = 0.6946754730664767

Iteration:  50%|#####     | 39/78 [00:04<00:04,  9.19it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:03, 11.32it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:02, 13.50it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 15.29it/s][A05/28/2023 14:05:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:32 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:05:32 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 158.23it/s]
05/28/2023 14:05:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:32 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:32 - INFO - __main__ -    dev: eval_loss = 0.6951584617296854
05/28/2023 14:05:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:32 - INFO - __main__ -    dev: infer_time = 4.1225555555555555
05/28/2023 14:05:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:32 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:32 - INFO - __main__ -     cls_loss = 0.6943518263953072
05/28/2023 14:05:32 - INFO - __main__ -     eval_loss = 0.6951584617296854
05/28/2023 14:05:32 - INFO - __main__ -     global_step = 49
05/28/2023 14:05:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:32 - INFO - __main__ -     infer_time = 4.1225555555555555
05/28/2023 14:05:32 - INFO - __main__ -     loss = 0.6943518263953072

Iteration:  65%|######5   | 51/78 [00:05<00:01, 15.21it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 16.94it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 18.32it/s][A05/28/2023 14:05:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:33 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:05:33 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 158.44it/s]
05/28/2023 14:05:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:33 - INFO - __main__ -    dev: acc = 0.4584837545126354
05/28/2023 14:05:33 - INFO - __main__ -    dev: eval_loss = 0.6940811475118002
05/28/2023 14:05:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:33 - INFO - __main__ -    dev: infer_time = 4.118222222222221
05/28/2023 14:05:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:33 - INFO - __main__ -     acc = 0.4584837545126354
05/28/2023 14:05:33 - INFO - __main__ -     cls_loss = 0.6938065502603176
05/28/2023 14:05:33 - INFO - __main__ -     eval_loss = 0.6940811475118002
05/28/2023 14:05:33 - INFO - __main__ -     global_step = 59
05/28/2023 14:05:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:33 - INFO - __main__ -     infer_time = 4.118222222222221
05/28/2023 14:05:33 - INFO - __main__ -     loss = 0.6938065502603176

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 17.08it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 18.44it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 19.50it/s][A05/28/2023 14:05:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:33 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:05:33 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 158.61it/s]
05/28/2023 14:05:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:33 - INFO - __main__ -    dev: acc = 0.4693140794223827
05/28/2023 14:05:33 - INFO - __main__ -    dev: eval_loss = 0.6935579445627
05/28/2023 14:05:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:33 - INFO - __main__ -    dev: infer_time = 4.088222222222222
05/28/2023 14:05:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:33 - INFO - __main__ -     acc = 0.4693140794223827
05/28/2023 14:05:33 - INFO - __main__ -     cls_loss = 0.6938060763953389
05/28/2023 14:05:33 - INFO - __main__ -     eval_loss = 0.6935579445627
05/28/2023 14:05:33 - INFO - __main__ -     global_step = 69
05/28/2023 14:05:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:33 - INFO - __main__ -     infer_time = 4.088222222222222
05/28/2023 14:05:33 - INFO - __main__ -     loss = 0.6938060763953389

Iteration:  88%|########8 | 69/78 [00:06<00:00, 18.05it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 19.00it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 19.90it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.03it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.48s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.48s/it]
05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   w_emb: 6226488

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   p_emb: 104448

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   t_emb: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_emb: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   query_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   key_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   value_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   self_numel: 125460

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 42228

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 78540

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ln_numel: 408

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   attention_numel: 167688

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   intermediate_numel: 78720

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   output_numel: 78948

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   layer_numel: 1626780
05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   dense_numel: 41820
05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   emb_numel: 6331752

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   encoder_numel: 1626780

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   pooler_numel: 41820

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   all parameters: 8000352

05/28/2023 14:05:34 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:34 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_hidden_size': 204, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [204, 204, 204, 204, 204]}
parameter size = 8000352
best_acc = 0.5306859205776173
time_per_batch_infer = 4.105 ms
infer_cnt = 63
**************E*************

05/28/2023 14:05:34 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [736, 736, 736, 736], 'sample_qkv_sizes': [252, 252, 252, 252]}
05/28/2023 14:05:34 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:05:34 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:05:34 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:34 - INFO - __main__ -   guid: train-0
05/28/2023 14:05:34 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:05:34 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:34 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:34 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:34 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:34 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:35 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:05:35 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:35 - INFO - __main__ -   guid: dev-0
05/28/2023 14:05:35 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:05:35 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:35 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:35 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:35 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:35 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:36 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:05:36 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:05:36 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:05:36 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:05:36 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:05:36 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:05:36 - INFO - __main__ -     Batch size = 32
05/28/2023 14:05:36 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 20.94it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.28it/s][A05/28/2023 14:05:37 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:37 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:05:37 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:37 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.71it/s]
05/28/2023 14:05:37 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:37 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:37 - INFO - __main__ -    dev: eval_loss = 0.6914676957660251
05/28/2023 14:05:37 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:37 - INFO - __main__ -    dev: infer_time = 3.3485555555555555
05/28/2023 14:05:37 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:37 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:37 - INFO - __main__ -     cls_loss = 0.6952637434005737
05/28/2023 14:05:37 - INFO - __main__ -     eval_loss = 0.6914676957660251
05/28/2023 14:05:37 - INFO - __main__ -     global_step = 9
05/28/2023 14:05:37 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:37 - INFO - __main__ -     infer_time = 3.3485555555555555
05/28/2023 14:05:37 - INFO - __main__ -     loss = 0.6952637434005737
05/28/2023 14:05:37 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.98it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.90it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.12it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.53it/s][A05/28/2023 14:05:38 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:38 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:05:38 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:38 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.65it/s]
05/28/2023 14:05:38 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:38 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:38 - INFO - __main__ -    dev: eval_loss = 0.6909414264890883
05/28/2023 14:05:38 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:38 - INFO - __main__ -    dev: infer_time = 3.3553333333333337
05/28/2023 14:05:38 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:38 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:38 - INFO - __main__ -     cls_loss = 0.6967528807489496
05/28/2023 14:05:38 - INFO - __main__ -     eval_loss = 0.6909414264890883
05/28/2023 14:05:38 - INFO - __main__ -     global_step = 19
05/28/2023 14:05:38 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:38 - INFO - __main__ -     infer_time = 3.3553333333333337
05/28/2023 14:05:38 - INFO - __main__ -     loss = 0.6967528807489496

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.64it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.93it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.18it/s][A05/28/2023 14:05:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:39 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:05:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.82it/s]
05/28/2023 14:05:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:39 - INFO - __main__ -    dev: acc = 0.5523465703971119
05/28/2023 14:05:39 - INFO - __main__ -    dev: eval_loss = 0.6924547685517205
05/28/2023 14:05:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:39 - INFO - __main__ -    dev: infer_time = 3.354666666666667
05/28/2023 14:05:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:39 - INFO - __main__ -     acc = 0.5523465703971119
05/28/2023 14:05:39 - INFO - __main__ -     cls_loss = 0.6965564324938017
05/28/2023 14:05:39 - INFO - __main__ -     eval_loss = 0.6924547685517205
05/28/2023 14:05:39 - INFO - __main__ -     global_step = 29
05/28/2023 14:05:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:39 - INFO - __main__ -     infer_time = 3.354666666666667
05/28/2023 14:05:39 - INFO - __main__ -     loss = 0.6965564324938017
05/28/2023 14:05:39 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:09,  5.03it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:06,  6.66it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:04,  8.58it/s][A05/28/2023 14:05:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:41 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:05:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.15it/s]
05/28/2023 14:05:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:41 - INFO - __main__ -    dev: acc = 0.5487364620938628
05/28/2023 14:05:41 - INFO - __main__ -    dev: eval_loss = 0.692281636926863
05/28/2023 14:05:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:41 - INFO - __main__ -    dev: infer_time = 3.3857777777777773
05/28/2023 14:05:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:41 - INFO - __main__ -     acc = 0.5487364620938628
05/28/2023 14:05:41 - INFO - __main__ -     cls_loss = 0.6958267933283097
05/28/2023 14:05:41 - INFO - __main__ -     eval_loss = 0.692281636926863
05/28/2023 14:05:41 - INFO - __main__ -     global_step = 39
05/28/2023 14:05:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:41 - INFO - __main__ -     infer_time = 3.3857777777777773
05/28/2023 14:05:41 - INFO - __main__ -     loss = 0.6958267933283097

Iteration:  50%|#####     | 39/78 [00:04<00:03,  9.97it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 12.12it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:02, 14.34it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 16.42it/s][A05/28/2023 14:05:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:41 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:05:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.38it/s]
05/28/2023 14:05:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:41 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:41 - INFO - __main__ -    dev: eval_loss = 0.695633053779602
05/28/2023 14:05:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:41 - INFO - __main__ -    dev: infer_time = 3.3870000000000005
05/28/2023 14:05:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:41 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:41 - INFO - __main__ -     cls_loss = 0.6948806704307089
05/28/2023 14:05:41 - INFO - __main__ -     eval_loss = 0.695633053779602
05/28/2023 14:05:41 - INFO - __main__ -     global_step = 49
05/28/2023 14:05:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:41 - INFO - __main__ -     infer_time = 3.3870000000000005
05/28/2023 14:05:41 - INFO - __main__ -     loss = 0.6948806704307089

Iteration:  65%|######5   | 51/78 [00:05<00:01, 16.21it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 18.08it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 19.38it/s][A05/28/2023 14:05:42 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:42 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:05:42 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:42 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.62it/s]
05/28/2023 14:05:42 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:42 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:42 - INFO - __main__ -    dev: eval_loss = 0.6945244802369012
05/28/2023 14:05:42 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:42 - INFO - __main__ -    dev: infer_time = 3.379666666666667
05/28/2023 14:05:42 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:42 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:42 - INFO - __main__ -     cls_loss = 0.694826519085189
05/28/2023 14:05:42 - INFO - __main__ -     eval_loss = 0.6945244802369012
05/28/2023 14:05:42 - INFO - __main__ -     global_step = 59
05/28/2023 14:05:42 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:42 - INFO - __main__ -     infer_time = 3.379666666666667
05/28/2023 14:05:42 - INFO - __main__ -     loss = 0.694826519085189

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 17.69it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 19.38it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 20.78it/s][A05/28/2023 14:05:42 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:42 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:05:42 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:42 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.64it/s]
05/28/2023 14:05:42 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:42 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:05:42 - INFO - __main__ -    dev: eval_loss = 0.6936532788806491
05/28/2023 14:05:42 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:42 - INFO - __main__ -    dev: infer_time = 3.4174444444444436
05/28/2023 14:05:42 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:42 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:05:42 - INFO - __main__ -     cls_loss = 0.6943254747252533
05/28/2023 14:05:42 - INFO - __main__ -     eval_loss = 0.6936532788806491
05/28/2023 14:05:42 - INFO - __main__ -     global_step = 69
05/28/2023 14:05:42 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:42 - INFO - __main__ -     infer_time = 3.4174444444444436
05/28/2023 14:05:42 - INFO - __main__ -     loss = 0.6943254747252533

Iteration:  88%|########8 | 69/78 [00:05<00:00, 18.92it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 19.85it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 20.84it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.36it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.31s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.31s/it]
05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   w_emb: 7691544

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   p_emb: 129024

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   t_emb: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ln_emb: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 186208

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 185724

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 186208

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   output_numel: 186228

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 186208

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 185724

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 186208

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   output_numel: 186228

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 186208

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 185724

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 186208

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   output_numel: 186228

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 186208

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 185724

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   intermediate_numel: 186208

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   output_numel: 186228

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   layer_numel: 2511856
05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   dense_numel: 63756
05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   emb_numel: 7821576

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   encoder_numel: 2511856

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   pooler_numel: 63756

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   all parameters: 10397188

05/28/2023 14:05:42 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:42 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 252, 'sample_intermediate_sizes': [736, 736, 736, 736], 'sample_qkv_sizes': [252, 252, 252, 252]}
parameter size = 10397188
best_acc = 0.5523465703971119
time_per_batch_infer = 3.375 ms
infer_cnt = 63
**************E*************

05/28/2023 14:05:43 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 312, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [312, 312, 312, 312]}
05/28/2023 14:05:43 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:05:43 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:05:43 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:43 - INFO - __main__ -   guid: train-0
05/28/2023 14:05:43 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:05:43 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:43 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:43 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:43 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:43 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:44 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:05:44 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:44 - INFO - __main__ -   guid: dev-0
05/28/2023 14:05:44 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:05:44 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:44 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:44 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:44 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:44 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:44 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:05:44 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:05:45 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:05:45 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:05:45 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:05:45 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:05:45 - INFO - __main__ -     Batch size = 32
05/28/2023 14:05:45 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.42it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.44it/s][A05/28/2023 14:05:45 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:45 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:05:45 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:45 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.36it/s]
05/28/2023 14:05:45 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:45 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:45 - INFO - __main__ -    dev: eval_loss = 0.7076744768354628
05/28/2023 14:05:45 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:45 - INFO - __main__ -    dev: infer_time = 3.3801111111111113
05/28/2023 14:05:45 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:45 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:45 - INFO - __main__ -     cls_loss = 0.6947745945718553
05/28/2023 14:05:45 - INFO - __main__ -     eval_loss = 0.7076744768354628
05/28/2023 14:05:45 - INFO - __main__ -     global_step = 9
05/28/2023 14:05:45 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:45 - INFO - __main__ -     infer_time = 3.3801111111111113
05/28/2023 14:05:45 - INFO - __main__ -     loss = 0.6947745945718553
05/28/2023 14:05:45 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.96it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.85it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.00it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.28it/s][A05/28/2023 14:05:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:47 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:05:47 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.25it/s]
05/28/2023 14:05:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:47 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:47 - INFO - __main__ -    dev: eval_loss = 0.6918536027272543
05/28/2023 14:05:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:47 - INFO - __main__ -    dev: infer_time = 3.409777777777778
05/28/2023 14:05:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:47 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:47 - INFO - __main__ -     cls_loss = 0.6946893993176912
05/28/2023 14:05:47 - INFO - __main__ -     eval_loss = 0.6918536027272543
05/28/2023 14:05:47 - INFO - __main__ -     global_step = 19
05/28/2023 14:05:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:47 - INFO - __main__ -     infer_time = 3.409777777777778
05/28/2023 14:05:47 - INFO - __main__ -     loss = 0.6946893993176912
05/28/2023 14:05:47 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.22it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.70it/s][A
Iteration:  35%|###4      | 27/78 [00:04<00:06,  7.44it/s][A05/28/2023 14:05:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:49 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:05:49 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.28it/s]
05/28/2023 14:05:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:49 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:49 - INFO - __main__ -    dev: eval_loss = 0.6924654973877801
05/28/2023 14:05:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:49 - INFO - __main__ -    dev: infer_time = 3.5715555555555554
05/28/2023 14:05:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:49 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:49 - INFO - __main__ -     cls_loss = 0.6933328612097378
05/28/2023 14:05:49 - INFO - __main__ -     eval_loss = 0.6924654973877801
05/28/2023 14:05:49 - INFO - __main__ -     global_step = 29
05/28/2023 14:05:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:49 - INFO - __main__ -     infer_time = 3.5715555555555554
05/28/2023 14:05:49 - INFO - __main__ -     loss = 0.6933328612097378

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.18it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.38it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.60it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.69it/s][A05/28/2023 14:05:50 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:50 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:05:50 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:50 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.34it/s]
05/28/2023 14:05:50 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:50 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:05:50 - INFO - __main__ -    dev: eval_loss = 0.6919699907302856
05/28/2023 14:05:50 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:50 - INFO - __main__ -    dev: infer_time = 3.4157777777777785
05/28/2023 14:05:50 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:50 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:05:50 - INFO - __main__ -     cls_loss = 0.69389797326846
05/28/2023 14:05:50 - INFO - __main__ -     eval_loss = 0.6919699907302856
05/28/2023 14:05:50 - INFO - __main__ -     global_step = 39
05/28/2023 14:05:50 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:50 - INFO - __main__ -     infer_time = 3.4157777777777785
05/28/2023 14:05:50 - INFO - __main__ -     loss = 0.69389797326846

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.61it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.21it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.83it/s][A05/28/2023 14:05:50 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:50 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:05:50 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:50 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.24it/s]
05/28/2023 14:05:50 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:50 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:50 - INFO - __main__ -    dev: eval_loss = 0.6946654849582248
05/28/2023 14:05:50 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:50 - INFO - __main__ -    dev: infer_time = 3.4622222222222225
05/28/2023 14:05:50 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:50 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:50 - INFO - __main__ -     cls_loss = 0.6937031697253792
05/28/2023 14:05:50 - INFO - __main__ -     eval_loss = 0.6946654849582248
05/28/2023 14:05:50 - INFO - __main__ -     global_step = 49
05/28/2023 14:05:50 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:50 - INFO - __main__ -     infer_time = 3.4622222222222225
05/28/2023 14:05:50 - INFO - __main__ -     loss = 0.6937031697253792

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.56it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 18.10it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 19.35it/s][A05/28/2023 14:05:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:51 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:05:51 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.16it/s]
05/28/2023 14:05:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:51 - INFO - __main__ -    dev: acc = 0.48014440433212996
05/28/2023 14:05:51 - INFO - __main__ -    dev: eval_loss = 0.6938057276937697
05/28/2023 14:05:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:51 - INFO - __main__ -    dev: infer_time = 3.428777777777778
05/28/2023 14:05:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:51 - INFO - __main__ -     acc = 0.48014440433212996
05/28/2023 14:05:51 - INFO - __main__ -     cls_loss = 0.6934139071884802
05/28/2023 14:05:51 - INFO - __main__ -     eval_loss = 0.6938057276937697
05/28/2023 14:05:51 - INFO - __main__ -     global_step = 59
05/28/2023 14:05:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:51 - INFO - __main__ -     infer_time = 3.428777777777778
05/28/2023 14:05:51 - INFO - __main__ -     loss = 0.6934139071884802

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 17.56it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:00, 18.88it/s][A
Iteration:  83%|########3 | 65/78 [00:05<00:00, 19.96it/s][A
Iteration:  87%|########7 | 68/78 [00:06<00:00, 20.82it/s][A05/28/2023 14:05:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:51 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:05:51 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.25it/s]
05/28/2023 14:05:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:51 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:51 - INFO - __main__ -    dev: eval_loss = 0.696217828326755
05/28/2023 14:05:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:51 - INFO - __main__ -    dev: infer_time = 3.412888888888889
05/28/2023 14:05:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:51 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:51 - INFO - __main__ -     cls_loss = 0.6934768868529279
05/28/2023 14:05:51 - INFO - __main__ -     eval_loss = 0.696217828326755
05/28/2023 14:05:51 - INFO - __main__ -     global_step = 69
05/28/2023 14:05:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:51 - INFO - __main__ -     infer_time = 3.412888888888889
05/28/2023 14:05:51 - INFO - __main__ -     loss = 0.6934768868529279

Iteration:  91%|#########1| 71/78 [00:06<00:00, 18.23it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 19.39it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 20.42it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.86it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.58s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.58s/it]
05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   w_emb: 9522864

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   p_emb: 159744

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   t_emb: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ln_emb: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   query_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   key_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   value_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ln_numel: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   self_numel: 292968

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   output_numel: 98280

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 180288

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 180024

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ln_numel: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   attention_numel: 391248

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   intermediate_numel: 180288

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   output_numel: 180648

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   query_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   key_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   value_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ln_numel: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   self_numel: 292968

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   output_numel: 98280

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 180288

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 180024

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ln_numel: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   attention_numel: 391248

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   intermediate_numel: 180288

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   output_numel: 180648

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   query_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   key_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   value_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ln_numel: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   self_numel: 292968

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   output_numel: 98280

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 180288

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 180024

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ln_numel: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   attention_numel: 391248

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   intermediate_numel: 180288

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   output_numel: 180648

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   query_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   key_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   value_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ln_numel: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   self_numel: 292968

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   output_numel: 98280

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 180288

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 180024

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ln_numel: 624

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   attention_numel: 391248

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   intermediate_numel: 180288

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   output_numel: 180648

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   layer_numel: 3008736
05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   dense_numel: 97656
05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   emb_numel: 9683856

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   encoder_numel: 3008736

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   pooler_numel: 97656

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   all parameters: 12790248

05/28/2023 14:05:52 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:05:52 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 312, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [312, 312, 312, 312]}
parameter size = 12790248
best_acc = 0.5270758122743683
time_per_batch_infer = 3.440 ms
infer_cnt = 63
**************E*************

05/28/2023 14:05:52 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [576, 576, 576], 'sample_qkv_sizes': [456, 456, 456]}
05/28/2023 14:05:52 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:05:52 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:05:52 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:52 - INFO - __main__ -   guid: train-0
05/28/2023 14:05:52 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:05:52 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:52 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:52 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:52 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:52 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:53 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:05:53 - INFO - __main__ -   *** Example ***
05/28/2023 14:05:53 - INFO - __main__ -   guid: dev-0
05/28/2023 14:05:53 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:05:53 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:53 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:53 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:05:53 - INFO - __main__ -   label: not_entailment
05/28/2023 14:05:53 - INFO - __main__ -   label_id: 1
05/28/2023 14:05:54 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:05:54 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:05:54 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:05:54 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:05:54 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:05:54 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:05:54 - INFO - __main__ -     Batch size = 32
05/28/2023 14:05:54 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.44it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.11it/s][A05/28/2023 14:05:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:54 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:05:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.79it/s]
05/28/2023 14:05:55 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:55 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:55 - INFO - __main__ -    dev: eval_loss = 0.6973018182648553
05/28/2023 14:05:55 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:55 - INFO - __main__ -    dev: infer_time = 2.782222222222222
05/28/2023 14:05:55 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:55 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:55 - INFO - __main__ -     cls_loss = 0.6947334474987454
05/28/2023 14:05:55 - INFO - __main__ -     eval_loss = 0.6973018182648553
05/28/2023 14:05:55 - INFO - __main__ -     global_step = 9
05/28/2023 14:05:55 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:55 - INFO - __main__ -     infer_time = 2.782222222222222
05/28/2023 14:05:55 - INFO - __main__ -     loss = 0.6947334474987454
05/28/2023 14:05:55 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.00it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.94it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.18it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.60it/s][A05/28/2023 14:05:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:56 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:05:56 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.24it/s]
05/28/2023 14:05:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:56 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:05:56 - INFO - __main__ -    dev: eval_loss = 0.6912145945760939
05/28/2023 14:05:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:56 - INFO - __main__ -    dev: infer_time = 2.8233333333333333
05/28/2023 14:05:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:56 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:05:56 - INFO - __main__ -     cls_loss = 0.6932569522606699
05/28/2023 14:05:56 - INFO - __main__ -     eval_loss = 0.6912145945760939
05/28/2023 14:05:56 - INFO - __main__ -     global_step = 19
05/28/2023 14:05:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:56 - INFO - __main__ -     infer_time = 2.8233333333333333
05/28/2023 14:05:56 - INFO - __main__ -     loss = 0.6932569522606699
05/28/2023 14:05:56 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.34it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.88it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.71it/s][A05/28/2023 14:05:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:58 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:05:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.46it/s]
05/28/2023 14:05:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:58 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:58 - INFO - __main__ -    dev: eval_loss = 0.7003261976771884
05/28/2023 14:05:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:58 - INFO - __main__ -    dev: infer_time = 2.8384444444444448
05/28/2023 14:05:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:58 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:58 - INFO - __main__ -     cls_loss = 0.6934268741772093
05/28/2023 14:05:58 - INFO - __main__ -     eval_loss = 0.7003261976771884
05/28/2023 14:05:58 - INFO - __main__ -     global_step = 29
05/28/2023 14:05:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:58 - INFO - __main__ -     infer_time = 2.8384444444444448
05/28/2023 14:05:58 - INFO - __main__ -     loss = 0.6934268741772093

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.07it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:04, 11.24it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.47it/s][A05/28/2023 14:05:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:59 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:05:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.71it/s]
05/28/2023 14:05:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:59 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:05:59 - INFO - __main__ -    dev: eval_loss = 0.6947547727160983
05/28/2023 14:05:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:59 - INFO - __main__ -    dev: infer_time = 2.832666666666667
05/28/2023 14:05:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:59 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:05:59 - INFO - __main__ -     cls_loss = 0.6934874073053018
05/28/2023 14:05:59 - INFO - __main__ -     eval_loss = 0.6947547727160983
05/28/2023 14:05:59 - INFO - __main__ -     global_step = 39
05/28/2023 14:05:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:59 - INFO - __main__ -     infer_time = 2.832666666666667
05/28/2023 14:05:59 - INFO - __main__ -     loss = 0.6934874073053018

Iteration:  50%|#####     | 39/78 [00:04<00:02, 13.92it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 16.00it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 17.88it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 19.51it/s][A05/28/2023 14:05:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:05:59 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:05:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:05:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.72it/s]
05/28/2023 14:05:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:05:59 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:05:59 - INFO - __main__ -    dev: eval_loss = 0.6897061798307631
05/28/2023 14:05:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:05:59 - INFO - __main__ -    dev: infer_time = 2.821555555555556
05/28/2023 14:05:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:05:59 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:05:59 - INFO - __main__ -     cls_loss = 0.6936103215022963
05/28/2023 14:05:59 - INFO - __main__ -     eval_loss = 0.6897061798307631
05/28/2023 14:05:59 - INFO - __main__ -     global_step = 49
05/28/2023 14:05:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:05:59 - INFO - __main__ -     infer_time = 2.821555555555556
05/28/2023 14:05:59 - INFO - __main__ -     loss = 0.6936103215022963

Iteration:  65%|######5   | 51/78 [00:05<00:01, 17.52it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 19.21it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 20.52it/s][A05/28/2023 14:06:00 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:00 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:06:00 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:00 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.69it/s]
05/28/2023 14:06:00 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:00 - INFO - __main__ -    dev: acc = 0.5487364620938628
05/28/2023 14:06:00 - INFO - __main__ -    dev: eval_loss = 0.6908762786123488
05/28/2023 14:06:00 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:00 - INFO - __main__ -    dev: infer_time = 2.810666666666667
05/28/2023 14:06:00 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:00 - INFO - __main__ -     acc = 0.5487364620938628
05/28/2023 14:06:00 - INFO - __main__ -     cls_loss = 0.6937177342883611
05/28/2023 14:06:00 - INFO - __main__ -     eval_loss = 0.6908762786123488
05/28/2023 14:06:00 - INFO - __main__ -     global_step = 59
05/28/2023 14:06:00 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:00 - INFO - __main__ -     infer_time = 2.810666666666667
05/28/2023 14:06:00 - INFO - __main__ -     loss = 0.6937177342883611
05/28/2023 14:06:00 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  77%|#######6  | 60/78 [00:06<00:03,  5.40it/s][A
Iteration:  81%|########  | 63/78 [00:06<00:02,  7.02it/s][A
Iteration:  85%|########4 | 66/78 [00:07<00:01,  8.93it/s][A05/28/2023 14:06:01 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:01 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:06:01 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:01 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.74it/s]
05/28/2023 14:06:01 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:01 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:01 - INFO - __main__ -    dev: eval_loss = 0.6955641110738119
05/28/2023 14:06:01 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:01 - INFO - __main__ -    dev: infer_time = 2.820222222222222
05/28/2023 14:06:01 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:01 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:01 - INFO - __main__ -     cls_loss = 0.6934581649476204
05/28/2023 14:06:01 - INFO - __main__ -     eval_loss = 0.6955641110738119
05/28/2023 14:06:01 - INFO - __main__ -     global_step = 69
05/28/2023 14:06:01 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:01 - INFO - __main__ -     infer_time = 2.820222222222222
05/28/2023 14:06:01 - INFO - __main__ -     loss = 0.6934581649476204

Iteration:  88%|########8 | 69/78 [00:07<00:00, 10.17it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:00, 12.31it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00, 14.50it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.20it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.65s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.65s/it]
05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   w_emb: 13918032

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   p_emb: 233472

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   t_emb: 912

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   ln_emb: 912

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 263232

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 263112

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   intermediate_numel: 263232

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   output_numel: 264024

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 263232

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 263112

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   intermediate_numel: 263232

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   output_numel: 264024

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 263232

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 263112

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   intermediate_numel: 263232

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   output_numel: 264024

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   layer_numel: 4085208
05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   dense_numel: 208392
05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   emb_numel: 14153328

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   encoder_numel: 4085208

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   pooler_numel: 208392

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   all parameters: 18446928

05/28/2023 14:06:02 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:06:02 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [576, 576, 576], 'sample_qkv_sizes': [456, 456, 456]}
parameter size = 18446928
best_acc = 0.5487364620938628
time_per_batch_infer = 2.818 ms
infer_cnt = 63
**************E*************

05/28/2023 14:06:02 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
05/28/2023 14:06:02 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:06:02 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:06:02 - INFO - __main__ -   *** Example ***
05/28/2023 14:06:02 - INFO - __main__ -   guid: train-0
05/28/2023 14:06:02 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:06:02 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:02 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:02 - INFO - __main__ -   label: not_entailment
05/28/2023 14:06:02 - INFO - __main__ -   label_id: 1
05/28/2023 14:06:04 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:06:04 - INFO - __main__ -   *** Example ***
05/28/2023 14:06:04 - INFO - __main__ -   guid: dev-0
05/28/2023 14:06:04 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:06:04 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:04 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:04 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:04 - INFO - __main__ -   label: not_entailment
05/28/2023 14:06:04 - INFO - __main__ -   label_id: 1
05/28/2023 14:06:04 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:06:04 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:06:04 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:06:04 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:06:04 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:06:04 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:06:04 - INFO - __main__ -     Batch size = 32
05/28/2023 14:06:04 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.92it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.07it/s][A05/28/2023 14:06:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:05 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:06:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 166.42it/s]
05/28/2023 14:06:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:05 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:06:05 - INFO - __main__ -    dev: eval_loss = 0.6922283503744338
05/28/2023 14:06:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:05 - INFO - __main__ -    dev: infer_time = 3.4244444444444446
05/28/2023 14:06:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:05 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:06:05 - INFO - __main__ -     cls_loss = 0.6935166120529175
05/28/2023 14:06:05 - INFO - __main__ -     eval_loss = 0.6922283503744338
05/28/2023 14:06:05 - INFO - __main__ -     global_step = 9
05/28/2023 14:06:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:05 - INFO - __main__ -     infer_time = 3.4244444444444446
05/28/2023 14:06:05 - INFO - __main__ -     loss = 0.6935166120529175
05/28/2023 14:06:05 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.03it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.97it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.27it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.79it/s][A05/28/2023 14:06:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:06 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:06:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.21it/s]
05/28/2023 14:06:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:06 - INFO - __main__ -    dev: acc = 0.44765342960288806
05/28/2023 14:06:06 - INFO - __main__ -    dev: eval_loss = 0.6932310660680135
05/28/2023 14:06:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:06 - INFO - __main__ -    dev: infer_time = 3.536777777777778
05/28/2023 14:06:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:07 - INFO - __main__ -     acc = 0.44765342960288806
05/28/2023 14:06:07 - INFO - __main__ -     cls_loss = 0.6960958493383307
05/28/2023 14:06:07 - INFO - __main__ -     eval_loss = 0.6932310660680135
05/28/2023 14:06:07 - INFO - __main__ -     global_step = 19
05/28/2023 14:06:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:07 - INFO - __main__ -     infer_time = 3.536777777777778
05/28/2023 14:06:07 - INFO - __main__ -     loss = 0.6960958493383307

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.96it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.44it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.71it/s][A05/28/2023 14:06:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:07 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:06:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.81it/s]
05/28/2023 14:06:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:07 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:07 - INFO - __main__ -    dev: eval_loss = 0.6958741280767653
05/28/2023 14:06:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:07 - INFO - __main__ -    dev: infer_time = 3.4168888888888893
05/28/2023 14:06:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:07 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:07 - INFO - __main__ -     cls_loss = 0.6957291200243193
05/28/2023 14:06:07 - INFO - __main__ -     eval_loss = 0.6958741280767653
05/28/2023 14:06:07 - INFO - __main__ -     global_step = 29
05/28/2023 14:06:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:07 - INFO - __main__ -     infer_time = 3.4168888888888893
05/28/2023 14:06:07 - INFO - __main__ -     loss = 0.6957291200243193

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.56it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 18.53it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 20.30it/s][A05/28/2023 14:06:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:07 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:06:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 166.51it/s]
05/28/2023 14:06:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:07 - INFO - __main__ -    dev: acc = 0.5667870036101083
05/28/2023 14:06:07 - INFO - __main__ -    dev: eval_loss = 0.6915292408731248
05/28/2023 14:06:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:07 - INFO - __main__ -    dev: infer_time = 3.4116666666666666
05/28/2023 14:06:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:07 - INFO - __main__ -     acc = 0.5667870036101083
05/28/2023 14:06:07 - INFO - __main__ -     cls_loss = 0.6948918272287418
05/28/2023 14:06:07 - INFO - __main__ -     eval_loss = 0.6915292408731248
05/28/2023 14:06:07 - INFO - __main__ -     global_step = 39
05/28/2023 14:06:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:07 - INFO - __main__ -     infer_time = 3.4116666666666666
05/28/2023 14:06:07 - INFO - __main__ -     loss = 0.6948918272287418
05/28/2023 14:06:07 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:04<00:07,  5.40it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:05,  7.03it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:03,  9.00it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:02, 11.22it/s][A05/28/2023 14:06:09 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:09 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:06:09 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:09 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 166.06it/s]
05/28/2023 14:06:09 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:09 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:06:09 - INFO - __main__ -    dev: eval_loss = 0.6907146705521477
05/28/2023 14:06:09 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:09 - INFO - __main__ -    dev: infer_time = 3.4393333333333334
05/28/2023 14:06:09 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:09 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:06:09 - INFO - __main__ -     cls_loss = 0.6946461553476295
05/28/2023 14:06:09 - INFO - __main__ -     eval_loss = 0.6907146705521477
05/28/2023 14:06:09 - INFO - __main__ -     global_step = 49
05/28/2023 14:06:09 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:09 - INFO - __main__ -     infer_time = 3.4393333333333334
05/28/2023 14:06:09 - INFO - __main__ -     loss = 0.6946461553476295

Iteration:  65%|######5   | 51/78 [00:04<00:02, 12.41it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 14.68it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:01, 16.93it/s][A05/28/2023 14:06:10 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:10 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:06:10 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:10 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 166.15it/s]
05/28/2023 14:06:10 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:10 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:10 - INFO - __main__ -    dev: eval_loss = 0.6929656333393521
05/28/2023 14:06:10 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:10 - INFO - __main__ -    dev: infer_time = 3.4143333333333334
05/28/2023 14:06:10 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:10 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:10 - INFO - __main__ -     cls_loss = 0.6943199351682501
05/28/2023 14:06:10 - INFO - __main__ -     eval_loss = 0.6929656333393521
05/28/2023 14:06:10 - INFO - __main__ -     global_step = 59
05/28/2023 14:06:10 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:10 - INFO - __main__ -     infer_time = 3.4143333333333334
05/28/2023 14:06:10 - INFO - __main__ -     loss = 0.6943199351682501

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 16.60it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 18.65it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 20.23it/s][A05/28/2023 14:06:10 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:10 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:06:10 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:10 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 166.08it/s]
05/28/2023 14:06:10 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:10 - INFO - __main__ -    dev: acc = 0.4620938628158845
05/28/2023 14:06:10 - INFO - __main__ -    dev: eval_loss = 0.6943775879012214
05/28/2023 14:06:10 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:10 - INFO - __main__ -    dev: infer_time = 3.432888888888889
05/28/2023 14:06:10 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:10 - INFO - __main__ -     acc = 0.4620938628158845
05/28/2023 14:06:10 - INFO - __main__ -     cls_loss = 0.6947056849797567
05/28/2023 14:06:10 - INFO - __main__ -     eval_loss = 0.6943775879012214
05/28/2023 14:06:10 - INFO - __main__ -     global_step = 69
05/28/2023 14:06:10 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:10 - INFO - __main__ -     infer_time = 3.432888888888889
05/28/2023 14:06:10 - INFO - __main__ -     loss = 0.6947056849797567

Iteration:  88%|########8 | 69/78 [00:05<00:00, 18.99it/s][A
Iteration:  92%|#########2| 72/78 [00:05<00:00, 20.42it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 21.88it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.73it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.13s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.13s/it]
05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   w_emb: 7691544

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   p_emb: 129024

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   t_emb: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ln_emb: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 113344

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 113148

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 113344

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   output_numel: 113652

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 113344

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 113148

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 113344

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   output_numel: 113652

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 113344

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 113148

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 113344

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   output_numel: 113652

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 113344

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 113148

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 113344

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   output_numel: 113652

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   layer_numel: 1930096
05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   dense_numel: 63756
05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   emb_numel: 7821576

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   encoder_numel: 1930096

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   pooler_numel: 63756

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   all parameters: 9815428

05/28/2023 14:06:10 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:06:10 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
parameter size = 9815428
best_acc = 0.5667870036101083
time_per_batch_infer = 3.439 ms
infer_cnt = 63
**************E*************

05/28/2023 14:06:10 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
05/28/2023 14:06:10 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:06:10 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:06:10 - INFO - __main__ -   *** Example ***
05/28/2023 14:06:10 - INFO - __main__ -   guid: train-0
05/28/2023 14:06:10 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:06:10 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:10 - INFO - __main__ -   label: not_entailment
05/28/2023 14:06:10 - INFO - __main__ -   label_id: 1
05/28/2023 14:06:12 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:06:12 - INFO - __main__ -   *** Example ***
05/28/2023 14:06:12 - INFO - __main__ -   guid: dev-0
05/28/2023 14:06:12 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:06:12 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:12 - INFO - __main__ -   label: not_entailment
05/28/2023 14:06:12 - INFO - __main__ -   label_id: 1
05/28/2023 14:06:12 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:06:12 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:06:13 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:06:13 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:06:13 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:06:13 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:06:13 - INFO - __main__ -     Batch size = 32
05/28/2023 14:06:13 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.74it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.48it/s][A05/28/2023 14:06:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:13 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:06:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.94it/s]
05/28/2023 14:06:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:13 - INFO - __main__ -    dev: acc = 0.555956678700361
05/28/2023 14:06:13 - INFO - __main__ -    dev: eval_loss = 0.6923905942175124
05/28/2023 14:06:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:13 - INFO - __main__ -    dev: infer_time = 3.382444444444444
05/28/2023 14:06:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:13 - INFO - __main__ -     acc = 0.555956678700361
05/28/2023 14:06:13 - INFO - __main__ -     cls_loss = 0.6964142984814115
05/28/2023 14:06:13 - INFO - __main__ -     eval_loss = 0.6923905942175124
05/28/2023 14:06:13 - INFO - __main__ -     global_step = 9
05/28/2023 14:06:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:13 - INFO - __main__ -     infer_time = 3.382444444444444
05/28/2023 14:06:13 - INFO - __main__ -     loss = 0.6964142984814115
05/28/2023 14:06:13 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.01it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.94it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.19it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.61it/s][A05/28/2023 14:06:15 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:15 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:06:15 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:15 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.30it/s]
05/28/2023 14:06:15 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:15 - INFO - __main__ -    dev: acc = 0.5054151624548736
05/28/2023 14:06:15 - INFO - __main__ -    dev: eval_loss = 0.6931297911538018
05/28/2023 14:06:15 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:15 - INFO - __main__ -    dev: infer_time = 3.4139999999999997
05/28/2023 14:06:15 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:15 - INFO - __main__ -     acc = 0.5054151624548736
05/28/2023 14:06:15 - INFO - __main__ -     cls_loss = 0.6974311690581473
05/28/2023 14:06:15 - INFO - __main__ -     eval_loss = 0.6931297911538018
05/28/2023 14:06:15 - INFO - __main__ -     global_step = 19
05/28/2023 14:06:15 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:15 - INFO - __main__ -     infer_time = 3.4139999999999997
05/28/2023 14:06:15 - INFO - __main__ -     loss = 0.6974311690581473

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.73it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.10it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.30it/s][A05/28/2023 14:06:16 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:16 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:06:16 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:16 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.98it/s]
05/28/2023 14:06:16 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:16 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:06:16 - INFO - __main__ -    dev: eval_loss = 0.6918250653478835
05/28/2023 14:06:16 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:16 - INFO - __main__ -    dev: infer_time = 3.3746666666666667
05/28/2023 14:06:16 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:16 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:06:16 - INFO - __main__ -     cls_loss = 0.6954607079769003
05/28/2023 14:06:16 - INFO - __main__ -     eval_loss = 0.6918250653478835
05/28/2023 14:06:16 - INFO - __main__ -     global_step = 29
05/28/2023 14:06:16 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:16 - INFO - __main__ -     infer_time = 3.3746666666666667
05/28/2023 14:06:16 - INFO - __main__ -     loss = 0.6954607079769003

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.13it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 18.02it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 19.66it/s][A05/28/2023 14:06:16 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:16 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:06:16 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:16 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 155.16it/s]
05/28/2023 14:06:16 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:16 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:06:16 - INFO - __main__ -    dev: eval_loss = 0.6915613346629672
05/28/2023 14:06:16 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:16 - INFO - __main__ -    dev: infer_time = 3.3840000000000003
05/28/2023 14:06:16 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:16 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:06:16 - INFO - __main__ -     cls_loss = 0.6956167694849845
05/28/2023 14:06:16 - INFO - __main__ -     eval_loss = 0.6915613346629672
05/28/2023 14:06:16 - INFO - __main__ -     global_step = 39
05/28/2023 14:06:16 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:16 - INFO - __main__ -     infer_time = 3.3840000000000003
05/28/2023 14:06:16 - INFO - __main__ -     loss = 0.6956167694849845

Iteration:  50%|#####     | 39/78 [00:03<00:02, 18.42it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 19.75it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 21.08it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 22.08it/s][A05/28/2023 14:06:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:17 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:06:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 155.08it/s]
05/28/2023 14:06:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:17 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:17 - INFO - __main__ -    dev: eval_loss = 0.6944606105486552
05/28/2023 14:06:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:17 - INFO - __main__ -    dev: infer_time = 3.382777777777777
05/28/2023 14:06:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:17 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:17 - INFO - __main__ -     cls_loss = 0.6955074850393801
05/28/2023 14:06:17 - INFO - __main__ -     eval_loss = 0.6944606105486552
05/28/2023 14:06:17 - INFO - __main__ -     global_step = 49
05/28/2023 14:06:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:17 - INFO - __main__ -     infer_time = 3.382777777777777
05/28/2023 14:06:17 - INFO - __main__ -     loss = 0.6955074850393801

Iteration:  65%|######5   | 51/78 [00:03<00:01, 19.63it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 20.93it/s][A
Iteration:  73%|#######3  | 57/78 [00:03<00:00, 21.97it/s][A05/28/2023 14:06:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:17 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:06:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 154.44it/s]
05/28/2023 14:06:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:17 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:17 - INFO - __main__ -    dev: eval_loss = 0.6937927868631151
05/28/2023 14:06:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:17 - INFO - __main__ -    dev: infer_time = 3.405777777777778
05/28/2023 14:06:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:17 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:17 - INFO - __main__ -     cls_loss = 0.6951004771863
05/28/2023 14:06:17 - INFO - __main__ -     eval_loss = 0.6937927868631151
05/28/2023 14:06:17 - INFO - __main__ -     global_step = 59
05/28/2023 14:06:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:17 - INFO - __main__ -     infer_time = 3.405777777777778
05/28/2023 14:06:17 - INFO - __main__ -     loss = 0.6951004771863

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 19.58it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 20.92it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 21.95it/s][A05/28/2023 14:06:18 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:18 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:06:18 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:18 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 155.15it/s]
05/28/2023 14:06:18 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:18 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:18 - INFO - __main__ -    dev: eval_loss = 0.6941285067134433
05/28/2023 14:06:18 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:18 - INFO - __main__ -    dev: infer_time = 3.3975555555555554
05/28/2023 14:06:18 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:18 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:18 - INFO - __main__ -     cls_loss = 0.694737668486609
05/28/2023 14:06:18 - INFO - __main__ -     eval_loss = 0.6941285067134433
05/28/2023 14:06:18 - INFO - __main__ -     global_step = 69
05/28/2023 14:06:18 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:18 - INFO - __main__ -     infer_time = 3.3975555555555554
05/28/2023 14:06:18 - INFO - __main__ -     loss = 0.694737668486609

Iteration:  88%|########8 | 69/78 [00:04<00:00, 19.80it/s][A
Iteration:  92%|#########2| 72/78 [00:04<00:00, 20.65it/s][A
Iteration:  96%|#########6| 75/78 [00:04<00:00, 21.52it/s][AIteration: 100%|##########| 78/78 [00:04<00:00, 15.79it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.94s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.94s/it]
05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   w_emb: 8424072

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   p_emb: 141312

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   t_emb: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ln_emb: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 141824

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 141588

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   intermediate_numel: 141824

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   output_numel: 142140

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 141824

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 141588

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   intermediate_numel: 141824

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   output_numel: 142140

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 141824

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 141588

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   intermediate_numel: 141824

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   output_numel: 142140

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 141824

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 141588

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   intermediate_numel: 141824

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   output_numel: 142140

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   layer_numel: 2361296
05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   dense_numel: 76452
05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   emb_numel: 8566488

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   encoder_numel: 2361296

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   pooler_numel: 76452

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   all parameters: 11004236

05/28/2023 14:06:18 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:06:18 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
parameter size = 11004236
best_acc = 0.555956678700361
time_per_batch_infer = 3.392 ms
infer_cnt = 63
**************E*************

05/28/2023 14:06:18 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 360, 'sample_intermediate_sizes': [928, 928, 928], 'sample_qkv_sizes': [360, 360, 360]}
05/28/2023 14:06:18 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:06:18 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:06:18 - INFO - __main__ -   *** Example ***
05/28/2023 14:06:18 - INFO - __main__ -   guid: train-0
05/28/2023 14:06:18 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:06:18 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:18 - INFO - __main__ -   label: not_entailment
05/28/2023 14:06:18 - INFO - __main__ -   label_id: 1
05/28/2023 14:06:20 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:06:20 - INFO - __main__ -   *** Example ***
05/28/2023 14:06:20 - INFO - __main__ -   guid: dev-0
05/28/2023 14:06:20 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:06:20 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:20 - INFO - __main__ -   label: not_entailment
05/28/2023 14:06:20 - INFO - __main__ -   label_id: 1
05/28/2023 14:06:20 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:06:20 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:06:20 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:06:20 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:06:20 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:06:20 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:06:20 - INFO - __main__ -     Batch size = 32
05/28/2023 14:06:20 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 24.07it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.38it/s][A05/28/2023 14:06:21 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:21 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:06:21 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:21 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 140.20it/s]
05/28/2023 14:06:21 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:21 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:21 - INFO - __main__ -    dev: eval_loss = 0.7080601387553744
05/28/2023 14:06:21 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:21 - INFO - __main__ -    dev: infer_time = 2.773
05/28/2023 14:06:21 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:21 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:21 - INFO - __main__ -     cls_loss = 0.694458974732293
05/28/2023 14:06:21 - INFO - __main__ -     eval_loss = 0.7080601387553744
05/28/2023 14:06:21 - INFO - __main__ -     global_step = 9
05/28/2023 14:06:21 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:21 - INFO - __main__ -     infer_time = 2.773
05/28/2023 14:06:21 - INFO - __main__ -     loss = 0.694458974732293
05/28/2023 14:06:21 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.92it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.86it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.08it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.49it/s][A05/28/2023 14:06:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:23 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:06:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.70it/s]
05/28/2023 14:06:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:23 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:06:23 - INFO - __main__ -    dev: eval_loss = 0.6914206014739143
05/28/2023 14:06:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:23 - INFO - __main__ -    dev: infer_time = 2.786
05/28/2023 14:06:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:23 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:06:23 - INFO - __main__ -     cls_loss = 0.7004735626672444
05/28/2023 14:06:23 - INFO - __main__ -     eval_loss = 0.6914206014739143
05/28/2023 14:06:23 - INFO - __main__ -     global_step = 19
05/28/2023 14:06:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:23 - INFO - __main__ -     infer_time = 2.786
05/28/2023 14:06:23 - INFO - __main__ -     loss = 0.7004735626672444
05/28/2023 14:06:23 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.37it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.92it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.77it/s][A05/28/2023 14:06:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:24 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:06:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.75it/s]
05/28/2023 14:06:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:24 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:24 - INFO - __main__ -    dev: eval_loss = 0.7001674837536283
05/28/2023 14:06:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:24 - INFO - __main__ -    dev: infer_time = 2.7778888888888895
05/28/2023 14:06:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:24 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:24 - INFO - __main__ -     cls_loss = 0.698964941090551
05/28/2023 14:06:24 - INFO - __main__ -     eval_loss = 0.7001674837536283
05/28/2023 14:06:24 - INFO - __main__ -     global_step = 29
05/28/2023 14:06:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:24 - INFO - __main__ -     infer_time = 2.7778888888888895
05/28/2023 14:06:24 - INFO - __main__ -     loss = 0.698964941090551

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.19it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.40it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.64it/s][A05/28/2023 14:06:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:25 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:06:25 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.79it/s]
05/28/2023 14:06:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:25 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:06:25 - INFO - __main__ -    dev: eval_loss = 0.6917635665999519
05/28/2023 14:06:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:25 - INFO - __main__ -    dev: infer_time = 2.7821111111111114
05/28/2023 14:06:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:25 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:06:25 - INFO - __main__ -     cls_loss = 0.6987561002755777
05/28/2023 14:06:25 - INFO - __main__ -     eval_loss = 0.6917635665999519
05/28/2023 14:06:25 - INFO - __main__ -     global_step = 39
05/28/2023 14:06:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:25 - INFO - __main__ -     infer_time = 2.7821111111111114
05/28/2023 14:06:25 - INFO - __main__ -     loss = 0.6987561002755777

Iteration:  50%|#####     | 39/78 [00:04<00:02, 14.16it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 16.31it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 18.17it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 19.44it/s][A05/28/2023 14:06:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:25 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:06:25 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.93it/s]
05/28/2023 14:06:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:25 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:06:25 - INFO - __main__ -    dev: eval_loss = 0.6916828089290195
05/28/2023 14:06:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:25 - INFO - __main__ -    dev: infer_time = 2.773333333333334
05/28/2023 14:06:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:25 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:06:25 - INFO - __main__ -     cls_loss = 0.69693174897408
05/28/2023 14:06:25 - INFO - __main__ -     eval_loss = 0.6916828089290195
05/28/2023 14:06:25 - INFO - __main__ -     global_step = 49
05/28/2023 14:06:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:25 - INFO - __main__ -     infer_time = 2.773333333333334
05/28/2023 14:06:25 - INFO - __main__ -     loss = 0.69693174897408

Iteration:  65%|######5   | 51/78 [00:05<00:01, 18.06it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 19.68it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:00, 21.00it/s][A05/28/2023 14:06:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:26 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:06:26 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 140.09it/s]
05/28/2023 14:06:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:26 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:06:26 - INFO - __main__ -    dev: eval_loss = 0.6911702818340726
05/28/2023 14:06:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:26 - INFO - __main__ -    dev: infer_time = 2.78
05/28/2023 14:06:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:26 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:06:26 - INFO - __main__ -     cls_loss = 0.6969956998097695
05/28/2023 14:06:26 - INFO - __main__ -     eval_loss = 0.6911702818340726
05/28/2023 14:06:26 - INFO - __main__ -     global_step = 59
05/28/2023 14:06:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:26 - INFO - __main__ -     infer_time = 2.78
05/28/2023 14:06:26 - INFO - __main__ -     loss = 0.6969956998097695

Iteration:  77%|#######6  | 60/78 [00:05<00:00, 18.76it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 20.15it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 21.30it/s][A05/28/2023 14:06:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:26 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:06:26 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 140.06it/s]
05/28/2023 14:06:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:26 - INFO - __main__ -    dev: acc = 0.48736462093862815
05/28/2023 14:06:26 - INFO - __main__ -    dev: eval_loss = 0.6932041446367899
05/28/2023 14:06:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:26 - INFO - __main__ -    dev: infer_time = 2.7776666666666667
05/28/2023 14:06:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:26 - INFO - __main__ -     acc = 0.48736462093862815
05/28/2023 14:06:26 - INFO - __main__ -     cls_loss = 0.6966087550356768
05/28/2023 14:06:26 - INFO - __main__ -     eval_loss = 0.6932041446367899
05/28/2023 14:06:26 - INFO - __main__ -     global_step = 69
05/28/2023 14:06:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:26 - INFO - __main__ -     infer_time = 2.7776666666666667
05/28/2023 14:06:26 - INFO - __main__ -     loss = 0.6966087550356768

Iteration:  88%|########8 | 69/78 [00:05<00:00, 18.90it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 20.30it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 21.01it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.37it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.31s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.31s/it]
05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   w_emb: 10987920

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   p_emb: 184320

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   t_emb: 720

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   ln_emb: 720

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   query_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   key_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   value_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   ln_numel: 720

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   self_numel: 389880

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   output_numel: 130680

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 335008

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 334440

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   ln_numel: 720

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   attention_numel: 520560

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   intermediate_numel: 335008

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   output_numel: 335160

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   query_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   key_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   value_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   ln_numel: 720

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   self_numel: 389880

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   output_numel: 130680

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 335008

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 334440

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   ln_numel: 720

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   attention_numel: 520560

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   intermediate_numel: 335008

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   output_numel: 335160

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   query_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   key_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   value_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   ln_numel: 720

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   self_numel: 389880

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   output_numel: 130680

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 335008

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 334440

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   ln_numel: 720

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   attention_numel: 520560

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   intermediate_numel: 335008

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   output_numel: 335160

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   layer_numel: 3572184
05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   dense_numel: 129960
05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   emb_numel: 11173680

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   encoder_numel: 3572184

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   pooler_numel: 129960

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   all parameters: 14875824

05/28/2023 14:06:27 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:06:27 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 360, 'sample_intermediate_sizes': [928, 928, 928], 'sample_qkv_sizes': [360, 360, 360]}
parameter size = 14875824
best_acc = 0.5270758122743683
time_per_batch_infer = 2.779 ms
infer_cnt = 63
**************E*************

>>> Starting Search of EE Iteration 2 ...
Namespace(arch_perfs_file='../output/kd_ptq_10/subbert.results', candidate_file='/n/home00/lbailey/bigger_and_faster/cands/kd_ptq_10.cands', ckpt_path='/n/home00/lbailey/bigger_and_faster/conf_datasets/lat_predictor_quant.pt', feature_dim=4, feature_norm=[564, 5, 1024, 564], gen_size=10, head_num_space=[1, 12], hidden_dim=2000, hidden_layer_num=3, hidden_size_space=[144, 528], intermediate_size_space=[128, 1024], lat_norm=200, latency_constraint=10.0, layer_num_space=[1, 5], method='Evolved', model='KD', output_file='../cands/1st_generation_kd_ptq_10.evo.cands', qkv_size_space=[144, 528])
Size of candidates: 999
Size of fast candidates: 98
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 300, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_qkv_sizes': [300, 300, 300, 300]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 300, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [300, 300, 300, 300]}
new_arch_latency: 9.999728202819824
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [456, 456, 456]}
after mutation, the new arch is : {'sample_layer_num': 2, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864], 'sample_num_attention_heads': [12, 12], 'sample_qkv_sizes': [468, 468]}
new_arch_latency: 7.335959374904633
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [456, 456, 456]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.365137457847595
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [640, 640, 640], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [324, 324, 324]}
new_arch_latency: 8.261822164058685
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
new_arch_latency: 10.616728663444519
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [800, 800, 800], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.240168869495392
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [832, 832, 832], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.310553014278412
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
new_arch_latency: 9.021583199501038
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
new_arch_latency: 8.843067288398743
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_qkv_sizes': [348, 348, 348, 348]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}
new_arch_latency: 10.93536913394928
old arch: {'sample_layer_num': 5, 'sample_hidden_size': 204, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [204, 204, 204, 204, 204]}
after mutation, the new arch is : {'sample_layer_num': 5, 'sample_hidden_size': 216, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [216, 216, 216, 216, 216]}
new_arch_latency: 10.350564122200012
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [448, 448, 448], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [264, 264, 264]}
new_arch_latency: 7.2220578789711
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
new_arch_latency: 9.12020206451416
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [456, 456, 456]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [800, 800, 800], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
new_arch_latency: 10.08303165435791
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [492, 492, 492]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 492, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [492, 492, 492]}
new_arch_latency: 10.777890682220459
old arch: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [432, 432, 432, 432]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
new_arch_latency: 10.953111946582794
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.503265261650085
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [800, 800, 800], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.240168869495392
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [896, 896, 896], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
new_arch_latency: 10.258431732654572
>>> Starting Evaluation of EE Iteration 3 ...
05/28/2023 14:06:49 - INFO - __main__ -   device: cuda n_gpu: 1
05/28/2023 14:06:49 - INFO - __main__ -   task_lis: ['rte']
05/28/2023 14:06:49 - INFO - __main__ -   data_dir_lis: ['/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/']
05/28/2023 14:06:49 - INFO - transformer.tokenization -   loading vocabulary file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3/vocab.txt
05/28/2023 14:06:49 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:06:49 - INFO - __main__ -   subbert_configs: [{'sample_layer_num': 4, 'sample_hidden_size': 300, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [300, 300, 300, 300]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 168, 'sample_intermediate_sizes': [928, 928, 928, 928], 'sample_qkv_sizes': [168, 168, 168, 168]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [704, 704, 704, 704], 'sample_qkv_sizes': [264, 264, 264, 264]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 480, 'sample_intermediate_sizes': [256, 256, 256], 'sample_qkv_sizes': [480, 480, 480]}, {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}, {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}, {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [832, 832, 832], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [608, 608, 608], 'sample_qkv_sizes': [492, 492, 492]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 276, 'sample_intermediate_sizes': [800, 800, 800, 800], 'sample_qkv_sizes': [276, 276, 276, 276]}, {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}, {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}, {'sample_layer_num': 5, 'sample_hidden_size': 216, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [216, 216, 216, 216, 216]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 372, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [372, 372, 372]}, {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [288, 288, 288, 288]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 300, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_qkv_sizes': [300, 300, 300, 300]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [576, 576, 576], 'sample_qkv_sizes': [492, 492, 492]}, {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [256, 256, 256, 256, 256], 'sample_qkv_sizes': [264, 264, 264, 264, 264]}, {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [800, 800, 800], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [192, 192, 192], 'sample_qkv_sizes': [456, 456, 456]}, {'sample_layer_num': 3, 'sample_hidden_size': 492, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [492, 492, 492]}, {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}, {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [896, 896, 896], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}, {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [992, 992, 992], 'sample_qkv_sizes': [288, 288, 288]}, {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 372, 'sample_intermediate_sizes': [320, 320, 320, 320], 'sample_qkv_sizes': [372, 372, 372, 372]}]
05/28/2023 14:06:49 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 300, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [300, 300, 300, 300]}
05/28/2023 14:06:49 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:06:49 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:06:49 - INFO - __main__ -   *** Example ***
05/28/2023 14:06:49 - INFO - __main__ -   guid: train-0
05/28/2023 14:06:49 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:06:49 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:49 - INFO - __main__ -   label: not_entailment
05/28/2023 14:06:49 - INFO - __main__ -   label_id: 1
05/28/2023 14:06:51 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:06:51 - INFO - __main__ -   *** Example ***
05/28/2023 14:06:51 - INFO - __main__ -   guid: dev-0
05/28/2023 14:06:51 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:06:51 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:51 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:06:51 - INFO - __main__ -   label: not_entailment
05/28/2023 14:06:51 - INFO - __main__ -   label_id: 1
05/28/2023 14:06:51 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:06:51 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:06:52 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:06:55 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:06:55 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:06:55 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:06:55 - INFO - __main__ -     Batch size = 32
05/28/2023 14:06:55 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A/n/home00/lbailey/bigger_and_faster/transformer/optimization.py:248: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)
  next_m.mul_(beta1).add_(1 - beta1, grad)

Iteration:   3%|2         | 2/78 [00:00<00:05, 15.13it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 20.00it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 21.87it/s][A05/28/2023 14:06:55 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:55 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:06:55 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:55 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 137.34it/s]
05/28/2023 14:06:55 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:55 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:06:55 - INFO - __main__ -    dev: eval_loss = 0.7081508901384141
05/28/2023 14:06:55 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:55 - INFO - __main__ -    dev: infer_time = 3.5151111111111115
05/28/2023 14:06:55 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:55 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:06:55 - INFO - __main__ -     cls_loss = 0.6967823637856377
05/28/2023 14:06:55 - INFO - __main__ -     eval_loss = 0.7081508901384141
05/28/2023 14:06:55 - INFO - __main__ -     global_step = 9
05/28/2023 14:06:55 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:55 - INFO - __main__ -     infer_time = 3.5151111111111115
05/28/2023 14:06:55 - INFO - __main__ -     loss = 0.6967823637856377
05/28/2023 14:06:55 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:01<00:15,  4.25it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:10,  6.11it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:07,  8.26it/s][A05/28/2023 14:06:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:57 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:06:57 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 137.79it/s]
05/28/2023 14:06:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:57 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:57 - INFO - __main__ -    dev: eval_loss = 0.7043146027459039
05/28/2023 14:06:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:57 - INFO - __main__ -    dev: infer_time = 3.450444444444445
05/28/2023 14:06:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:57 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:57 - INFO - __main__ -     cls_loss = 0.7096984511927554
05/28/2023 14:06:57 - INFO - __main__ -     eval_loss = 0.7043146027459039
05/28/2023 14:06:57 - INFO - __main__ -     global_step = 19
05/28/2023 14:06:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:57 - INFO - __main__ -     infer_time = 3.450444444444445
05/28/2023 14:06:57 - INFO - __main__ -     loss = 0.7096984511927554

Iteration:  24%|##4       | 19/78 [00:02<00:06,  9.06it/s][A
Iteration:  28%|##8       | 22/78 [00:02<00:04, 11.55it/s][A
Iteration:  32%|###2      | 25/78 [00:02<00:03, 14.01it/s][A
Iteration:  36%|###5      | 28/78 [00:02<00:03, 16.17it/s][A05/28/2023 14:06:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:58 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:06:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.20it/s]
05/28/2023 14:06:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:58 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:06:58 - INFO - __main__ -    dev: eval_loss = 0.697087393866645
05/28/2023 14:06:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:58 - INFO - __main__ -    dev: infer_time = 3.4378888888888888
05/28/2023 14:06:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:58 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:06:58 - INFO - __main__ -     cls_loss = 0.7046674058355135
05/28/2023 14:06:58 - INFO - __main__ -     eval_loss = 0.697087393866645
05/28/2023 14:06:58 - INFO - __main__ -     global_step = 29
05/28/2023 14:06:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:58 - INFO - __main__ -     infer_time = 3.4378888888888888
05/28/2023 14:06:58 - INFO - __main__ -     loss = 0.7046674058355135

Iteration:  40%|###9      | 31/78 [00:02<00:03, 15.63it/s][A
Iteration:  44%|####3     | 34/78 [00:03<00:02, 17.61it/s][A
Iteration:  47%|####7     | 37/78 [00:03<00:02, 19.27it/s][A05/28/2023 14:06:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:58 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:06:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 137.97it/s]
05/28/2023 14:06:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:58 - INFO - __main__ -    dev: acc = 0.5018050541516246
05/28/2023 14:06:58 - INFO - __main__ -    dev: eval_loss = 0.6932802663909065
05/28/2023 14:06:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:58 - INFO - __main__ -    dev: infer_time = 3.444666666666667
05/28/2023 14:06:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:58 - INFO - __main__ -     acc = 0.5018050541516246
05/28/2023 14:06:58 - INFO - __main__ -     cls_loss = 0.7012157547168243
05/28/2023 14:06:58 - INFO - __main__ -     eval_loss = 0.6932802663909065
05/28/2023 14:06:58 - INFO - __main__ -     global_step = 39
05/28/2023 14:06:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:58 - INFO - __main__ -     infer_time = 3.444666666666667
05/28/2023 14:06:58 - INFO - __main__ -     loss = 0.7012157547168243

Iteration:  51%|#####1    | 40/78 [00:03<00:02, 17.74it/s][A
Iteration:  55%|#####5    | 43/78 [00:03<00:01, 19.33it/s][A
Iteration:  59%|#####8    | 46/78 [00:03<00:01, 20.40it/s][A05/28/2023 14:06:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:59 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:06:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 137.80it/s]
05/28/2023 14:06:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:59 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:06:59 - INFO - __main__ -    dev: eval_loss = 0.692337327533298
05/28/2023 14:06:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:59 - INFO - __main__ -    dev: infer_time = 3.464
05/28/2023 14:06:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:59 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:06:59 - INFO - __main__ -     cls_loss = 0.6996905377932957
05/28/2023 14:06:59 - INFO - __main__ -     eval_loss = 0.692337327533298
05/28/2023 14:06:59 - INFO - __main__ -     global_step = 49
05/28/2023 14:06:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:59 - INFO - __main__ -     infer_time = 3.464
05/28/2023 14:06:59 - INFO - __main__ -     loss = 0.6996905377932957

Iteration:  63%|######2   | 49/78 [00:03<00:01, 18.48it/s][A
Iteration:  67%|######6   | 52/78 [00:03<00:01, 19.83it/s][A
Iteration:  71%|#######   | 55/78 [00:04<00:01, 21.08it/s][A
Iteration:  74%|#######4  | 58/78 [00:04<00:00, 21.89it/s][A05/28/2023 14:06:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:06:59 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:06:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:06:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.15it/s]
05/28/2023 14:06:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:06:59 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:06:59 - INFO - __main__ -    dev: eval_loss = 0.6919398175345527
05/28/2023 14:06:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:06:59 - INFO - __main__ -    dev: infer_time = 3.4715555555555557
05/28/2023 14:06:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:06:59 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:06:59 - INFO - __main__ -     cls_loss = 0.698560030783637
05/28/2023 14:06:59 - INFO - __main__ -     eval_loss = 0.6919398175345527
05/28/2023 14:06:59 - INFO - __main__ -     global_step = 59
05/28/2023 14:06:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:06:59 - INFO - __main__ -     infer_time = 3.4715555555555557
05/28/2023 14:06:59 - INFO - __main__ -     loss = 0.698560030783637

Iteration:  78%|#######8  | 61/78 [00:04<00:00, 19.28it/s][A
Iteration:  82%|########2 | 64/78 [00:04<00:00, 20.39it/s][A
Iteration:  86%|########5 | 67/78 [00:04<00:00, 21.43it/s][A05/28/2023 14:07:00 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:00 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:07:00 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:00 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.20it/s]
05/28/2023 14:07:00 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:00 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:07:00 - INFO - __main__ -    dev: eval_loss = 0.6914169788360596
05/28/2023 14:07:00 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:00 - INFO - __main__ -    dev: infer_time = 3.4587777777777777
05/28/2023 14:07:00 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:00 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:07:00 - INFO - __main__ -     cls_loss = 0.6976572465205538
05/28/2023 14:07:00 - INFO - __main__ -     eval_loss = 0.6914169788360596
05/28/2023 14:07:00 - INFO - __main__ -     global_step = 69
05/28/2023 14:07:00 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:00 - INFO - __main__ -     infer_time = 3.4587777777777777
05/28/2023 14:07:00 - INFO - __main__ -     loss = 0.6976572465205538
05/28/2023 14:07:00 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  90%|########9 | 70/78 [00:06<00:01,  5.49it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00,  7.12it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00,  9.03it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.21it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.39s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.39s/it]
05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   w_emb: 9156600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   p_emb: 153600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   t_emb: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ln_emb: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 154112

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 153900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   intermediate_numel: 154112

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   output_numel: 154500

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 154112

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 153900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   intermediate_numel: 154112

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   output_numel: 154500

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 154112

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 153900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   intermediate_numel: 154112

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   output_numel: 154500

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 154112

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 153900

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   intermediate_numel: 154112

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   output_numel: 154500

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   layer_numel: 2681648
05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   dense_numel: 90300
05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   emb_numel: 9311400

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   encoder_numel: 2681648

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   pooler_numel: 90300

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   all parameters: 12083348

05/28/2023 14:07:01 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:01 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 300, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [300, 300, 300, 300]}
parameter size = 12083348
best_acc = 0.5379061371841155
time_per_batch_infer = 3.463 ms
infer_cnt = 63
**************E*************

05/28/2023 14:07:01 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 168, 'sample_intermediate_sizes': [928, 928, 928, 928], 'sample_qkv_sizes': [168, 168, 168, 168]}
05/28/2023 14:07:01 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:07:01 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:07:01 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:01 - INFO - __main__ -   guid: train-0
05/28/2023 14:07:01 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:07:01 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:01 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:01 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:03 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:07:03 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:03 - INFO - __main__ -   guid: dev-0
05/28/2023 14:07:03 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:07:03 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:03 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:03 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:03 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:03 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:03 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:07:03 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:07:04 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:07:04 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:07:04 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:07:04 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:07:04 - INFO - __main__ -     Batch size = 32
05/28/2023 14:07:04 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 24.09it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.82it/s][A05/28/2023 14:07:04 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:04 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:07:04 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:04 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.10it/s]
05/28/2023 14:07:04 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:04 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:04 - INFO - __main__ -    dev: eval_loss = 0.6918980148103502
05/28/2023 14:07:04 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:04 - INFO - __main__ -    dev: infer_time = 3.4881111111111114
05/28/2023 14:07:04 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:04 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:04 - INFO - __main__ -     cls_loss = 0.6940561003155179
05/28/2023 14:07:04 - INFO - __main__ -     eval_loss = 0.6918980148103502
05/28/2023 14:07:04 - INFO - __main__ -     global_step = 9
05/28/2023 14:07:04 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:04 - INFO - __main__ -     infer_time = 3.4881111111111114
05/28/2023 14:07:04 - INFO - __main__ -     loss = 0.6940561003155179
05/28/2023 14:07:04 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:16,  4.06it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:10,  6.02it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.29it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.70it/s][A05/28/2023 14:07:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:06 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:07:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.53it/s]
05/28/2023 14:07:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:06 - INFO - __main__ -    dev: acc = 0.49097472924187724
05/28/2023 14:07:06 - INFO - __main__ -    dev: eval_loss = 0.6931562887297736
05/28/2023 14:07:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:06 - INFO - __main__ -    dev: infer_time = 3.5663333333333336
05/28/2023 14:07:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:06 - INFO - __main__ -     acc = 0.49097472924187724
05/28/2023 14:07:06 - INFO - __main__ -     cls_loss = 0.6945877984950417
05/28/2023 14:07:06 - INFO - __main__ -     eval_loss = 0.6931562887297736
05/28/2023 14:07:06 - INFO - __main__ -     global_step = 19
05/28/2023 14:07:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:06 - INFO - __main__ -     infer_time = 3.5663333333333336
05/28/2023 14:07:06 - INFO - __main__ -     loss = 0.6945877984950417

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.96it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.35it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.41it/s][A05/28/2023 14:07:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:06 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:07:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.80it/s]
05/28/2023 14:07:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:07 - INFO - __main__ -    dev: eval_loss = 0.6914617816607157
05/28/2023 14:07:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:07 - INFO - __main__ -    dev: infer_time = 3.4851111111111117
05/28/2023 14:07:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:07 - INFO - __main__ -     cls_loss = 0.693362217524956
05/28/2023 14:07:07 - INFO - __main__ -     eval_loss = 0.6914617816607157
05/28/2023 14:07:07 - INFO - __main__ -     global_step = 29
05/28/2023 14:07:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:07 - INFO - __main__ -     infer_time = 3.4851111111111117
05/28/2023 14:07:07 - INFO - __main__ -     loss = 0.693362217524956

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.35it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 18.47it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 20.01it/s][A05/28/2023 14:07:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:07 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:07:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.81it/s]
05/28/2023 14:07:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:07 - INFO - __main__ -    dev: eval_loss = 0.6933638718393114
05/28/2023 14:07:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:07 - INFO - __main__ -    dev: infer_time = 3.5012222222222222
05/28/2023 14:07:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:07 - INFO - __main__ -     cls_loss = 0.6930814110315763
05/28/2023 14:07:07 - INFO - __main__ -     eval_loss = 0.6933638718393114
05/28/2023 14:07:07 - INFO - __main__ -     global_step = 39
05/28/2023 14:07:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:07 - INFO - __main__ -     infer_time = 3.5012222222222222
05/28/2023 14:07:07 - INFO - __main__ -     loss = 0.6930814110315763

Iteration:  50%|#####     | 39/78 [00:03<00:02, 18.76it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 20.29it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 21.50it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 22.42it/s][A05/28/2023 14:07:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:07 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:07:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.98it/s]
05/28/2023 14:07:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:07 - INFO - __main__ -    dev: eval_loss = 0.6914601458443536
05/28/2023 14:07:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:07 - INFO - __main__ -    dev: infer_time = 3.5183333333333335
05/28/2023 14:07:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:07 - INFO - __main__ -     cls_loss = 0.694483930967292
05/28/2023 14:07:07 - INFO - __main__ -     eval_loss = 0.6914601458443536
05/28/2023 14:07:07 - INFO - __main__ -     global_step = 49
05/28/2023 14:07:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:07 - INFO - __main__ -     infer_time = 3.5183333333333335
05/28/2023 14:07:07 - INFO - __main__ -     loss = 0.694483930967292

Iteration:  65%|######5   | 51/78 [00:03<00:01, 20.18it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 21.43it/s][A
Iteration:  73%|#######3  | 57/78 [00:03<00:00, 22.39it/s][A05/28/2023 14:07:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:08 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:07:08 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.66it/s]
05/28/2023 14:07:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:08 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:08 - INFO - __main__ -    dev: eval_loss = 0.6919465065002441
05/28/2023 14:07:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:08 - INFO - __main__ -    dev: infer_time = 3.513888888888889
05/28/2023 14:07:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:08 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:08 - INFO - __main__ -     cls_loss = 0.6953804684897601
05/28/2023 14:07:08 - INFO - __main__ -     eval_loss = 0.6919465065002441
05/28/2023 14:07:08 - INFO - __main__ -     global_step = 59
05/28/2023 14:07:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:08 - INFO - __main__ -     infer_time = 3.513888888888889
05/28/2023 14:07:08 - INFO - __main__ -     loss = 0.6953804684897601

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 20.09it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 21.35it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 22.40it/s][A05/28/2023 14:07:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:08 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:07:08 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 165.70it/s]
05/28/2023 14:07:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:08 - INFO - __main__ -    dev: acc = 0.5415162454873647
05/28/2023 14:07:08 - INFO - __main__ -    dev: eval_loss = 0.6926012701458402
05/28/2023 14:07:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:08 - INFO - __main__ -    dev: infer_time = 3.505222222222222
05/28/2023 14:07:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:08 - INFO - __main__ -     acc = 0.5415162454873647
05/28/2023 14:07:08 - INFO - __main__ -     cls_loss = 0.6951363371766132
05/28/2023 14:07:08 - INFO - __main__ -     eval_loss = 0.6926012701458402
05/28/2023 14:07:08 - INFO - __main__ -     global_step = 69
05/28/2023 14:07:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:08 - INFO - __main__ -     infer_time = 3.505222222222222
05/28/2023 14:07:08 - INFO - __main__ -     loss = 0.6951363371766132
05/28/2023 14:07:08 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  88%|########8 | 69/78 [00:05<00:01,  5.61it/s][A
Iteration:  92%|#########2| 72/78 [00:05<00:00,  7.28it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00,  9.25it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.72it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.13s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.13s/it]
05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   w_emb: 5127696

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   p_emb: 86016

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   t_emb: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ln_emb: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 156832

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 156072

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 156832

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   output_numel: 156408

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 156832

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 156072

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 156832

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   output_numel: 156408

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 156832

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 156072

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 156832

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   output_numel: 156408

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   query_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   key_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   value_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   self_numel: 85176

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   output_numel: 28728

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 156832

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 156072

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ln_numel: 336

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   attention_numel: 113904

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 156832

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   output_numel: 156408

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   layer_numel: 1708576
05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   dense_numel: 28392
05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   emb_numel: 5214384

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   encoder_numel: 1708576

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   pooler_numel: 28392

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   all parameters: 6951352

05/28/2023 14:07:10 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:10 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 168, 'sample_intermediate_sizes': [928, 928, 928, 928], 'sample_qkv_sizes': [168, 168, 168, 168]}
parameter size = 6951352
best_acc = 0.5415162454873647
time_per_batch_infer = 3.511 ms
infer_cnt = 63
**************E*************

05/28/2023 14:07:10 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [704, 704, 704, 704], 'sample_qkv_sizes': [264, 264, 264, 264]}
05/28/2023 14:07:10 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:07:10 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:07:10 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:10 - INFO - __main__ -   guid: train-0
05/28/2023 14:07:10 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:07:10 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:10 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:10 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:12 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:07:12 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:12 - INFO - __main__ -   guid: dev-0
05/28/2023 14:07:12 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:07:12 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:12 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:12 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:12 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:07:12 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:07:13 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:07:13 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:07:13 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:07:13 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:07:13 - INFO - __main__ -     Batch size = 32
05/28/2023 14:07:13 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.02it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.69it/s][A05/28/2023 14:07:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:13 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:07:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 141.09it/s]
05/28/2023 14:07:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:13 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:13 - INFO - __main__ -    dev: eval_loss = 0.7045250998602973
05/28/2023 14:07:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:13 - INFO - __main__ -    dev: infer_time = 3.460111111111111
05/28/2023 14:07:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:13 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:13 - INFO - __main__ -     cls_loss = 0.7020054260889689
05/28/2023 14:07:13 - INFO - __main__ -     eval_loss = 0.7045250998602973
05/28/2023 14:07:13 - INFO - __main__ -     global_step = 9
05/28/2023 14:07:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:13 - INFO - __main__ -     infer_time = 3.460111111111111
05/28/2023 14:07:13 - INFO - __main__ -     loss = 0.7020054260889689
05/28/2023 14:07:13 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.99it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.88it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.03it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.31it/s][A05/28/2023 14:07:15 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:15 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:07:15 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:15 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 142.05it/s]
05/28/2023 14:07:15 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:15 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:15 - INFO - __main__ -    dev: eval_loss = 0.6917877064810859
05/28/2023 14:07:15 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:15 - INFO - __main__ -    dev: infer_time = 3.4977777777777774
05/28/2023 14:07:15 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:15 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:15 - INFO - __main__ -     cls_loss = 0.7001600202761198
05/28/2023 14:07:15 - INFO - __main__ -     eval_loss = 0.6917877064810859
05/28/2023 14:07:15 - INFO - __main__ -     global_step = 19
05/28/2023 14:07:15 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:15 - INFO - __main__ -     infer_time = 3.4977777777777774
05/28/2023 14:07:15 - INFO - __main__ -     loss = 0.7001600202761198
05/28/2023 14:07:15 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.38it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.90it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.70it/s][A05/28/2023 14:07:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:17 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:07:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 141.92it/s]
05/28/2023 14:07:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:17 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:07:17 - INFO - __main__ -    dev: eval_loss = 0.6928077737490336
05/28/2023 14:07:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:17 - INFO - __main__ -    dev: infer_time = 3.4455555555555555
05/28/2023 14:07:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:17 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:07:17 - INFO - __main__ -     cls_loss = 0.6978637411676604
05/28/2023 14:07:17 - INFO - __main__ -     eval_loss = 0.6928077737490336
05/28/2023 14:07:17 - INFO - __main__ -     global_step = 29
05/28/2023 14:07:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:17 - INFO - __main__ -     infer_time = 3.4455555555555555
05/28/2023 14:07:17 - INFO - __main__ -     loss = 0.6978637411676604

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.43it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.67it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.89it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.97it/s][A05/28/2023 14:07:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:17 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:07:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 143.03it/s]
05/28/2023 14:07:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:17 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:07:17 - INFO - __main__ -    dev: eval_loss = 0.6911704076661004
05/28/2023 14:07:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:17 - INFO - __main__ -    dev: infer_time = 3.4347777777777777
05/28/2023 14:07:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:17 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:07:17 - INFO - __main__ -     cls_loss = 0.6966570447652768
05/28/2023 14:07:17 - INFO - __main__ -     eval_loss = 0.6911704076661004
05/28/2023 14:07:17 - INFO - __main__ -     global_step = 39
05/28/2023 14:07:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:17 - INFO - __main__ -     infer_time = 3.4347777777777777
05/28/2023 14:07:17 - INFO - __main__ -     loss = 0.6966570447652768

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 15.05it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.90it/s][A
Iteration:  60%|######    | 47/78 [00:04<00:01, 18.42it/s][A05/28/2023 14:07:18 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:18 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:07:18 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:18 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 142.83it/s]
05/28/2023 14:07:18 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:18 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 14:07:18 - INFO - __main__ -    dev: eval_loss = 0.6917180087831285
05/28/2023 14:07:18 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:18 - INFO - __main__ -    dev: infer_time = 3.4337777777777783
05/28/2023 14:07:18 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:18 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 14:07:18 - INFO - __main__ -     cls_loss = 0.6961485536731019
05/28/2023 14:07:18 - INFO - __main__ -     eval_loss = 0.6917180087831285
05/28/2023 14:07:18 - INFO - __main__ -     global_step = 49
05/28/2023 14:07:18 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:18 - INFO - __main__ -     infer_time = 3.4337777777777783
05/28/2023 14:07:18 - INFO - __main__ -     loss = 0.6961485536731019
05/28/2023 14:07:18 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  64%|######4   | 50/78 [00:06<00:05,  5.25it/s][A
Iteration:  68%|######7   | 53/78 [00:06<00:03,  6.85it/s][A
Iteration:  72%|#######1  | 56/78 [00:06<00:02,  8.68it/s][A05/28/2023 14:07:19 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:19 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:07:19 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:19 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 142.44it/s]
05/28/2023 14:07:20 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:20 - INFO - __main__ -    dev: acc = 0.516245487364621
05/28/2023 14:07:20 - INFO - __main__ -    dev: eval_loss = 0.6925336983468797
05/28/2023 14:07:20 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:20 - INFO - __main__ -    dev: infer_time = 3.4675555555555557
05/28/2023 14:07:20 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:20 - INFO - __main__ -     acc = 0.516245487364621
05/28/2023 14:07:20 - INFO - __main__ -     cls_loss = 0.6959519851005683
05/28/2023 14:07:20 - INFO - __main__ -     eval_loss = 0.6925336983468797
05/28/2023 14:07:20 - INFO - __main__ -     global_step = 59
05/28/2023 14:07:20 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:20 - INFO - __main__ -     infer_time = 3.4675555555555557
05/28/2023 14:07:20 - INFO - __main__ -     loss = 0.6959519851005683

Iteration:  76%|#######5  | 59/78 [00:06<00:01,  9.93it/s][A
Iteration:  79%|#######9  | 62/78 [00:07<00:01, 11.93it/s][A
Iteration:  83%|########3 | 65/78 [00:07<00:00, 14.02it/s][A
Iteration:  87%|########7 | 68/78 [00:07<00:00, 15.91it/s][A05/28/2023 14:07:20 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:20 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:07:20 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:20 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 142.80it/s]
05/28/2023 14:07:20 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:20 - INFO - __main__ -    dev: acc = 0.49097472924187724
05/28/2023 14:07:20 - INFO - __main__ -    dev: eval_loss = 0.6935674481921725
05/28/2023 14:07:20 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:20 - INFO - __main__ -    dev: infer_time = 3.4408888888888884
05/28/2023 14:07:20 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:20 - INFO - __main__ -     acc = 0.49097472924187724
05/28/2023 14:07:20 - INFO - __main__ -     cls_loss = 0.6953479163888572
05/28/2023 14:07:20 - INFO - __main__ -     eval_loss = 0.6935674481921725
05/28/2023 14:07:20 - INFO - __main__ -     global_step = 69
05/28/2023 14:07:20 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:20 - INFO - __main__ -     infer_time = 3.4408888888888884
05/28/2023 14:07:20 - INFO - __main__ -     loss = 0.6953479163888572

Iteration:  91%|#########1| 71/78 [00:07<00:00, 15.36it/s][A
Iteration:  95%|#########4| 74/78 [00:07<00:00, 17.08it/s][A
Iteration:  99%|#########8| 77/78 [00:07<00:00, 18.60it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.07it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.75s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.75s/it]
05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   w_emb: 8057808

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   p_emb: 135168

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   t_emb: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ln_emb: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 186560

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 186120

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 186560

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   output_numel: 186648

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 186560

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 186120

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 186560

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   output_numel: 186648

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 186560

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 186120

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 186560

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   output_numel: 186648

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 186560

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 186120

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 186560

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   output_numel: 186648

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   layer_numel: 2614304
05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   dense_numel: 69960
05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   emb_numel: 8194032

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   encoder_numel: 2614304

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   pooler_numel: 69960

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   all parameters: 10878296

05/28/2023 14:07:20 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:20 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [704, 704, 704, 704], 'sample_qkv_sizes': [264, 264, 264, 264]}
parameter size = 10878296
best_acc = 0.5342960288808665
time_per_batch_infer = 3.454 ms
infer_cnt = 63
**************E*************

05/28/2023 14:07:20 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 480, 'sample_intermediate_sizes': [256, 256, 256], 'sample_qkv_sizes': [480, 480, 480]}
05/28/2023 14:07:20 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:07:20 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:07:20 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:20 - INFO - __main__ -   guid: train-0
05/28/2023 14:07:20 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:07:20 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:20 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:20 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:22 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:07:22 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:22 - INFO - __main__ -   guid: dev-0
05/28/2023 14:07:22 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:07:22 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:22 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:22 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:22 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:22 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:22 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:07:22 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:07:23 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:07:23 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:07:23 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:07:23 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:07:23 - INFO - __main__ -     Batch size = 32
05/28/2023 14:07:23 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.94it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.22it/s][A05/28/2023 14:07:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:23 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:07:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 142.61it/s]
05/28/2023 14:07:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:23 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:23 - INFO - __main__ -    dev: eval_loss = 0.7004651096132066
05/28/2023 14:07:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:23 - INFO - __main__ -    dev: infer_time = 2.8553333333333333
05/28/2023 14:07:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:23 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:23 - INFO - __main__ -     cls_loss = 0.6903328763114082
05/28/2023 14:07:23 - INFO - __main__ -     eval_loss = 0.7004651096132066
05/28/2023 14:07:23 - INFO - __main__ -     global_step = 9
05/28/2023 14:07:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:23 - INFO - __main__ -     infer_time = 2.8553333333333333
05/28/2023 14:07:23 - INFO - __main__ -     loss = 0.6903328763114082
05/28/2023 14:07:23 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.70it/s][A
Iteration:  15%|#5        | 12/78 [00:02<00:11,  5.55it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.64it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.90it/s][A05/28/2023 14:07:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:25 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:07:25 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 142.73it/s]
05/28/2023 14:07:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:25 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:25 - INFO - __main__ -    dev: eval_loss = 0.709771090083652
05/28/2023 14:07:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:25 - INFO - __main__ -    dev: infer_time = 2.868111111111111
05/28/2023 14:07:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:25 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:25 - INFO - __main__ -     cls_loss = 0.6926212185307553
05/28/2023 14:07:25 - INFO - __main__ -     eval_loss = 0.709771090083652
05/28/2023 14:07:25 - INFO - __main__ -     global_step = 19
05/28/2023 14:07:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:25 - INFO - __main__ -     infer_time = 2.868111111111111
05/28/2023 14:07:25 - INFO - __main__ -     loss = 0.6926212185307553

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.18it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.59it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.83it/s][A05/28/2023 14:07:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:26 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:07:26 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 143.13it/s]
05/28/2023 14:07:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:26 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:26 - INFO - __main__ -    dev: eval_loss = 0.6944200462765164
05/28/2023 14:07:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:26 - INFO - __main__ -    dev: infer_time = 2.8434444444444447
05/28/2023 14:07:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:26 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:26 - INFO - __main__ -     cls_loss = 0.6925912931047636
05/28/2023 14:07:26 - INFO - __main__ -     eval_loss = 0.6944200462765164
05/28/2023 14:07:26 - INFO - __main__ -     global_step = 29
05/28/2023 14:07:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:26 - INFO - __main__ -     infer_time = 2.8434444444444447
05/28/2023 14:07:26 - INFO - __main__ -     loss = 0.6925912931047636
05/28/2023 14:07:26 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:09,  5.09it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:06,  6.74it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:04,  8.65it/s][A05/28/2023 14:07:28 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:28 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:07:28 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:28 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 142.53it/s]
05/28/2023 14:07:28 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:28 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:07:28 - INFO - __main__ -    dev: eval_loss = 0.6901595791180929
05/28/2023 14:07:28 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:28 - INFO - __main__ -    dev: infer_time = 2.890888888888889
05/28/2023 14:07:28 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:28 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:07:28 - INFO - __main__ -     cls_loss = 0.6959927127911494
05/28/2023 14:07:28 - INFO - __main__ -     eval_loss = 0.6901595791180929
05/28/2023 14:07:28 - INFO - __main__ -     global_step = 39
05/28/2023 14:07:28 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:28 - INFO - __main__ -     infer_time = 2.890888888888889
05/28/2023 14:07:28 - INFO - __main__ -     loss = 0.6959927127911494
05/28/2023 14:07:28 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:05<00:08,  4.37it/s][A
Iteration:  54%|#####3    | 42/78 [00:06<00:06,  5.80it/s][A
Iteration:  58%|#####7    | 45/78 [00:06<00:04,  7.55it/s][A
Iteration:  62%|######1   | 48/78 [00:06<00:03,  9.54it/s][A05/28/2023 14:07:29 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:29 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:07:29 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:29 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 142.76it/s]
05/28/2023 14:07:29 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:29 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:29 - INFO - __main__ -    dev: eval_loss = 0.6953091224034628
05/28/2023 14:07:29 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:29 - INFO - __main__ -    dev: infer_time = 2.8804444444444446
05/28/2023 14:07:29 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:29 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:29 - INFO - __main__ -     cls_loss = 0.6961026824250514
05/28/2023 14:07:29 - INFO - __main__ -     eval_loss = 0.6953091224034628
05/28/2023 14:07:29 - INFO - __main__ -     global_step = 49
05/28/2023 14:07:29 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:29 - INFO - __main__ -     infer_time = 2.8804444444444446
05/28/2023 14:07:29 - INFO - __main__ -     loss = 0.6961026824250514

Iteration:  65%|######5   | 51/78 [00:06<00:02, 10.70it/s][A
Iteration:  69%|######9   | 54/78 [00:06<00:01, 12.90it/s][A
Iteration:  73%|#######3  | 57/78 [00:06<00:01, 15.05it/s][A05/28/2023 14:07:30 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:30 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:07:30 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:30 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 143.30it/s]
05/28/2023 14:07:30 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:30 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:30 - INFO - __main__ -    dev: eval_loss = 0.6903184519873725
05/28/2023 14:07:30 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:30 - INFO - __main__ -    dev: infer_time = 2.8562222222222218
05/28/2023 14:07:30 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:30 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:30 - INFO - __main__ -     cls_loss = 0.6958015904588214
05/28/2023 14:07:30 - INFO - __main__ -     eval_loss = 0.6903184519873725
05/28/2023 14:07:30 - INFO - __main__ -     global_step = 59
05/28/2023 14:07:30 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:30 - INFO - __main__ -     infer_time = 2.8562222222222218
05/28/2023 14:07:30 - INFO - __main__ -     loss = 0.6958015904588214

Iteration:  77%|#######6  | 60/78 [00:06<00:01, 15.23it/s][A
Iteration:  81%|########  | 63/78 [00:07<00:00, 17.26it/s][A
Iteration:  85%|########4 | 66/78 [00:07<00:00, 18.99it/s][A05/28/2023 14:07:30 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:30 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:07:30 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:30 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 143.21it/s]
05/28/2023 14:07:30 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:30 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:30 - INFO - __main__ -    dev: eval_loss = 0.6909894280963473
05/28/2023 14:07:30 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:30 - INFO - __main__ -    dev: infer_time = 2.856333333333333
05/28/2023 14:07:30 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:30 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:30 - INFO - __main__ -     cls_loss = 0.6953511618185735
05/28/2023 14:07:30 - INFO - __main__ -     eval_loss = 0.6909894280963473
05/28/2023 14:07:30 - INFO - __main__ -     global_step = 69
05/28/2023 14:07:30 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:30 - INFO - __main__ -     infer_time = 2.856333333333333
05/28/2023 14:07:30 - INFO - __main__ -     loss = 0.6953511618185735

Iteration:  88%|########8 | 69/78 [00:07<00:00, 17.82it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:00, 19.48it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00, 20.80it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.19it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.65s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.65s/it]
05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   w_emb: 14650560

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   p_emb: 245760

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   t_emb: 960

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   ln_emb: 960

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   query_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   key_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   value_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   self_numel: 692640

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   output_numel: 231840

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 123136

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 123360

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   attention_numel: 924480

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   intermediate_numel: 123136

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   output_numel: 124320

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   query_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   key_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   value_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   self_numel: 692640

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   output_numel: 231840

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 123136

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 123360

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   attention_numel: 924480

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   intermediate_numel: 123136

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   output_numel: 124320

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   query_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   key_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   value_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   self_numel: 692640

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   output_numel: 231840

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 123136

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 123360

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   ln_numel: 960

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   attention_numel: 924480

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   intermediate_numel: 123136

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   output_numel: 124320

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   layer_numel: 3515808
05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   dense_numel: 230880
05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   emb_numel: 14898240

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   encoder_numel: 3515808

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   pooler_numel: 230880

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   all parameters: 18644928

05/28/2023 14:07:31 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:31 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 480, 'sample_intermediate_sizes': [256, 256, 256], 'sample_qkv_sizes': [480, 480, 480]}
parameter size = 18644928
best_acc = 0.5306859205776173
time_per_batch_infer = 2.864 ms
infer_cnt = 63
**************E*************

05/28/2023 14:07:31 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
05/28/2023 14:07:31 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:07:31 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:07:31 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:31 - INFO - __main__ -   guid: train-0
05/28/2023 14:07:31 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:07:31 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:31 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:31 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:31 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:31 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:33 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:07:33 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:33 - INFO - __main__ -   guid: dev-0
05/28/2023 14:07:33 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:07:33 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:33 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:33 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:33 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:33 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:33 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:07:33 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:07:33 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:07:33 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:07:33 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:07:33 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:07:33 - INFO - __main__ -     Batch size = 32
05/28/2023 14:07:33 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.68it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.15it/s][A05/28/2023 14:07:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:34 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:07:34 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.44it/s]
05/28/2023 14:07:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:34 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:34 - INFO - __main__ -    dev: eval_loss = 0.6921909782621596
05/28/2023 14:07:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:34 - INFO - __main__ -    dev: infer_time = 2.8609999999999998
05/28/2023 14:07:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:34 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:34 - INFO - __main__ -     cls_loss = 0.6989777684211731
05/28/2023 14:07:34 - INFO - __main__ -     eval_loss = 0.6921909782621596
05/28/2023 14:07:34 - INFO - __main__ -     global_step = 9
05/28/2023 14:07:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:34 - INFO - __main__ -     infer_time = 2.8609999999999998
05/28/2023 14:07:34 - INFO - __main__ -     loss = 0.6989777684211731
05/28/2023 14:07:34 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.91it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.79it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  7.89it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.13it/s][A05/28/2023 14:07:36 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:36 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:07:36 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:36 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.18it/s]
05/28/2023 14:07:36 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:36 - INFO - __main__ -    dev: acc = 0.5523465703971119
05/28/2023 14:07:36 - INFO - __main__ -    dev: eval_loss = 0.6917404267523024
05/28/2023 14:07:36 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:36 - INFO - __main__ -    dev: infer_time = 2.8612222222222226
05/28/2023 14:07:36 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:36 - INFO - __main__ -     acc = 0.5523465703971119
05/28/2023 14:07:36 - INFO - __main__ -     cls_loss = 0.6945304400042484
05/28/2023 14:07:36 - INFO - __main__ -     eval_loss = 0.6917404267523024
05/28/2023 14:07:36 - INFO - __main__ -     global_step = 19
05/28/2023 14:07:36 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:36 - INFO - __main__ -     infer_time = 2.8612222222222226
05/28/2023 14:07:36 - INFO - __main__ -     loss = 0.6945304400042484
05/28/2023 14:07:36 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:14,  3.92it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.46it/s][A
Iteration:  33%|###3      | 26/78 [00:03<00:07,  7.28it/s][A05/28/2023 14:07:37 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:37 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:07:37 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:37 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.38it/s]
05/28/2023 14:07:37 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:37 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:37 - INFO - __main__ -    dev: eval_loss = 0.7038216392199198
05/28/2023 14:07:37 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:37 - INFO - __main__ -    dev: infer_time = 2.8592222222222228
05/28/2023 14:07:37 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:37 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:37 - INFO - __main__ -     cls_loss = 0.6954545399238323
05/28/2023 14:07:37 - INFO - __main__ -     eval_loss = 0.7038216392199198
05/28/2023 14:07:37 - INFO - __main__ -     global_step = 29
05/28/2023 14:07:37 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:37 - INFO - __main__ -     infer_time = 2.8592222222222228
05/28/2023 14:07:37 - INFO - __main__ -     loss = 0.6954545399238323

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.58it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.63it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.72it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.70it/s][A05/28/2023 14:07:38 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:38 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:07:38 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:38 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.63it/s]
05/28/2023 14:07:38 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:38 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:38 - INFO - __main__ -    dev: eval_loss = 0.6912729342778524
05/28/2023 14:07:38 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:38 - INFO - __main__ -    dev: infer_time = 2.8507777777777776
05/28/2023 14:07:38 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:38 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:38 - INFO - __main__ -     cls_loss = 0.6949802713516431
05/28/2023 14:07:38 - INFO - __main__ -     eval_loss = 0.6912729342778524
05/28/2023 14:07:38 - INFO - __main__ -     global_step = 39
05/28/2023 14:07:38 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:38 - INFO - __main__ -     infer_time = 2.8507777777777776
05/28/2023 14:07:38 - INFO - __main__ -     loss = 0.6949802713516431

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.33it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.15it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.72it/s][A05/28/2023 14:07:38 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:38 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:07:38 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:38 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.71it/s]
05/28/2023 14:07:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:39 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:39 - INFO - __main__ -    dev: eval_loss = 0.6914082765579224
05/28/2023 14:07:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:39 - INFO - __main__ -    dev: infer_time = 2.869222222222222
05/28/2023 14:07:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:39 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:39 - INFO - __main__ -     cls_loss = 0.6949636023871752
05/28/2023 14:07:39 - INFO - __main__ -     eval_loss = 0.6914082765579224
05/28/2023 14:07:39 - INFO - __main__ -     global_step = 49
05/28/2023 14:07:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:39 - INFO - __main__ -     infer_time = 2.869222222222222
05/28/2023 14:07:39 - INFO - __main__ -     loss = 0.6949636023871752

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.22it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 17.76it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 19.01it/s][A05/28/2023 14:07:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:39 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:07:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.53it/s]
05/28/2023 14:07:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:39 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:39 - INFO - __main__ -    dev: eval_loss = 0.6911806133058336
05/28/2023 14:07:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:39 - INFO - __main__ -    dev: infer_time = 2.8529999999999998
05/28/2023 14:07:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:39 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:39 - INFO - __main__ -     cls_loss = 0.6950764019610518
05/28/2023 14:07:39 - INFO - __main__ -     eval_loss = 0.6911806133058336
05/28/2023 14:07:39 - INFO - __main__ -     global_step = 59
05/28/2023 14:07:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:39 - INFO - __main__ -     infer_time = 2.8529999999999998
05/28/2023 14:07:39 - INFO - __main__ -     loss = 0.6950764019610518

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 16.93it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:00, 18.15it/s][A
Iteration:  83%|########3 | 65/78 [00:06<00:00, 19.28it/s][A
Iteration:  87%|########7 | 68/78 [00:06<00:00, 20.23it/s][A05/28/2023 14:07:40 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:40 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:07:40 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:40 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.74it/s]
05/28/2023 14:07:40 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:40 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:40 - INFO - __main__ -    dev: eval_loss = 0.69192017449273
05/28/2023 14:07:40 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:40 - INFO - __main__ -    dev: infer_time = 2.8353333333333333
05/28/2023 14:07:40 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:40 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:40 - INFO - __main__ -     cls_loss = 0.6950373096742491
05/28/2023 14:07:40 - INFO - __main__ -     eval_loss = 0.69192017449273
05/28/2023 14:07:40 - INFO - __main__ -     global_step = 69
05/28/2023 14:07:40 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:40 - INFO - __main__ -     infer_time = 2.8353333333333333
05/28/2023 14:07:40 - INFO - __main__ -     loss = 0.6950373096742491

Iteration:  91%|#########1| 71/78 [00:06<00:00, 17.64it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 18.86it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 19.84it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.75it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.64s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.64s/it]
05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   w_emb: 14284296

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   p_emb: 239616

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   t_emb: 936

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   ln_emb: 936

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 405216

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 404820

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   intermediate_numel: 405216

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   output_numel: 405756

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 405216

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 404820

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   intermediate_numel: 405216

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   output_numel: 405756

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 405216

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 404820

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   intermediate_numel: 405216

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   output_numel: 405756

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   layer_numel: 5069628
05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   dense_numel: 219492
05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   emb_numel: 14525784

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   encoder_numel: 5069628

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   pooler_numel: 219492

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   all parameters: 19814904

05/28/2023 14:07:40 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:40 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
parameter size = 19814904
best_acc = 0.5523465703971119
time_per_batch_infer = 2.856 ms
infer_cnt = 63
**************E*************

05/28/2023 14:07:40 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
05/28/2023 14:07:40 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:07:40 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:07:40 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:40 - INFO - __main__ -   guid: train-0
05/28/2023 14:07:40 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:07:40 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:40 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:40 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:40 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:40 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:42 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:07:42 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:42 - INFO - __main__ -   guid: dev-0
05/28/2023 14:07:42 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:07:42 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:42 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:42 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:42 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:42 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:42 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:07:42 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:07:42 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:07:43 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:07:43 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:07:43 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:07:43 - INFO - __main__ -     Batch size = 32
05/28/2023 14:07:43 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.26it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.98it/s][A05/28/2023 14:07:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:43 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:07:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.24it/s]
05/28/2023 14:07:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:43 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:07:43 - INFO - __main__ -    dev: eval_loss = 0.6914996637238396
05/28/2023 14:07:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:43 - INFO - __main__ -    dev: infer_time = 3.453
05/28/2023 14:07:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:43 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:07:43 - INFO - __main__ -     cls_loss = 0.6941590044233534
05/28/2023 14:07:43 - INFO - __main__ -     eval_loss = 0.6914996637238396
05/28/2023 14:07:43 - INFO - __main__ -     global_step = 9
05/28/2023 14:07:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:43 - INFO - __main__ -     infer_time = 3.453
05/28/2023 14:07:43 - INFO - __main__ -     loss = 0.6941590044233534
05/28/2023 14:07:43 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.84it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.67it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.76it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.96it/s][A05/28/2023 14:07:45 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:45 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:07:45 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:45 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.00it/s]
05/28/2023 14:07:45 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:45 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:45 - INFO - __main__ -    dev: eval_loss = 0.6915329694747925
05/28/2023 14:07:45 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:45 - INFO - __main__ -    dev: infer_time = 3.511444444444444
05/28/2023 14:07:45 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:45 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:45 - INFO - __main__ -     cls_loss = 0.6953137109154149
05/28/2023 14:07:45 - INFO - __main__ -     eval_loss = 0.6915329694747925
05/28/2023 14:07:45 - INFO - __main__ -     global_step = 19
05/28/2023 14:07:45 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:45 - INFO - __main__ -     infer_time = 3.511444444444444
05/28/2023 14:07:45 - INFO - __main__ -     loss = 0.6953137109154149
05/28/2023 14:07:45 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:15,  3.65it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.11it/s][A
Iteration:  33%|###3      | 26/78 [00:04<00:07,  6.83it/s][A05/28/2023 14:07:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:47 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:07:47 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.23it/s]
05/28/2023 14:07:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:47 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:47 - INFO - __main__ -    dev: eval_loss = 0.6953703628646003
05/28/2023 14:07:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:47 - INFO - __main__ -    dev: infer_time = 3.4841111111111114
05/28/2023 14:07:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:47 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:47 - INFO - __main__ -     cls_loss = 0.6951158231702345
05/28/2023 14:07:47 - INFO - __main__ -     eval_loss = 0.6953703628646003
05/28/2023 14:07:47 - INFO - __main__ -     global_step = 29
05/28/2023 14:07:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:47 - INFO - __main__ -     infer_time = 3.4841111111111114
05/28/2023 14:07:47 - INFO - __main__ -     loss = 0.6951158231702345

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.18it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.21it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.18it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.17it/s][A05/28/2023 14:07:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:47 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:07:47 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.21it/s]
05/28/2023 14:07:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:47 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:07:47 - INFO - __main__ -    dev: eval_loss = 0.6918860077857971
05/28/2023 14:07:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:47 - INFO - __main__ -    dev: infer_time = 3.4615555555555555
05/28/2023 14:07:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:47 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:07:47 - INFO - __main__ -     cls_loss = 0.6955393659762847
05/28/2023 14:07:47 - INFO - __main__ -     eval_loss = 0.6918860077857971
05/28/2023 14:07:47 - INFO - __main__ -     global_step = 39
05/28/2023 14:07:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:47 - INFO - __main__ -     infer_time = 3.4615555555555555
05/28/2023 14:07:47 - INFO - __main__ -     loss = 0.6955393659762847

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 13.99it/s][A
Iteration:  56%|#####6    | 44/78 [00:05<00:02, 15.47it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.05it/s][A05/28/2023 14:07:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:48 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:07:48 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.04it/s]
05/28/2023 14:07:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:48 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:48 - INFO - __main__ -    dev: eval_loss = 0.6953539583418105
05/28/2023 14:07:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:48 - INFO - __main__ -    dev: infer_time = 3.4584444444444444
05/28/2023 14:07:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:48 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:48 - INFO - __main__ -     cls_loss = 0.695970889256925
05/28/2023 14:07:48 - INFO - __main__ -     eval_loss = 0.6953539583418105
05/28/2023 14:07:48 - INFO - __main__ -     global_step = 49
05/28/2023 14:07:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:48 - INFO - __main__ -     infer_time = 3.4584444444444444
05/28/2023 14:07:48 - INFO - __main__ -     loss = 0.695970889256925

Iteration:  63%|######2   | 49/78 [00:05<00:01, 15.44it/s][A
Iteration:  67%|######6   | 52/78 [00:05<00:01, 16.91it/s][A
Iteration:  71%|#######   | 55/78 [00:05<00:01, 18.37it/s][A
Iteration:  74%|#######4  | 58/78 [00:05<00:01, 19.33it/s][A05/28/2023 14:07:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:48 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:07:48 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.51it/s]
05/28/2023 14:07:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:48 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:48 - INFO - __main__ -    dev: eval_loss = 0.7042989863289727
05/28/2023 14:07:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:48 - INFO - __main__ -    dev: infer_time = 3.445666666666667
05/28/2023 14:07:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:48 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:48 - INFO - __main__ -     cls_loss = 0.6947063025781663
05/28/2023 14:07:48 - INFO - __main__ -     eval_loss = 0.7042989863289727
05/28/2023 14:07:48 - INFO - __main__ -     global_step = 59
05/28/2023 14:07:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:48 - INFO - __main__ -     infer_time = 3.445666666666667
05/28/2023 14:07:48 - INFO - __main__ -     loss = 0.6947063025781663

Iteration:  78%|#######8  | 61/78 [00:06<00:00, 17.35it/s][A
Iteration:  82%|########2 | 64/78 [00:06<00:00, 18.64it/s][A
Iteration:  86%|########5 | 67/78 [00:06<00:00, 19.48it/s][A05/28/2023 14:07:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:49 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:07:49 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 128.30it/s]
05/28/2023 14:07:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:49 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:49 - INFO - __main__ -    dev: eval_loss = 0.6989752385351393
05/28/2023 14:07:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:49 - INFO - __main__ -    dev: infer_time = 3.457444444444444
05/28/2023 14:07:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:49 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:49 - INFO - __main__ -     cls_loss = 0.6955560247103373
05/28/2023 14:07:49 - INFO - __main__ -     eval_loss = 0.6989752385351393
05/28/2023 14:07:49 - INFO - __main__ -     global_step = 69
05/28/2023 14:07:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:49 - INFO - __main__ -     infer_time = 3.457444444444444
05/28/2023 14:07:49 - INFO - __main__ -     loss = 0.6955560247103373

Iteration:  90%|########9 | 70/78 [00:06<00:00, 17.15it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 18.39it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 19.43it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.37it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.86s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.86s/it]
05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   w_emb: 9889128

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   p_emb: 165888

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   t_emb: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ln_emb: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 197600

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 197316

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 197600

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   output_numel: 197964

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 197600

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 197316

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 197600

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   output_numel: 197964

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 197600

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 197316

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 197600

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   output_numel: 197964

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   query_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   key_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   value_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   self_numel: 315900

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   output_numel: 105948

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 197600

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 197316

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ln_numel: 648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   attention_numel: 421848

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   intermediate_numel: 197600

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   output_numel: 197964

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   layer_numel: 3269648
05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   dense_numel: 105300
05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   emb_numel: 10056312

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   encoder_numel: 3269648

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   pooler_numel: 105300

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   all parameters: 13431260

05/28/2023 14:07:49 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:07:49 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 324, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [324, 324, 324, 324]}
parameter size = 13431260
best_acc = 0.5270758122743683
time_per_batch_infer = 3.467 ms
infer_cnt = 63
**************E*************

05/28/2023 14:07:49 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [832, 832, 832], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
05/28/2023 14:07:49 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:07:49 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:07:49 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:49 - INFO - __main__ -   guid: train-0
05/28/2023 14:07:49 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:07:49 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:49 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:49 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:51 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:07:51 - INFO - __main__ -   *** Example ***
05/28/2023 14:07:51 - INFO - __main__ -   guid: dev-0
05/28/2023 14:07:51 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:07:51 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:51 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:51 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:07:51 - INFO - __main__ -   label: not_entailment
05/28/2023 14:07:51 - INFO - __main__ -   label_id: 1
05/28/2023 14:07:51 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:07:51 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:07:52 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:07:52 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:07:52 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:07:52 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:07:52 - INFO - __main__ -     Batch size = 32
05/28/2023 14:07:52 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.65it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.32it/s][A05/28/2023 14:07:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:52 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:07:52 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.19it/s]
05/28/2023 14:07:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:52 - INFO - __main__ -    dev: acc = 0.4657039711191336
05/28/2023 14:07:52 - INFO - __main__ -    dev: eval_loss = 0.6948820816146003
05/28/2023 14:07:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:52 - INFO - __main__ -    dev: infer_time = 2.8405555555555555
05/28/2023 14:07:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:52 - INFO - __main__ -     acc = 0.4657039711191336
05/28/2023 14:07:52 - INFO - __main__ -     cls_loss = 0.6997637814945645
05/28/2023 14:07:52 - INFO - __main__ -     eval_loss = 0.6948820816146003
05/28/2023 14:07:52 - INFO - __main__ -     global_step = 9
05/28/2023 14:07:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:52 - INFO - __main__ -     infer_time = 2.8405555555555555
05/28/2023 14:07:52 - INFO - __main__ -     loss = 0.6997637814945645
05/28/2023 14:07:52 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:19,  3.50it/s][A
Iteration:  15%|#5        | 12/78 [00:02<00:12,  5.24it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.25it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.43it/s][A05/28/2023 14:07:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:54 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:07:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.80it/s]
05/28/2023 14:07:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:54 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:07:54 - INFO - __main__ -    dev: eval_loss = 0.6924205289946662
05/28/2023 14:07:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:54 - INFO - __main__ -    dev: infer_time = 2.85
05/28/2023 14:07:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:54 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:07:54 - INFO - __main__ -     cls_loss = 0.6991647576030932
05/28/2023 14:07:54 - INFO - __main__ -     eval_loss = 0.6924205289946662
05/28/2023 14:07:54 - INFO - __main__ -     global_step = 19
05/28/2023 14:07:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:54 - INFO - __main__ -     infer_time = 2.85
05/28/2023 14:07:54 - INFO - __main__ -     loss = 0.6991647576030932
05/28/2023 14:07:55 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.24it/s][A
Iteration:  31%|###       | 24/78 [00:04<00:09,  5.71it/s][A
Iteration:  35%|###4      | 27/78 [00:04<00:06,  7.45it/s][A05/28/2023 14:07:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:56 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:07:56 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.91it/s]
05/28/2023 14:07:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:56 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:07:56 - INFO - __main__ -    dev: eval_loss = 0.6957634952333238
05/28/2023 14:07:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:56 - INFO - __main__ -    dev: infer_time = 2.8732222222222226
05/28/2023 14:07:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:56 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:07:56 - INFO - __main__ -     cls_loss = 0.6979672374396488
05/28/2023 14:07:56 - INFO - __main__ -     eval_loss = 0.6957634952333238
05/28/2023 14:07:56 - INFO - __main__ -     global_step = 29
05/28/2023 14:07:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:56 - INFO - __main__ -     infer_time = 2.8732222222222226
05/28/2023 14:07:56 - INFO - __main__ -     loss = 0.6979672374396488

Iteration:  37%|###7      | 29/78 [00:04<00:06,  8.10it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.26it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.47it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.53it/s][A05/28/2023 14:07:57 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:57 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:07:57 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:57 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.06it/s]
05/28/2023 14:07:57 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:57 - INFO - __main__ -    dev: acc = 0.5523465703971119
05/28/2023 14:07:57 - INFO - __main__ -    dev: eval_loss = 0.6906705498695374
05/28/2023 14:07:57 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:57 - INFO - __main__ -    dev: infer_time = 2.8624444444444443
05/28/2023 14:07:57 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:57 - INFO - __main__ -     acc = 0.5523465703971119
05/28/2023 14:07:57 - INFO - __main__ -     cls_loss = 0.6969663271537194
05/28/2023 14:07:57 - INFO - __main__ -     eval_loss = 0.6906705498695374
05/28/2023 14:07:57 - INFO - __main__ -     global_step = 39
05/28/2023 14:07:57 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:57 - INFO - __main__ -     infer_time = 2.8624444444444443
05/28/2023 14:07:57 - INFO - __main__ -     loss = 0.6969663271537194
05/28/2023 14:07:57 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  53%|#####2    | 41/78 [00:06<00:07,  4.92it/s][A
Iteration:  56%|#####6    | 44/78 [00:06<00:05,  6.47it/s][A
Iteration:  60%|######    | 47/78 [00:06<00:03,  8.26it/s][A05/28/2023 14:07:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:59 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:07:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.74it/s]
05/28/2023 14:07:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:59 - INFO - __main__ -    dev: acc = 0.5415162454873647
05/28/2023 14:07:59 - INFO - __main__ -    dev: eval_loss = 0.6890046066708035
05/28/2023 14:07:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:59 - INFO - __main__ -    dev: infer_time = 2.8986666666666667
05/28/2023 14:07:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:59 - INFO - __main__ -     acc = 0.5415162454873647
05/28/2023 14:07:59 - INFO - __main__ -     cls_loss = 0.6960342721063264
05/28/2023 14:07:59 - INFO - __main__ -     eval_loss = 0.6890046066708035
05/28/2023 14:07:59 - INFO - __main__ -     global_step = 49
05/28/2023 14:07:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:59 - INFO - __main__ -     infer_time = 2.8986666666666667
05/28/2023 14:07:59 - INFO - __main__ -     loss = 0.6960342721063264

Iteration:  63%|######2   | 49/78 [00:06<00:03,  8.82it/s][A
Iteration:  67%|######6   | 52/78 [00:06<00:02, 10.98it/s][A
Iteration:  71%|#######   | 55/78 [00:06<00:01, 13.10it/s][A
Iteration:  74%|#######4  | 58/78 [00:07<00:01, 15.04it/s][A05/28/2023 14:07:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:07:59 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:07:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:07:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 117.05it/s]
05/28/2023 14:07:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:07:59 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 14:07:59 - INFO - __main__ -    dev: eval_loss = 0.6926555434862772
05/28/2023 14:07:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:07:59 - INFO - __main__ -    dev: infer_time = 2.898111111111111
05/28/2023 14:07:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:07:59 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 14:07:59 - INFO - __main__ -     cls_loss = 0.6954538498894643
05/28/2023 14:07:59 - INFO - __main__ -     eval_loss = 0.6926555434862772
05/28/2023 14:07:59 - INFO - __main__ -     global_step = 59
05/28/2023 14:07:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:07:59 - INFO - __main__ -     infer_time = 2.898111111111111
05/28/2023 14:07:59 - INFO - __main__ -     loss = 0.6954538498894643

Iteration:  78%|#######8  | 61/78 [00:07<00:01, 14.61it/s][A
Iteration:  82%|########2 | 64/78 [00:07<00:00, 16.35it/s][A
Iteration:  86%|########5 | 67/78 [00:07<00:00, 17.87it/s][A05/28/2023 14:08:00 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:00 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:08:00 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:00 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.81it/s]
05/28/2023 14:08:00 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:00 - INFO - __main__ -    dev: acc = 0.5595667870036101
05/28/2023 14:08:00 - INFO - __main__ -    dev: eval_loss = 0.6909248563978407
05/28/2023 14:08:00 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:00 - INFO - __main__ -    dev: infer_time = 2.9022222222222225
05/28/2023 14:08:00 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:00 - INFO - __main__ -     acc = 0.5595667870036101
05/28/2023 14:08:00 - INFO - __main__ -     cls_loss = 0.6957835684651914
05/28/2023 14:08:00 - INFO - __main__ -     eval_loss = 0.6909248563978407
05/28/2023 14:08:00 - INFO - __main__ -     global_step = 69
05/28/2023 14:08:00 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:00 - INFO - __main__ -     infer_time = 2.9022222222222225
05/28/2023 14:08:00 - INFO - __main__ -     loss = 0.6957835684651914
05/28/2023 14:08:00 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  90%|########9 | 70/78 [00:09<00:01,  5.23it/s][A
Iteration:  94%|#########3| 73/78 [00:09<00:00,  6.79it/s][A
Iteration:  97%|#########7| 76/78 [00:09<00:00,  8.54it/s][AIteration: 100%|##########| 78/78 [00:09<00:00,  8.30it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.40s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.40s/it]
05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   w_emb: 14284296

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   p_emb: 239616

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   t_emb: 936

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   ln_emb: 936

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 390208

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 389844

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   intermediate_numel: 390208

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   output_numel: 390780

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 390208

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 389844

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   intermediate_numel: 390208

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   output_numel: 390780

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   query_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   key_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   value_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   self_numel: 658476

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   output_numel: 220428

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 390208

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 389844

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   ln_numel: 936

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   attention_numel: 878904

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   intermediate_numel: 390208

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   output_numel: 390780

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   layer_numel: 4979676
05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   dense_numel: 219492
05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   emb_numel: 14525784

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   encoder_numel: 4979676

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   pooler_numel: 219492

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   all parameters: 19724952

05/28/2023 14:08:01 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:01 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [832, 832, 832], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
parameter size = 19724952
best_acc = 0.5595667870036101
time_per_batch_infer = 2.875 ms
infer_cnt = 63
**************E*************

05/28/2023 14:08:01 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [608, 608, 608], 'sample_qkv_sizes': [492, 492, 492]}
05/28/2023 14:08:01 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:08:01 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:08:01 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:01 - INFO - __main__ -   guid: train-0
05/28/2023 14:08:01 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:08:01 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:01 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:01 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:01 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:01 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:03 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:08:03 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:03 - INFO - __main__ -   guid: dev-0
05/28/2023 14:08:03 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:08:03 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:03 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:03 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:03 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:03 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:03 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:08:03 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:08:04 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:08:04 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:08:04 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:08:04 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:08:04 - INFO - __main__ -     Batch size = 32
05/28/2023 14:08:04 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.69it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.58it/s][A05/28/2023 14:08:04 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:04 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:08:04 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:04 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.50it/s]
05/28/2023 14:08:04 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:04 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:08:04 - INFO - __main__ -    dev: eval_loss = 0.6903922160466512
05/28/2023 14:08:04 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:04 - INFO - __main__ -    dev: infer_time = 2.8901111111111115
05/28/2023 14:08:04 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:04 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:08:04 - INFO - __main__ -     cls_loss = 0.7054189708497789
05/28/2023 14:08:04 - INFO - __main__ -     eval_loss = 0.6903922160466512
05/28/2023 14:08:04 - INFO - __main__ -     global_step = 9
05/28/2023 14:08:04 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:04 - INFO - __main__ -     infer_time = 2.8901111111111115
05/28/2023 14:08:04 - INFO - __main__ -     loss = 0.7054189708497789
05/28/2023 14:08:04 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.94it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.83it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.01it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.32it/s][A05/28/2023 14:08:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:06 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:08:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 120.87it/s]
05/28/2023 14:08:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:06 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:06 - INFO - __main__ -    dev: eval_loss = 0.7048145996199714
05/28/2023 14:08:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:06 - INFO - __main__ -    dev: infer_time = 2.880222222222222
05/28/2023 14:08:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:06 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:06 - INFO - __main__ -     cls_loss = 0.7045913715111581
05/28/2023 14:08:06 - INFO - __main__ -     eval_loss = 0.7048145996199714
05/28/2023 14:08:06 - INFO - __main__ -     global_step = 19
05/28/2023 14:08:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:06 - INFO - __main__ -     infer_time = 2.880222222222222
05/28/2023 14:08:06 - INFO - __main__ -     loss = 0.7045913715111581

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.32it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.54it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.69it/s][A05/28/2023 14:08:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:07 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:08:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.63it/s]
05/28/2023 14:08:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:07 - INFO - __main__ -    dev: eval_loss = 0.7041297091378106
05/28/2023 14:08:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:07 - INFO - __main__ -    dev: infer_time = 2.8679999999999994
05/28/2023 14:08:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:07 - INFO - __main__ -     cls_loss = 0.70143729859385
05/28/2023 14:08:07 - INFO - __main__ -     eval_loss = 0.7041297091378106
05/28/2023 14:08:07 - INFO - __main__ -     global_step = 29
05/28/2023 14:08:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:07 - INFO - __main__ -     infer_time = 2.8679999999999994
05/28/2023 14:08:07 - INFO - __main__ -     loss = 0.70143729859385

Iteration:  38%|###8      | 30/78 [00:02<00:03, 15.10it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 16.96it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 18.46it/s][A05/28/2023 14:08:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:07 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:08:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.65it/s]
05/28/2023 14:08:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:07 - INFO - __main__ -    dev: acc = 0.516245487364621
05/28/2023 14:08:07 - INFO - __main__ -    dev: eval_loss = 0.692642092704773
05/28/2023 14:08:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:07 - INFO - __main__ -    dev: infer_time = 2.868444444444444
05/28/2023 14:08:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:07 - INFO - __main__ -     acc = 0.516245487364621
05/28/2023 14:08:07 - INFO - __main__ -     cls_loss = 0.7031632524270278
05/28/2023 14:08:07 - INFO - __main__ -     eval_loss = 0.692642092704773
05/28/2023 14:08:07 - INFO - __main__ -     global_step = 39
05/28/2023 14:08:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:07 - INFO - __main__ -     infer_time = 2.868444444444444
05/28/2023 14:08:07 - INFO - __main__ -     loss = 0.7031632524270278

Iteration:  50%|#####     | 39/78 [00:03<00:02, 17.02it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 18.63it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 19.85it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 20.97it/s][A05/28/2023 14:08:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:08 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:08:08 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.64it/s]
05/28/2023 14:08:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:08 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:08 - INFO - __main__ -    dev: eval_loss = 0.7086129718356662
05/28/2023 14:08:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:08 - INFO - __main__ -    dev: infer_time = 2.882111111111111
05/28/2023 14:08:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:08 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:08 - INFO - __main__ -     cls_loss = 0.7015439746331196
05/28/2023 14:08:08 - INFO - __main__ -     eval_loss = 0.7086129718356662
05/28/2023 14:08:08 - INFO - __main__ -     global_step = 49
05/28/2023 14:08:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:08 - INFO - __main__ -     infer_time = 2.882111111111111
05/28/2023 14:08:08 - INFO - __main__ -     loss = 0.7015439746331196

Iteration:  65%|######5   | 51/78 [00:03<00:01, 18.33it/s][A
Iteration:  69%|######9   | 54/78 [00:04<00:01, 19.55it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:01, 20.63it/s][A05/28/2023 14:08:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:08 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:08:08 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.40it/s]
05/28/2023 14:08:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:08 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:08 - INFO - __main__ -    dev: eval_loss = 0.6937506066428291
05/28/2023 14:08:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:08 - INFO - __main__ -    dev: infer_time = 2.8906666666666667
05/28/2023 14:08:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:08 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:08 - INFO - __main__ -     cls_loss = 0.7009308914006767
05/28/2023 14:08:08 - INFO - __main__ -     eval_loss = 0.6937506066428291
05/28/2023 14:08:08 - INFO - __main__ -     global_step = 59
05/28/2023 14:08:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:08 - INFO - __main__ -     infer_time = 2.8906666666666667
05/28/2023 14:08:08 - INFO - __main__ -     loss = 0.7009308914006767

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 18.23it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 19.37it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 20.43it/s][A05/28/2023 14:08:09 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:09 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:08:09 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:09 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 121.22it/s]
05/28/2023 14:08:09 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:09 - INFO - __main__ -    dev: acc = 0.5523465703971119
05/28/2023 14:08:09 - INFO - __main__ -    dev: eval_loss = 0.6914972729153104
05/28/2023 14:08:09 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:09 - INFO - __main__ -    dev: infer_time = 2.888888888888889
05/28/2023 14:08:09 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:09 - INFO - __main__ -     acc = 0.5523465703971119
05/28/2023 14:08:09 - INFO - __main__ -     cls_loss = 0.7000908566557843
05/28/2023 14:08:09 - INFO - __main__ -     eval_loss = 0.6914972729153104
05/28/2023 14:08:09 - INFO - __main__ -     global_step = 69
05/28/2023 14:08:09 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:09 - INFO - __main__ -     infer_time = 2.888888888888889
05/28/2023 14:08:09 - INFO - __main__ -     loss = 0.7000908566557843
05/28/2023 14:08:09 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  88%|########8 | 69/78 [00:06<00:01,  5.37it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00,  6.96it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00,  8.81it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.02it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.49s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.49s/it]
05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   w_emb: 15016824

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   p_emb: 251904

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   t_emb: 984

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   ln_emb: 984

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 299744

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 299628

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 299744

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   output_numel: 300612

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 299744

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 299628

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 299744

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   output_numel: 300612

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 299744

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 299628

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   intermediate_numel: 299744

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   output_numel: 300612

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   layer_numel: 4714692
05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   dense_numel: 242556
05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   emb_numel: 15270696

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   encoder_numel: 4714692

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   pooler_numel: 242556

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   all parameters: 20227944

05/28/2023 14:08:10 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:10 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [608, 608, 608], 'sample_qkv_sizes': [492, 492, 492]}
parameter size = 20227944
best_acc = 0.5523465703971119
time_per_batch_infer = 2.881 ms
infer_cnt = 63
**************E*************

05/28/2023 14:08:10 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 276, 'sample_intermediate_sizes': [800, 800, 800, 800], 'sample_qkv_sizes': [276, 276, 276, 276]}
05/28/2023 14:08:10 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:08:11 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:08:11 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:11 - INFO - __main__ -   guid: train-0
05/28/2023 14:08:11 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:08:11 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:11 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:11 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:11 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:11 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:12 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:08:12 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:12 - INFO - __main__ -   guid: dev-0
05/28/2023 14:08:12 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:08:12 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:12 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:12 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:12 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:08:13 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:08:13 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:08:13 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:08:13 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:08:13 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:08:13 - INFO - __main__ -     Batch size = 32
05/28/2023 14:08:13 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.73it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.26it/s][A05/28/2023 14:08:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:13 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:08:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.31it/s]
05/28/2023 14:08:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:14 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:14 - INFO - __main__ -    dev: eval_loss = 0.7001021504402161
05/28/2023 14:08:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:14 - INFO - __main__ -    dev: infer_time = 3.5187777777777782
05/28/2023 14:08:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:14 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:14 - INFO - __main__ -     cls_loss = 0.6879439221488105
05/28/2023 14:08:14 - INFO - __main__ -     eval_loss = 0.7001021504402161
05/28/2023 14:08:14 - INFO - __main__ -     global_step = 9
05/28/2023 14:08:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:14 - INFO - __main__ -     infer_time = 3.5187777777777782
05/28/2023 14:08:14 - INFO - __main__ -     loss = 0.6879439221488105
05/28/2023 14:08:14 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.96it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.84it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  7.95it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.22it/s][A05/28/2023 14:08:15 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:15 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:08:15 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:15 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.41it/s]
05/28/2023 14:08:15 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:15 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:15 - INFO - __main__ -    dev: eval_loss = 0.7040497528182136
05/28/2023 14:08:15 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:15 - INFO - __main__ -    dev: infer_time = 3.5637777777777777
05/28/2023 14:08:15 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:15 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:15 - INFO - __main__ -     cls_loss = 0.6986276570119356
05/28/2023 14:08:15 - INFO - __main__ -     eval_loss = 0.7040497528182136
05/28/2023 14:08:15 - INFO - __main__ -     global_step = 19
05/28/2023 14:08:15 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:15 - INFO - __main__ -     infer_time = 3.5637777777777777
05/28/2023 14:08:15 - INFO - __main__ -     loss = 0.6986276570119356

Iteration:  26%|##5       | 20/78 [00:02<00:05, 10.67it/s][A
Iteration:  29%|##9       | 23/78 [00:02<00:04, 13.08it/s][A
Iteration:  33%|###3      | 26/78 [00:02<00:03, 15.22it/s][A05/28/2023 14:08:16 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:16 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:08:16 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:16 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.85it/s]
05/28/2023 14:08:16 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:16 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:16 - INFO - __main__ -    dev: eval_loss = 0.6987087527910868
05/28/2023 14:08:16 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:16 - INFO - __main__ -    dev: infer_time = 3.5345555555555555
05/28/2023 14:08:16 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:16 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:16 - INFO - __main__ -     cls_loss = 0.6980505071837326
05/28/2023 14:08:16 - INFO - __main__ -     eval_loss = 0.6987087527910868
05/28/2023 14:08:16 - INFO - __main__ -     global_step = 29
05/28/2023 14:08:16 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:16 - INFO - __main__ -     infer_time = 3.5345555555555555
05/28/2023 14:08:16 - INFO - __main__ -     loss = 0.6980505071837326

Iteration:  37%|###7      | 29/78 [00:02<00:03, 15.06it/s][A
Iteration:  41%|####1     | 32/78 [00:02<00:02, 16.83it/s][A
Iteration:  45%|####4     | 35/78 [00:03<00:02, 18.31it/s][A
Iteration:  49%|####8     | 38/78 [00:03<00:02, 19.43it/s][A05/28/2023 14:08:16 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:16 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:08:16 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:16 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.94it/s]
05/28/2023 14:08:16 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:16 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:16 - INFO - __main__ -    dev: eval_loss = 0.6953559385405647
05/28/2023 14:08:16 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:16 - INFO - __main__ -    dev: infer_time = 3.5356666666666663
05/28/2023 14:08:16 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:16 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:16 - INFO - __main__ -     cls_loss = 0.696705543077909
05/28/2023 14:08:16 - INFO - __main__ -     eval_loss = 0.6953559385405647
05/28/2023 14:08:16 - INFO - __main__ -     global_step = 39
05/28/2023 14:08:16 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:16 - INFO - __main__ -     infer_time = 3.5356666666666663
05/28/2023 14:08:16 - INFO - __main__ -     loss = 0.696705543077909

Iteration:  53%|#####2    | 41/78 [00:03<00:02, 17.44it/s][A
Iteration:  56%|#####6    | 44/78 [00:03<00:01, 18.84it/s][A
Iteration:  60%|######    | 47/78 [00:03<00:01, 19.81it/s][A05/28/2023 14:08:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:17 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:08:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.90it/s]
05/28/2023 14:08:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:17 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:17 - INFO - __main__ -    dev: eval_loss = 0.6971539656321207
05/28/2023 14:08:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:17 - INFO - __main__ -    dev: infer_time = 3.548666666666667
05/28/2023 14:08:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:17 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:17 - INFO - __main__ -     cls_loss = 0.6958835331761107
05/28/2023 14:08:17 - INFO - __main__ -     eval_loss = 0.6971539656321207
05/28/2023 14:08:17 - INFO - __main__ -     global_step = 49
05/28/2023 14:08:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:17 - INFO - __main__ -     infer_time = 3.548666666666667
05/28/2023 14:08:17 - INFO - __main__ -     loss = 0.6958835331761107

Iteration:  64%|######4   | 50/78 [00:03<00:01, 17.94it/s][A
Iteration:  68%|######7   | 53/78 [00:04<00:01, 19.15it/s][A
Iteration:  72%|#######1  | 56/78 [00:04<00:01, 20.16it/s][A05/28/2023 14:08:17 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:17 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:08:17 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:17 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.75it/s]
05/28/2023 14:08:17 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:17 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:08:17 - INFO - __main__ -    dev: eval_loss = 0.6927362018161349
05/28/2023 14:08:17 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:17 - INFO - __main__ -    dev: infer_time = 3.5416666666666665
05/28/2023 14:08:17 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:17 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:08:17 - INFO - __main__ -     cls_loss = 0.6959702493780751
05/28/2023 14:08:17 - INFO - __main__ -     eval_loss = 0.6927362018161349
05/28/2023 14:08:17 - INFO - __main__ -     global_step = 59
05/28/2023 14:08:17 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:17 - INFO - __main__ -     infer_time = 3.5416666666666665
05/28/2023 14:08:17 - INFO - __main__ -     loss = 0.6959702493780751
05/28/2023 14:08:17 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  76%|#######5  | 59/78 [00:05<00:03,  5.42it/s][A
Iteration:  79%|#######9  | 62/78 [00:05<00:02,  6.98it/s][A
Iteration:  83%|########3 | 65/78 [00:05<00:01,  8.80it/s][A
Iteration:  87%|########7 | 68/78 [00:06<00:00, 10.77it/s][A05/28/2023 14:08:19 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:19 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:08:19 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:19 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.49it/s]
05/28/2023 14:08:19 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:19 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:19 - INFO - __main__ -    dev: eval_loss = 0.6911972827381558
05/28/2023 14:08:19 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:19 - INFO - __main__ -    dev: infer_time = 3.5617777777777775
05/28/2023 14:08:19 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:19 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:19 - INFO - __main__ -     cls_loss = 0.6957931371702664
05/28/2023 14:08:19 - INFO - __main__ -     eval_loss = 0.6911972827381558
05/28/2023 14:08:19 - INFO - __main__ -     global_step = 69
05/28/2023 14:08:19 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:19 - INFO - __main__ -     infer_time = 3.5617777777777775
05/28/2023 14:08:19 - INFO - __main__ -     loss = 0.6957931371702664

Iteration:  90%|########9 | 70/78 [00:06<00:00, 11.00it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 13.21it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 15.24it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.95it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.52s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.53s/it]
05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   w_emb: 8424072

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   p_emb: 141312

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   t_emb: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ln_emb: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 221600

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 221076

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 221600

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   output_numel: 221628

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 221600

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 221076

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 221600

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   output_numel: 221628

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 221600

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 221076

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 221600

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   output_numel: 221628

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   query_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   key_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   value_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   self_numel: 229356

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   output_numel: 77004

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 221600

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 221076

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ln_numel: 552

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   attention_numel: 306360

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   intermediate_numel: 221600

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   output_numel: 221628

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   layer_numel: 2998352
05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   dense_numel: 76452
05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   emb_numel: 8566488

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   encoder_numel: 2998352

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   pooler_numel: 76452

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   all parameters: 11641292

05/28/2023 14:08:20 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:20 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 276, 'sample_intermediate_sizes': [800, 800, 800, 800], 'sample_qkv_sizes': [276, 276, 276, 276]}
parameter size = 11641292
best_acc = 0.5379061371841155
time_per_batch_infer = 3.544 ms
infer_cnt = 63
**************E*************

05/28/2023 14:08:20 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
05/28/2023 14:08:20 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:08:20 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:08:20 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:20 - INFO - __main__ -   guid: train-0
05/28/2023 14:08:20 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:08:20 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:20 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:20 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:21 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:08:21 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:21 - INFO - __main__ -   guid: dev-0
05/28/2023 14:08:21 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:08:21 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:21 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:21 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:22 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:08:22 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:08:22 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:08:22 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:08:22 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:08:22 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:08:22 - INFO - __main__ -     Batch size = 32
05/28/2023 14:08:22 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 24.58it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.74it/s][A05/28/2023 14:08:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:23 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:08:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.37it/s]
05/28/2023 14:08:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:23 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:23 - INFO - __main__ -    dev: eval_loss = 0.6935232480367025
05/28/2023 14:08:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:23 - INFO - __main__ -    dev: infer_time = 3.496888888888889
05/28/2023 14:08:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:23 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:23 - INFO - __main__ -     cls_loss = 0.6915452414088779
05/28/2023 14:08:23 - INFO - __main__ -     eval_loss = 0.6935232480367025
05/28/2023 14:08:23 - INFO - __main__ -     global_step = 9
05/28/2023 14:08:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:23 - INFO - __main__ -     infer_time = 3.496888888888889
05/28/2023 14:08:23 - INFO - __main__ -     loss = 0.6915452414088779
05/28/2023 14:08:23 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.05it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:10,  6.01it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.32it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.74it/s][A05/28/2023 14:08:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:24 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:08:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.29it/s]
05/28/2023 14:08:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:24 - INFO - __main__ -    dev: acc = 0.5018050541516246
05/28/2023 14:08:24 - INFO - __main__ -    dev: eval_loss = 0.6931571231948005
05/28/2023 14:08:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:24 - INFO - __main__ -    dev: infer_time = 3.5001111111111114
05/28/2023 14:08:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:24 - INFO - __main__ -     acc = 0.5018050541516246
05/28/2023 14:08:24 - INFO - __main__ -     cls_loss = 0.6945118872742904
05/28/2023 14:08:24 - INFO - __main__ -     eval_loss = 0.6931571231948005
05/28/2023 14:08:24 - INFO - __main__ -     global_step = 19
05/28/2023 14:08:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:24 - INFO - __main__ -     infer_time = 3.5001111111111114
05/28/2023 14:08:24 - INFO - __main__ -     loss = 0.6945118872742904

Iteration:  27%|##6       | 21/78 [00:02<00:04, 11.98it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 14.52it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 16.88it/s][A05/28/2023 14:08:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:25 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:08:25 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.73it/s]
05/28/2023 14:08:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:25 - INFO - __main__ -    dev: acc = 0.48014440433212996
05/28/2023 14:08:25 - INFO - __main__ -    dev: eval_loss = 0.6945028040144179
05/28/2023 14:08:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:25 - INFO - __main__ -    dev: infer_time = 3.505888888888889
05/28/2023 14:08:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:25 - INFO - __main__ -     acc = 0.48014440433212996
05/28/2023 14:08:25 - INFO - __main__ -     cls_loss = 0.6940269326341564
05/28/2023 14:08:25 - INFO - __main__ -     eval_loss = 0.6945028040144179
05/28/2023 14:08:25 - INFO - __main__ -     global_step = 29
05/28/2023 14:08:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:25 - INFO - __main__ -     infer_time = 3.505888888888889
05/28/2023 14:08:25 - INFO - __main__ -     loss = 0.6940269326341564

Iteration:  38%|###8      | 30/78 [00:02<00:02, 16.86it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 18.82it/s][A
Iteration:  46%|####6     | 36/78 [00:02<00:02, 20.62it/s][A05/28/2023 14:08:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:25 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:08:25 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.78it/s]
05/28/2023 14:08:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:25 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:08:25 - INFO - __main__ -    dev: eval_loss = 0.691263410780165
05/28/2023 14:08:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:25 - INFO - __main__ -    dev: infer_time = 3.5073333333333334
05/28/2023 14:08:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:25 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:08:25 - INFO - __main__ -     cls_loss = 0.6940070482400748
05/28/2023 14:08:25 - INFO - __main__ -     eval_loss = 0.691263410780165
05/28/2023 14:08:25 - INFO - __main__ -     global_step = 39
05/28/2023 14:08:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:25 - INFO - __main__ -     infer_time = 3.5073333333333334
05/28/2023 14:08:25 - INFO - __main__ -     loss = 0.6940070482400748

Iteration:  50%|#####     | 39/78 [00:03<00:02, 19.45it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 20.83it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 21.85it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 23.06it/s][A05/28/2023 14:08:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:26 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:08:26 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.98it/s]
05/28/2023 14:08:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:26 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:26 - INFO - __main__ -    dev: eval_loss = 0.6913144257333543
05/28/2023 14:08:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:26 - INFO - __main__ -    dev: infer_time = 3.511222222222222
05/28/2023 14:08:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:26 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:26 - INFO - __main__ -     cls_loss = 0.6936985546228837
05/28/2023 14:08:26 - INFO - __main__ -     eval_loss = 0.6913144257333543
05/28/2023 14:08:26 - INFO - __main__ -     global_step = 49
05/28/2023 14:08:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:26 - INFO - __main__ -     infer_time = 3.511222222222222
05/28/2023 14:08:26 - INFO - __main__ -     loss = 0.6936985546228837

Iteration:  65%|######5   | 51/78 [00:03<00:01, 20.71it/s][A
Iteration:  69%|######9   | 54/78 [00:03<00:01, 22.07it/s][A
Iteration:  73%|#######3  | 57/78 [00:03<00:00, 23.10it/s][A05/28/2023 14:08:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:26 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:08:26 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.64it/s]
05/28/2023 14:08:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:26 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:26 - INFO - __main__ -    dev: eval_loss = 0.6908270253075494
05/28/2023 14:08:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:26 - INFO - __main__ -    dev: infer_time = 3.513444444444445
05/28/2023 14:08:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:26 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:26 - INFO - __main__ -     cls_loss = 0.6940522537393085
05/28/2023 14:08:26 - INFO - __main__ -     eval_loss = 0.6908270253075494
05/28/2023 14:08:26 - INFO - __main__ -     global_step = 59
05/28/2023 14:08:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:26 - INFO - __main__ -     infer_time = 3.513444444444445
05/28/2023 14:08:26 - INFO - __main__ -     loss = 0.6940522537393085

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 20.63it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 21.80it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 23.00it/s][A05/28/2023 14:08:27 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:27 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:08:27 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:27 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 169.84it/s]
05/28/2023 14:08:27 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:27 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:08:27 - INFO - __main__ -    dev: eval_loss = 0.6914801134003533
05/28/2023 14:08:27 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:27 - INFO - __main__ -    dev: infer_time = 3.518111111111111
05/28/2023 14:08:27 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:27 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:08:27 - INFO - __main__ -     cls_loss = 0.6945293295210686
05/28/2023 14:08:27 - INFO - __main__ -     eval_loss = 0.6914801134003533
05/28/2023 14:08:27 - INFO - __main__ -     global_step = 69
05/28/2023 14:08:27 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:27 - INFO - __main__ -     infer_time = 3.518111111111111
05/28/2023 14:08:27 - INFO - __main__ -     loss = 0.6945293295210686
05/28/2023 14:08:27 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  88%|########8 | 69/78 [00:05<00:01,  5.40it/s][A
Iteration:  92%|#########2| 72/78 [00:05<00:00,  7.04it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00,  9.04it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.73it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.13s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.13s/it]
05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   w_emb: 7691544

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   p_emb: 129024

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   t_emb: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ln_emb: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 97152

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 97020

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   intermediate_numel: 97152

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   output_numel: 97524

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 97152

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 97020

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   intermediate_numel: 97152

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   output_numel: 97524

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 97152

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 97020

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   intermediate_numel: 97152

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   output_numel: 97524

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 97152

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 97020

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   intermediate_numel: 97152

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   output_numel: 97524

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   layer_numel: 1800816
05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   dense_numel: 63756
05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   emb_numel: 7821576

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   encoder_numel: 1800816

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   pooler_numel: 63756

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   all parameters: 9686148

05/28/2023 14:08:28 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:28 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
parameter size = 9686148
best_acc = 0.5379061371841155
time_per_batch_infer = 3.508 ms
infer_cnt = 63
**************E*************

05/28/2023 14:08:28 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}
05/28/2023 14:08:28 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:08:28 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:08:28 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:28 - INFO - __main__ -   guid: train-0
05/28/2023 14:08:28 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:08:28 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:28 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:28 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:28 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:28 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:30 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:08:30 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:30 - INFO - __main__ -   guid: dev-0
05/28/2023 14:08:30 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:08:30 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:30 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:30 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:30 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:30 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:30 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:08:30 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:08:31 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:08:31 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:08:31 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:08:31 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:08:31 - INFO - __main__ -     Batch size = 32
05/28/2023 14:08:31 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   3%|2         | 2/78 [00:00<00:03, 19.60it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 20.99it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 21.54it/s][A05/28/2023 14:08:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:31 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:08:31 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.58it/s]
05/28/2023 14:08:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:31 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:31 - INFO - __main__ -    dev: eval_loss = 0.6909578243891398
05/28/2023 14:08:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:31 - INFO - __main__ -    dev: infer_time = 3.5562222222222224
05/28/2023 14:08:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:31 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:31 - INFO - __main__ -     cls_loss = 0.6971065402030945
05/28/2023 14:08:31 - INFO - __main__ -     eval_loss = 0.6909578243891398
05/28/2023 14:08:31 - INFO - __main__ -     global_step = 9
05/28/2023 14:08:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:31 - INFO - __main__ -     infer_time = 3.5562222222222224
05/28/2023 14:08:31 - INFO - __main__ -     loss = 0.6971065402030945
05/28/2023 14:08:31 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:01<00:16,  4.17it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:10,  5.95it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:07,  7.95it/s][A05/28/2023 14:08:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:33 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:08:33 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.10it/s]
05/28/2023 14:08:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:33 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:33 - INFO - __main__ -    dev: eval_loss = 0.6911763283941481
05/28/2023 14:08:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:33 - INFO - __main__ -    dev: infer_time = 3.6727777777777786
05/28/2023 14:08:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:33 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:33 - INFO - __main__ -     cls_loss = 0.6969246080047206
05/28/2023 14:08:33 - INFO - __main__ -     eval_loss = 0.6911763283941481
05/28/2023 14:08:33 - INFO - __main__ -     global_step = 19
05/28/2023 14:08:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:33 - INFO - __main__ -     infer_time = 3.6727777777777786
05/28/2023 14:08:33 - INFO - __main__ -     loss = 0.6969246080047206

Iteration:  24%|##4       | 19/78 [00:02<00:06,  8.63it/s][A
Iteration:  28%|##8       | 22/78 [00:02<00:05, 10.75it/s][A
Iteration:  32%|###2      | 25/78 [00:02<00:04, 12.95it/s][A
Iteration:  36%|###5      | 28/78 [00:02<00:03, 14.82it/s][A05/28/2023 14:08:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:34 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:08:34 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.19it/s]
05/28/2023 14:08:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:34 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:34 - INFO - __main__ -    dev: eval_loss = 0.6925155189302232
05/28/2023 14:08:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:34 - INFO - __main__ -    dev: infer_time = 3.624
05/28/2023 14:08:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:34 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:34 - INFO - __main__ -     cls_loss = 0.6994836309860493
05/28/2023 14:08:34 - INFO - __main__ -     eval_loss = 0.6925155189302232
05/28/2023 14:08:34 - INFO - __main__ -     global_step = 29
05/28/2023 14:08:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:34 - INFO - __main__ -     infer_time = 3.624
05/28/2023 14:08:34 - INFO - __main__ -     loss = 0.6994836309860493

Iteration:  38%|###8      | 30/78 [00:02<00:03, 13.72it/s][A
Iteration:  42%|####2     | 33/78 [00:03<00:02, 15.61it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 17.15it/s][A05/28/2023 14:08:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:34 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:08:34 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.37it/s]
05/28/2023 14:08:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:34 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:34 - INFO - __main__ -    dev: eval_loss = 0.703267342514462
05/28/2023 14:08:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:34 - INFO - __main__ -    dev: infer_time = 3.6338888888888894
05/28/2023 14:08:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:34 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:34 - INFO - __main__ -     cls_loss = 0.697635103494693
05/28/2023 14:08:34 - INFO - __main__ -     eval_loss = 0.703267342514462
05/28/2023 14:08:34 - INFO - __main__ -     global_step = 39
05/28/2023 14:08:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:34 - INFO - __main__ -     infer_time = 3.6338888888888894
05/28/2023 14:08:34 - INFO - __main__ -     loss = 0.697635103494693

Iteration:  50%|#####     | 39/78 [00:03<00:02, 15.95it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:02, 17.33it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 18.58it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 19.45it/s][A05/28/2023 14:08:35 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:35 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:08:35 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:35 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.43it/s]
05/28/2023 14:08:35 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:35 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:35 - INFO - __main__ -    dev: eval_loss = 0.7041315568817986
05/28/2023 14:08:35 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:35 - INFO - __main__ -    dev: infer_time = 3.635111111111111
05/28/2023 14:08:35 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:35 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:35 - INFO - __main__ -     cls_loss = 0.6978318460133611
05/28/2023 14:08:35 - INFO - __main__ -     eval_loss = 0.7041315568817986
05/28/2023 14:08:35 - INFO - __main__ -     global_step = 49
05/28/2023 14:08:35 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:35 - INFO - __main__ -     infer_time = 3.635111111111111
05/28/2023 14:08:35 - INFO - __main__ -     loss = 0.6978318460133611

Iteration:  65%|######5   | 51/78 [00:04<00:01, 17.12it/s][A
Iteration:  69%|######9   | 54/78 [00:04<00:01, 18.27it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:01, 19.23it/s][A05/28/2023 14:08:35 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:35 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:08:35 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:35 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.42it/s]
05/28/2023 14:08:35 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:35 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:08:35 - INFO - __main__ -    dev: eval_loss = 0.6956780155499777
05/28/2023 14:08:35 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:35 - INFO - __main__ -    dev: infer_time = 3.622444444444445
05/28/2023 14:08:35 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:35 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:08:35 - INFO - __main__ -     cls_loss = 0.6968571311336452
05/28/2023 14:08:35 - INFO - __main__ -     eval_loss = 0.6956780155499777
05/28/2023 14:08:35 - INFO - __main__ -     global_step = 59
05/28/2023 14:08:35 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:35 - INFO - __main__ -     infer_time = 3.622444444444445
05/28/2023 14:08:35 - INFO - __main__ -     loss = 0.6968571311336452

Iteration:  77%|#######6  | 60/78 [00:04<00:01, 17.16it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 18.29it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 19.26it/s][A05/28/2023 14:08:36 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:36 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:08:36 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:36 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 124.46it/s]
05/28/2023 14:08:36 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:36 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 14:08:36 - INFO - __main__ -    dev: eval_loss = 0.693200323316786
05/28/2023 14:08:36 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:36 - INFO - __main__ -    dev: infer_time = 3.612222222222222
05/28/2023 14:08:36 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:36 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 14:08:36 - INFO - __main__ -     cls_loss = 0.696457452532174
05/28/2023 14:08:36 - INFO - __main__ -     eval_loss = 0.693200323316786
05/28/2023 14:08:36 - INFO - __main__ -     global_step = 69
05/28/2023 14:08:36 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:36 - INFO - __main__ -     infer_time = 3.612222222222222
05/28/2023 14:08:36 - INFO - __main__ -     loss = 0.696457452532174

Iteration:  88%|########8 | 69/78 [00:05<00:00, 17.19it/s][A
Iteration:  91%|#########1| 71/78 [00:05<00:00, 17.58it/s][A
Iteration:  95%|#########4| 74/78 [00:05<00:00, 18.82it/s][A
Iteration:  99%|#########8| 77/78 [00:05<00:00, 19.63it/s][AIteration: 100%|##########| 78/78 [00:05<00:00, 14.25it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.48s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.48s/it]
05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   w_emb: 10621656

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   p_emb: 178176

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   t_emb: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ln_emb: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   query_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   key_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   value_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   self_numel: 364356

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   output_numel: 122148

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 212192

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 211932

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ln_numel: 696

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   attention_numel: 486504

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 212192

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   output_numel: 212628

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   layer_numel: 3645296
05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   dense_numel: 121452
05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   emb_numel: 10801224

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   encoder_numel: 3645296

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   pooler_numel: 121452

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   all parameters: 14567972

05/28/2023 14:08:36 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:36 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 348, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [348, 348, 348, 348]}
parameter size = 14567972
best_acc = 0.5270758122743683
time_per_batch_infer = 3.622 ms
infer_cnt = 63
**************E*************

05/28/2023 14:08:36 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_hidden_size': 216, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [216, 216, 216, 216, 216]}
05/28/2023 14:08:36 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:08:36 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:08:36 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:36 - INFO - __main__ -   guid: train-0
05/28/2023 14:08:36 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:08:36 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:36 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:36 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:38 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:08:38 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:38 - INFO - __main__ -   guid: dev-0
05/28/2023 14:08:38 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:08:38 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:38 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:38 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:38 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:38 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:08:38 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:08:39 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:08:39 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:08:39 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:08:39 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:08:39 - INFO - __main__ -     Batch size = 32
05/28/2023 14:08:39 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.58it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.41it/s][A05/28/2023 14:08:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:39 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:08:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.59it/s]
05/28/2023 14:08:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:39 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:39 - INFO - __main__ -    dev: eval_loss = 0.6918371121088663
05/28/2023 14:08:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:39 - INFO - __main__ -    dev: infer_time = 4.158111111111111
05/28/2023 14:08:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:39 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:39 - INFO - __main__ -     cls_loss = 0.6926107340388827
05/28/2023 14:08:39 - INFO - __main__ -     eval_loss = 0.6918371121088663
05/28/2023 14:08:39 - INFO - __main__ -     global_step = 9
05/28/2023 14:08:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:39 - INFO - __main__ -     infer_time = 4.158111111111111
05/28/2023 14:08:39 - INFO - __main__ -     loss = 0.6926107340388827
05/28/2023 14:08:39 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.98it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.84it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  7.95it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.20it/s][A05/28/2023 14:08:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:41 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:08:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 156.66it/s]
05/28/2023 14:08:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:41 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:08:41 - INFO - __main__ -    dev: eval_loss = 0.6921792030334473
05/28/2023 14:08:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:41 - INFO - __main__ -    dev: infer_time = 4.212777777777777
05/28/2023 14:08:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:41 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:08:41 - INFO - __main__ -     cls_loss = 0.6960242516116092
05/28/2023 14:08:41 - INFO - __main__ -     eval_loss = 0.6921792030334473
05/28/2023 14:08:41 - INFO - __main__ -     global_step = 19
05/28/2023 14:08:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:41 - INFO - __main__ -     infer_time = 4.212777777777777
05/28/2023 14:08:41 - INFO - __main__ -     loss = 0.6960242516116092
05/28/2023 14:08:41 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:15,  3.73it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.23it/s][A
Iteration:  33%|###3      | 26/78 [00:04<00:07,  7.00it/s][A05/28/2023 14:08:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:43 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:08:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.09it/s]
05/28/2023 14:08:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:43 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:08:43 - INFO - __main__ -    dev: eval_loss = 0.6919671561982896
05/28/2023 14:08:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:43 - INFO - __main__ -    dev: infer_time = 4.209222222222222
05/28/2023 14:08:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:43 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:08:43 - INFO - __main__ -     cls_loss = 0.6947732608893822
05/28/2023 14:08:43 - INFO - __main__ -     eval_loss = 0.6919671561982896
05/28/2023 14:08:43 - INFO - __main__ -     global_step = 29
05/28/2023 14:08:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:43 - INFO - __main__ -     infer_time = 4.209222222222222
05/28/2023 14:08:43 - INFO - __main__ -     loss = 0.6947732608893822

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.45it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.45it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.55it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.51it/s][A05/28/2023 14:08:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:44 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:08:44 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.38it/s]
05/28/2023 14:08:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:44 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:44 - INFO - __main__ -    dev: eval_loss = 0.6909740302297804
05/28/2023 14:08:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:44 - INFO - __main__ -    dev: infer_time = 4.1817777777777785
05/28/2023 14:08:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:44 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:44 - INFO - __main__ -     cls_loss = 0.6946375431158603
05/28/2023 14:08:44 - INFO - __main__ -     eval_loss = 0.6909740302297804
05/28/2023 14:08:44 - INFO - __main__ -     global_step = 39
05/28/2023 14:08:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:44 - INFO - __main__ -     infer_time = 4.1817777777777785
05/28/2023 14:08:44 - INFO - __main__ -     loss = 0.6946375431158603

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.51it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 16.24it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.57it/s][A05/28/2023 14:08:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:44 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:08:44 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 157.29it/s]
05/28/2023 14:08:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:44 - INFO - __main__ -    dev: acc = 0.5306859205776173
05/28/2023 14:08:44 - INFO - __main__ -    dev: eval_loss = 0.691002243094974
05/28/2023 14:08:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:44 - INFO - __main__ -    dev: infer_time = 4.138111111111111
05/28/2023 14:08:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:44 - INFO - __main__ -     acc = 0.5306859205776173
05/28/2023 14:08:44 - INFO - __main__ -     cls_loss = 0.6948929356068981
05/28/2023 14:08:44 - INFO - __main__ -     eval_loss = 0.691002243094974
05/28/2023 14:08:44 - INFO - __main__ -     global_step = 49
05/28/2023 14:08:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:44 - INFO - __main__ -     infer_time = 4.138111111111111
05/28/2023 14:08:44 - INFO - __main__ -     loss = 0.6948929356068981

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.48it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 18.08it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 19.10it/s][A05/28/2023 14:08:45 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:45 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:08:45 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:45 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 156.05it/s]
05/28/2023 14:08:45 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:45 - INFO - __main__ -    dev: acc = 0.5415162454873647
05/28/2023 14:08:45 - INFO - __main__ -    dev: eval_loss = 0.6926744315359328
05/28/2023 14:08:45 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:45 - INFO - __main__ -    dev: infer_time = 4.356222222222223
05/28/2023 14:08:45 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:45 - INFO - __main__ -     acc = 0.5415162454873647
05/28/2023 14:08:45 - INFO - __main__ -     cls_loss = 0.6941604674872706
05/28/2023 14:08:45 - INFO - __main__ -     eval_loss = 0.6926744315359328
05/28/2023 14:08:45 - INFO - __main__ -     global_step = 59
05/28/2023 14:08:45 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:45 - INFO - __main__ -     infer_time = 4.356222222222223
05/28/2023 14:08:45 - INFO - __main__ -     loss = 0.6941604674872706
05/28/2023 14:08:45 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  76%|#######5  | 59/78 [00:07<00:03,  5.31it/s][A
Iteration:  78%|#######8  | 61/78 [00:07<00:02,  6.29it/s][A
Iteration:  82%|########2 | 64/78 [00:07<00:01,  8.09it/s][A
Iteration:  86%|########5 | 67/78 [00:07<00:01, 10.14it/s][A05/28/2023 14:08:47 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:47 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:08:47 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:47 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 156.97it/s]
05/28/2023 14:08:47 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:47 - INFO - __main__ -    dev: acc = 0.49097472924187724
05/28/2023 14:08:47 - INFO - __main__ -    dev: eval_loss = 0.6950687103801303
05/28/2023 14:08:47 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:47 - INFO - __main__ -    dev: infer_time = 4.178000000000001
05/28/2023 14:08:47 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:47 - INFO - __main__ -     acc = 0.49097472924187724
05/28/2023 14:08:47 - INFO - __main__ -     cls_loss = 0.6938064823979917
05/28/2023 14:08:47 - INFO - __main__ -     eval_loss = 0.6950687103801303
05/28/2023 14:08:47 - INFO - __main__ -     global_step = 69
05/28/2023 14:08:47 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:47 - INFO - __main__ -     infer_time = 4.178000000000001
05/28/2023 14:08:47 - INFO - __main__ -     loss = 0.6938064823979917

Iteration:  88%|########8 | 69/78 [00:07<00:00, 10.68it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:00, 12.81it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00, 14.93it/s][AIteration: 100%|##########| 78/78 [00:08<00:00,  9.74it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.01s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.01s/it]
05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   w_emb: 6592752

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   p_emb: 110592

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   t_emb: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_emb: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83160

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 83592

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83160

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 83592

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83160

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 83592

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83160

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 83592

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   query_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   key_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   value_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   self_numel: 140616

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 47304

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 83160

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ln_numel: 432

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   attention_numel: 187920

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83328

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   output_numel: 83592

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   layer_numel: 1774200
05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   dense_numel: 46872
05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   emb_numel: 6704208

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   encoder_numel: 1774200

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   pooler_numel: 46872

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   all parameters: 8525280

05/28/2023 14:08:47 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:08:47 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_hidden_size': 216, 'sample_intermediate_sizes': [384, 384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [216, 216, 216, 216, 216]}
parameter size = 8525280
best_acc = 0.5415162454873647
time_per_batch_infer = 4.205 ms
infer_cnt = 63
**************E*************

05/28/2023 14:08:47 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 372, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [372, 372, 372]}
05/28/2023 14:08:47 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:08:47 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:08:47 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:47 - INFO - __main__ -   guid: train-0
05/28/2023 14:08:47 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:08:47 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:47 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:47 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:47 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:47 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:49 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:08:49 - INFO - __main__ -   *** Example ***
05/28/2023 14:08:49 - INFO - __main__ -   guid: dev-0
05/28/2023 14:08:49 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:08:49 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:49 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:49 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:08:49 - INFO - __main__ -   label: not_entailment
05/28/2023 14:08:49 - INFO - __main__ -   label_id: 1
05/28/2023 14:08:49 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:08:49 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:08:50 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:08:50 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:08:50 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:08:50 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:08:50 - INFO - __main__ -     Batch size = 32
05/28/2023 14:08:50 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.40it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.19it/s][A05/28/2023 14:08:50 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:50 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:08:50 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:50 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 138.90it/s]
05/28/2023 14:08:50 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:50 - INFO - __main__ -    dev: acc = 0.4693140794223827
05/28/2023 14:08:50 - INFO - __main__ -    dev: eval_loss = 0.6937141882048713
05/28/2023 14:08:50 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:50 - INFO - __main__ -    dev: infer_time = 2.858222222222222
05/28/2023 14:08:50 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:50 - INFO - __main__ -     acc = 0.4693140794223827
05/28/2023 14:08:50 - INFO - __main__ -     cls_loss = 0.6968696978357103
05/28/2023 14:08:50 - INFO - __main__ -     eval_loss = 0.6937141882048713
05/28/2023 14:08:50 - INFO - __main__ -     global_step = 9
05/28/2023 14:08:50 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:50 - INFO - __main__ -     infer_time = 2.858222222222222
05/28/2023 14:08:50 - INFO - __main__ -     loss = 0.6968696978357103
05/28/2023 14:08:50 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.02it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:10,  6.02it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.28it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.70it/s][A05/28/2023 14:08:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:52 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:08:52 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.17it/s]
05/28/2023 14:08:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:52 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 14:08:52 - INFO - __main__ -    dev: eval_loss = 0.6944945587052239
05/28/2023 14:08:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:52 - INFO - __main__ -    dev: infer_time = 2.8422222222222224
05/28/2023 14:08:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:52 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 14:08:52 - INFO - __main__ -     cls_loss = 0.6999259120539615
05/28/2023 14:08:52 - INFO - __main__ -     eval_loss = 0.6944945587052239
05/28/2023 14:08:52 - INFO - __main__ -     global_step = 19
05/28/2023 14:08:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:52 - INFO - __main__ -     infer_time = 2.8422222222222224
05/28/2023 14:08:52 - INFO - __main__ -     loss = 0.6999259120539615
05/28/2023 14:08:52 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.45it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:08,  6.03it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.89it/s][A05/28/2023 14:08:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:54 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:08:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.30it/s]
05/28/2023 14:08:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:54 - INFO - __main__ -    dev: acc = 0.48014440433212996
05/28/2023 14:08:54 - INFO - __main__ -    dev: eval_loss = 0.6943130956755744
05/28/2023 14:08:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:54 - INFO - __main__ -    dev: infer_time = 2.8342222222222215
05/28/2023 14:08:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:54 - INFO - __main__ -     acc = 0.48014440433212996
05/28/2023 14:08:54 - INFO - __main__ -     cls_loss = 0.6978055690896923
05/28/2023 14:08:54 - INFO - __main__ -     eval_loss = 0.6943130956755744
05/28/2023 14:08:54 - INFO - __main__ -     global_step = 29
05/28/2023 14:08:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:54 - INFO - __main__ -     infer_time = 2.8342222222222215
05/28/2023 14:08:54 - INFO - __main__ -     loss = 0.6978055690896923

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.32it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.53it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.76it/s][A05/28/2023 14:08:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:54 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:08:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.51it/s]
05/28/2023 14:08:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:54 - INFO - __main__ -    dev: acc = 0.5054151624548736
05/28/2023 14:08:54 - INFO - __main__ -    dev: eval_loss = 0.6914816366301643
05/28/2023 14:08:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:54 - INFO - __main__ -    dev: infer_time = 2.860222222222222
05/28/2023 14:08:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:54 - INFO - __main__ -     acc = 0.5054151624548736
05/28/2023 14:08:54 - INFO - __main__ -     cls_loss = 0.6967738240193098
05/28/2023 14:08:54 - INFO - __main__ -     eval_loss = 0.6914816366301643
05/28/2023 14:08:54 - INFO - __main__ -     global_step = 39
05/28/2023 14:08:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:54 - INFO - __main__ -     infer_time = 2.860222222222222
05/28/2023 14:08:54 - INFO - __main__ -     loss = 0.6967738240193098
05/28/2023 14:08:54 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:05<00:07,  4.97it/s][A
Iteration:  54%|#####3    | 42/78 [00:05<00:05,  6.54it/s][A
Iteration:  58%|#####7    | 45/78 [00:05<00:03,  8.40it/s][A
Iteration:  62%|######1   | 48/78 [00:06<00:02, 10.50it/s][A05/28/2023 14:08:56 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:56 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:08:56 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:56 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.47it/s]
05/28/2023 14:08:56 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:56 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:56 - INFO - __main__ -    dev: eval_loss = 0.6899195777045356
05/28/2023 14:08:56 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:56 - INFO - __main__ -    dev: infer_time = 2.8296666666666663
05/28/2023 14:08:56 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:56 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:56 - INFO - __main__ -     cls_loss = 0.6955509137134163
05/28/2023 14:08:56 - INFO - __main__ -     eval_loss = 0.6899195777045356
05/28/2023 14:08:56 - INFO - __main__ -     global_step = 49
05/28/2023 14:08:56 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:56 - INFO - __main__ -     infer_time = 2.8296666666666663
05/28/2023 14:08:56 - INFO - __main__ -     loss = 0.6955509137134163
05/28/2023 14:08:56 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  65%|######5   | 51/78 [00:07<00:05,  4.62it/s][A
Iteration:  69%|######9   | 54/78 [00:07<00:03,  6.12it/s][A
Iteration:  73%|#######3  | 57/78 [00:07<00:02,  7.87it/s][A05/28/2023 14:08:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:58 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:08:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.54it/s]
05/28/2023 14:08:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:58 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:08:58 - INFO - __main__ -    dev: eval_loss = 0.6906324691242642
05/28/2023 14:08:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:58 - INFO - __main__ -    dev: infer_time = 2.8307777777777776
05/28/2023 14:08:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:58 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:08:58 - INFO - __main__ -     cls_loss = 0.6948080729630034
05/28/2023 14:08:58 - INFO - __main__ -     eval_loss = 0.6906324691242642
05/28/2023 14:08:58 - INFO - __main__ -     global_step = 59
05/28/2023 14:08:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:58 - INFO - __main__ -     infer_time = 2.8307777777777776
05/28/2023 14:08:58 - INFO - __main__ -     loss = 0.6948080729630034

Iteration:  77%|#######6  | 60/78 [00:08<00:01,  9.21it/s][A
Iteration:  81%|########  | 63/78 [00:08<00:01, 11.38it/s][A
Iteration:  85%|########4 | 66/78 [00:08<00:00, 13.58it/s][A05/28/2023 14:08:58 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:08:58 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:08:58 - INFO - __main__ -     Num examples = 277
05/28/2023 14:08:58 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 139.68it/s]
05/28/2023 14:08:58 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:08:58 - INFO - __main__ -    dev: acc = 0.5487364620938628
05/28/2023 14:08:58 - INFO - __main__ -    dev: eval_loss = 0.6890291902754042
05/28/2023 14:08:58 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:08:58 - INFO - __main__ -    dev: infer_time = 2.8234444444444446
05/28/2023 14:08:58 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:08:58 - INFO - __main__ -     acc = 0.5487364620938628
05/28/2023 14:08:58 - INFO - __main__ -     cls_loss = 0.6952187600343124
05/28/2023 14:08:58 - INFO - __main__ -     eval_loss = 0.6890291902754042
05/28/2023 14:08:58 - INFO - __main__ -     global_step = 69
05/28/2023 14:08:58 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:08:58 - INFO - __main__ -     infer_time = 2.8234444444444446
05/28/2023 14:08:58 - INFO - __main__ -     loss = 0.6952187600343124
05/28/2023 14:08:58 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  88%|########8 | 69/78 [00:09<00:01,  4.98it/s][A
Iteration:  92%|#########2| 72/78 [00:09<00:00,  6.55it/s][A
Iteration:  96%|#########6| 75/78 [00:10<00:00,  8.41it/s][AIteration: 100%|##########| 78/78 [00:10<00:00,  7.72it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.11s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.11s/it]
05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   w_emb: 11354184

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   p_emb: 190464

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   t_emb: 744

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   ln_emb: 744

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   query_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   key_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   value_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   self_numel: 416268

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   output_numel: 139500

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 322272

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 321780

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   attention_numel: 555768

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 322272

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   output_numel: 322524

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   query_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   key_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   value_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   self_numel: 416268

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   output_numel: 139500

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 322272

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 321780

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   attention_numel: 555768

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 322272

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   output_numel: 322524

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   query_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   key_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   value_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   self_numel: 416268

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   output_numel: 139500

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 322272

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 321780

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   attention_numel: 555768

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   intermediate_numel: 322272

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   output_numel: 322524

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   layer_numel: 3601692
05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   dense_numel: 138756
05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   emb_numel: 11546136

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   encoder_numel: 3601692

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   pooler_numel: 138756

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   all parameters: 15286584

05/28/2023 14:09:00 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:00 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 372, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [372, 372, 372]}
parameter size = 15286584
best_acc = 0.5487364620938628
time_per_batch_infer = 2.840 ms
infer_cnt = 63
**************E*************

05/28/2023 14:09:00 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
05/28/2023 14:09:00 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:09:00 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:09:00 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:00 - INFO - __main__ -   guid: train-0
05/28/2023 14:09:00 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:09:00 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:00 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:00 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:00 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:00 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:02 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:09:02 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:02 - INFO - __main__ -   guid: dev-0
05/28/2023 14:09:02 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:09:02 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:02 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:02 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:02 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:02 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:02 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:09:02 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:09:02 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:09:02 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:09:02 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:09:02 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:09:02 - INFO - __main__ -     Batch size = 32
05/28/2023 14:09:02 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.40it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.31it/s][A05/28/2023 14:09:03 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:03 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:09:03 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:03 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 163.06it/s]
05/28/2023 14:09:03 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:03 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:09:03 - INFO - __main__ -    dev: eval_loss = 0.6916037466790941
05/28/2023 14:09:03 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:03 - INFO - __main__ -    dev: infer_time = 3.500222222222222
05/28/2023 14:09:03 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:03 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:09:03 - INFO - __main__ -     cls_loss = 0.6938754717508951
05/28/2023 14:09:03 - INFO - __main__ -     eval_loss = 0.6916037466790941
05/28/2023 14:09:03 - INFO - __main__ -     global_step = 9
05/28/2023 14:09:03 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:03 - INFO - __main__ -     infer_time = 3.500222222222222
05/28/2023 14:09:03 - INFO - __main__ -     loss = 0.6938754717508951
05/28/2023 14:09:03 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.98it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.91it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.17it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.58it/s][A05/28/2023 14:09:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:05 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:09:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 162.63it/s]
05/28/2023 14:09:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:05 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:05 - INFO - __main__ -    dev: eval_loss = 0.691725061999427
05/28/2023 14:09:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:05 - INFO - __main__ -    dev: infer_time = 3.560888888888888
05/28/2023 14:09:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:05 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:05 - INFO - __main__ -     cls_loss = 0.6951531485507363
05/28/2023 14:09:05 - INFO - __main__ -     eval_loss = 0.691725061999427
05/28/2023 14:09:05 - INFO - __main__ -     global_step = 19
05/28/2023 14:09:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:05 - INFO - __main__ -     infer_time = 3.560888888888888
05/28/2023 14:09:05 - INFO - __main__ -     loss = 0.6951531485507363
05/28/2023 14:09:05 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.47it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:08,  6.06it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.95it/s][A05/28/2023 14:09:06 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:06 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:09:06 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:06 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 162.92it/s]
05/28/2023 14:09:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:06 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:06 - INFO - __main__ -    dev: eval_loss = 0.6910156077808804
05/28/2023 14:09:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:06 - INFO - __main__ -    dev: infer_time = 3.531
05/28/2023 14:09:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:06 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:06 - INFO - __main__ -     cls_loss = 0.6942729107264815
05/28/2023 14:09:06 - INFO - __main__ -     eval_loss = 0.6910156077808804
05/28/2023 14:09:06 - INFO - __main__ -     global_step = 29
05/28/2023 14:09:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:06 - INFO - __main__ -     infer_time = 3.531
05/28/2023 14:09:06 - INFO - __main__ -     loss = 0.6942729107264815

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.37it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.61it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.85it/s][A05/28/2023 14:09:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:07 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:09:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 160.87it/s]
05/28/2023 14:09:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:07 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:07 - INFO - __main__ -    dev: eval_loss = 0.6925130751397874
05/28/2023 14:09:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:07 - INFO - __main__ -    dev: infer_time = 3.6526666666666663
05/28/2023 14:09:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:07 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:07 - INFO - __main__ -     cls_loss = 0.6938902475895026
05/28/2023 14:09:07 - INFO - __main__ -     eval_loss = 0.6925130751397874
05/28/2023 14:09:07 - INFO - __main__ -     global_step = 39
05/28/2023 14:09:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:07 - INFO - __main__ -     infer_time = 3.6526666666666663
05/28/2023 14:09:07 - INFO - __main__ -     loss = 0.6938902475895026

Iteration:  50%|#####     | 39/78 [00:04<00:02, 14.50it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 16.37it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 18.26it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 19.80it/s][A05/28/2023 14:09:07 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:07 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:09:07 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:07 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 160.06it/s]
05/28/2023 14:09:07 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:07 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:09:07 - INFO - __main__ -    dev: eval_loss = 0.6943302353223165
05/28/2023 14:09:07 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:07 - INFO - __main__ -    dev: infer_time = 3.46
05/28/2023 14:09:07 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:07 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:09:07 - INFO - __main__ -     cls_loss = 0.69375208689242
05/28/2023 14:09:07 - INFO - __main__ -     eval_loss = 0.6943302353223165
05/28/2023 14:09:07 - INFO - __main__ -     global_step = 49
05/28/2023 14:09:07 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:07 - INFO - __main__ -     infer_time = 3.46
05/28/2023 14:09:07 - INFO - __main__ -     loss = 0.69375208689242

Iteration:  65%|######5   | 51/78 [00:04<00:01, 18.69it/s][A
Iteration:  69%|######9   | 54/78 [00:05<00:01, 20.13it/s][A
Iteration:  73%|#######3  | 57/78 [00:05<00:00, 21.36it/s][A05/28/2023 14:09:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:08 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:09:08 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 162.12it/s]
05/28/2023 14:09:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:08 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:08 - INFO - __main__ -    dev: eval_loss = 0.7006427778138055
05/28/2023 14:09:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:08 - INFO - __main__ -    dev: infer_time = 3.619888888888889
05/28/2023 14:09:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:08 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:08 - INFO - __main__ -     cls_loss = 0.6928550469673286
05/28/2023 14:09:08 - INFO - __main__ -     eval_loss = 0.7006427778138055
05/28/2023 14:09:08 - INFO - __main__ -     global_step = 59
05/28/2023 14:09:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:08 - INFO - __main__ -     infer_time = 3.619888888888889
05/28/2023 14:09:08 - INFO - __main__ -     loss = 0.6928550469673286

Iteration:  77%|#######6  | 60/78 [00:05<00:00, 19.49it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 20.90it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 22.01it/s][A05/28/2023 14:09:08 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:08 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:09:08 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:08 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 161.87it/s]
05/28/2023 14:09:08 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:08 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:08 - INFO - __main__ -    dev: eval_loss = 0.7005428936746385
05/28/2023 14:09:08 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:08 - INFO - __main__ -    dev: infer_time = 3.6402222222222225
05/28/2023 14:09:08 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:08 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:08 - INFO - __main__ -     cls_loss = 0.6930221690647844
05/28/2023 14:09:08 - INFO - __main__ -     eval_loss = 0.7005428936746385
05/28/2023 14:09:08 - INFO - __main__ -     global_step = 69
05/28/2023 14:09:08 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:08 - INFO - __main__ -     infer_time = 3.6402222222222225
05/28/2023 14:09:08 - INFO - __main__ -     loss = 0.6930221690647844

Iteration:  88%|########8 | 69/78 [00:05<00:00, 19.87it/s][A
Iteration:  92%|#########2| 72/78 [00:05<00:00, 20.88it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 21.88it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.62it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.18s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.18s/it]
05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   w_emb: 7691544

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   p_emb: 129024

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   t_emb: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ln_emb: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121440

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121212

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 121440

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   output_numel: 121716

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121440

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121212

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 121440

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   output_numel: 121716

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121440

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121212

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 121440

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   output_numel: 121716

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   query_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   key_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   value_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   self_numel: 191268

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   output_numel: 64260

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121440

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 121212

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ln_numel: 504

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   attention_numel: 255528

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   intermediate_numel: 121440

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   output_numel: 121716

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   layer_numel: 1994736
05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   dense_numel: 63756
05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   emb_numel: 7821576

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   encoder_numel: 1994736

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   pooler_numel: 63756

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   all parameters: 9880068

05/28/2023 14:09:09 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:09 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [480, 480, 480, 480], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
parameter size = 9880068
best_acc = 0.5270758122743683
time_per_batch_infer = 3.566 ms
infer_cnt = 63
**************E*************

05/28/2023 14:09:09 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [288, 288, 288, 288]}
05/28/2023 14:09:09 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:09:09 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:09:09 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:09 - INFO - __main__ -   guid: train-0
05/28/2023 14:09:09 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:09:09 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:09 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:09 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:09 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:09 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:11 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:09:11 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:11 - INFO - __main__ -   guid: dev-0
05/28/2023 14:09:11 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:09:11 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:11 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:11 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:11 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:11 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:11 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:09:11 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:09:11 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:09:11 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:09:11 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:09:11 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:09:11 - INFO - __main__ -     Batch size = 32
05/28/2023 14:09:11 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 23.28it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 24.38it/s][A05/28/2023 14:09:12 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:12 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:09:12 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:12 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.03it/s]
05/28/2023 14:09:12 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:12 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:12 - INFO - __main__ -    dev: eval_loss = 0.7126789622836642
05/28/2023 14:09:12 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:12 - INFO - __main__ -    dev: infer_time = 3.608222222222222
05/28/2023 14:09:12 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:12 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:12 - INFO - __main__ -     cls_loss = 0.6942283246252272
05/28/2023 14:09:12 - INFO - __main__ -     eval_loss = 0.7126789622836642
05/28/2023 14:09:12 - INFO - __main__ -     global_step = 9
05/28/2023 14:09:12 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:12 - INFO - __main__ -     infer_time = 3.608222222222222
05/28/2023 14:09:12 - INFO - __main__ -     loss = 0.6942283246252272
05/28/2023 14:09:12 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:16,  4.06it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  6.00it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.26it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.67it/s][A05/28/2023 14:09:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:13 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:09:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 174.22it/s]
05/28/2023 14:09:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:14 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:14 - INFO - __main__ -    dev: eval_loss = 0.6974335047933791
05/28/2023 14:09:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:14 - INFO - __main__ -    dev: infer_time = 3.4427777777777773
05/28/2023 14:09:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:14 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:14 - INFO - __main__ -     cls_loss = 0.699596781479685
05/28/2023 14:09:14 - INFO - __main__ -     eval_loss = 0.6974335047933791
05/28/2023 14:09:14 - INFO - __main__ -     global_step = 19
05/28/2023 14:09:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:14 - INFO - __main__ -     infer_time = 3.4427777777777773
05/28/2023 14:09:14 - INFO - __main__ -     loss = 0.699596781479685
05/28/2023 14:09:14 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.31it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:09,  5.87it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.70it/s][A05/28/2023 14:09:15 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:15 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:09:15 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:15 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 173.95it/s]
05/28/2023 14:09:15 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:15 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:15 - INFO - __main__ -    dev: eval_loss = 0.6913561423619589
05/28/2023 14:09:15 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:15 - INFO - __main__ -    dev: infer_time = 3.474
05/28/2023 14:09:15 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:15 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:15 - INFO - __main__ -     cls_loss = 0.6968878959787304
05/28/2023 14:09:15 - INFO - __main__ -     eval_loss = 0.6913561423619589
05/28/2023 14:09:15 - INFO - __main__ -     global_step = 29
05/28/2023 14:09:15 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:15 - INFO - __main__ -     infer_time = 3.474
05/28/2023 14:09:15 - INFO - __main__ -     loss = 0.6968878959787304

Iteration:  38%|###8      | 30/78 [00:04<00:05,  9.20it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.43it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.67it/s][A05/28/2023 14:09:16 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:16 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:09:16 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:16 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 174.66it/s]
05/28/2023 14:09:16 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:16 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:16 - INFO - __main__ -    dev: eval_loss = 0.6996568904982673
05/28/2023 14:09:16 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:16 - INFO - __main__ -    dev: infer_time = 3.4405555555555556
05/28/2023 14:09:16 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:16 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:16 - INFO - __main__ -     cls_loss = 0.6960294628754641
05/28/2023 14:09:16 - INFO - __main__ -     eval_loss = 0.6996568904982673
05/28/2023 14:09:16 - INFO - __main__ -     global_step = 39
05/28/2023 14:09:16 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:16 - INFO - __main__ -     infer_time = 3.4405555555555556
05/28/2023 14:09:16 - INFO - __main__ -     loss = 0.6960294628754641

Iteration:  50%|#####     | 39/78 [00:04<00:02, 14.45it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 16.51it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:01, 18.32it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 19.95it/s][A05/28/2023 14:09:16 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:16 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:09:16 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:16 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 174.78it/s]
05/28/2023 14:09:16 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:16 - INFO - __main__ -    dev: acc = 0.5415162454873647
05/28/2023 14:09:16 - INFO - __main__ -    dev: eval_loss = 0.6929369502597384
05/28/2023 14:09:16 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:16 - INFO - __main__ -    dev: infer_time = 3.4531111111111112
05/28/2023 14:09:16 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:16 - INFO - __main__ -     acc = 0.5415162454873647
05/28/2023 14:09:16 - INFO - __main__ -     cls_loss = 0.6964660311231807
05/28/2023 14:09:16 - INFO - __main__ -     eval_loss = 0.6929369502597384
05/28/2023 14:09:16 - INFO - __main__ -     global_step = 49
05/28/2023 14:09:16 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:16 - INFO - __main__ -     infer_time = 3.4531111111111112
05/28/2023 14:09:16 - INFO - __main__ -     loss = 0.6964660311231807
05/28/2023 14:09:16 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  65%|######5   | 51/78 [00:06<00:05,  5.23it/s][A
Iteration:  69%|######9   | 54/78 [00:06<00:03,  6.87it/s][A
Iteration:  73%|#######3  | 57/78 [00:06<00:02,  8.77it/s][A05/28/2023 14:09:18 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:18 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:09:18 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:18 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 174.16it/s]
05/28/2023 14:09:18 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:18 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:18 - INFO - __main__ -    dev: eval_loss = 0.6914178861512078
05/28/2023 14:09:18 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:18 - INFO - __main__ -    dev: infer_time = 3.4298888888888888
05/28/2023 14:09:18 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:18 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:18 - INFO - __main__ -     cls_loss = 0.6959599973791737
05/28/2023 14:09:18 - INFO - __main__ -     eval_loss = 0.6914178861512078
05/28/2023 14:09:18 - INFO - __main__ -     global_step = 59
05/28/2023 14:09:18 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:18 - INFO - __main__ -     infer_time = 3.4298888888888888
05/28/2023 14:09:18 - INFO - __main__ -     loss = 0.6959599973791737

Iteration:  77%|#######6  | 60/78 [00:06<00:01, 10.21it/s][A
Iteration:  81%|########  | 63/78 [00:06<00:01, 12.42it/s][A
Iteration:  85%|########4 | 66/78 [00:07<00:00, 14.60it/s][A05/28/2023 14:09:19 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:19 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:09:19 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:19 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 174.46it/s]
05/28/2023 14:09:19 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:19 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:19 - INFO - __main__ -    dev: eval_loss = 0.6917283799913194
05/28/2023 14:09:19 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:19 - INFO - __main__ -    dev: infer_time = 3.4527777777777775
05/28/2023 14:09:19 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:19 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:19 - INFO - __main__ -     cls_loss = 0.6957193733989329
05/28/2023 14:09:19 - INFO - __main__ -     eval_loss = 0.6917283799913194
05/28/2023 14:09:19 - INFO - __main__ -     global_step = 69
05/28/2023 14:09:19 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:19 - INFO - __main__ -     infer_time = 3.4527777777777775
05/28/2023 14:09:19 - INFO - __main__ -     loss = 0.6957193733989329

Iteration:  88%|########8 | 69/78 [00:07<00:00, 15.14it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:00, 17.08it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00, 18.86it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.30it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.58s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.58s/it]
05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   w_emb: 8790336

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   p_emb: 147456

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   t_emb: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ln_emb: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 55488

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 55584

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   intermediate_numel: 55488

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   output_numel: 56160

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 55488

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 55584

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   intermediate_numel: 55488

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   output_numel: 56160

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 55488

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 55584

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   intermediate_numel: 55488

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   output_numel: 56160

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 55488

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 55584

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   intermediate_numel: 55488

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   output_numel: 56160

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   layer_numel: 1780608
05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   dense_numel: 83232
05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   emb_numel: 8938944

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   encoder_numel: 1780608

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   pooler_numel: 83232

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   all parameters: 10802784

05/28/2023 14:09:19 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:19 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_qkv_sizes': [288, 288, 288, 288]}
parameter size = 10802784
best_acc = 0.5415162454873647
time_per_batch_infer = 3.472 ms
infer_cnt = 63
**************E*************

05/28/2023 14:09:19 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 300, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_qkv_sizes': [300, 300, 300, 300]}
05/28/2023 14:09:19 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:09:19 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:09:19 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:19 - INFO - __main__ -   guid: train-0
05/28/2023 14:09:19 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:09:19 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:19 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:19 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:19 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:19 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:21 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:09:21 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:21 - INFO - __main__ -   guid: dev-0
05/28/2023 14:09:21 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:09:21 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:21 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:21 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:21 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:21 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:21 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:09:21 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:09:22 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:09:22 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:09:22 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:09:22 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:09:22 - INFO - __main__ -     Batch size = 32
05/28/2023 14:09:22 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.84it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.51it/s][A05/28/2023 14:09:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:22 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:09:22 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.10it/s]
05/28/2023 14:09:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:22 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:22 - INFO - __main__ -    dev: eval_loss = 0.6916661461194357
05/28/2023 14:09:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:22 - INFO - __main__ -    dev: infer_time = 3.478555555555556
05/28/2023 14:09:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:22 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:22 - INFO - __main__ -     cls_loss = 0.6950972080230713
05/28/2023 14:09:22 - INFO - __main__ -     eval_loss = 0.6916661461194357
05/28/2023 14:09:22 - INFO - __main__ -     global_step = 9
05/28/2023 14:09:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:22 - INFO - __main__ -     infer_time = 3.478555555555556
05/28/2023 14:09:22 - INFO - __main__ -     loss = 0.6950972080230713
05/28/2023 14:09:22 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.97it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.85it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.01it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.30it/s][A05/28/2023 14:09:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:24 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:09:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.08it/s]
05/28/2023 14:09:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:24 - INFO - __main__ -    dev: acc = 0.48736462093862815
05/28/2023 14:09:24 - INFO - __main__ -    dev: eval_loss = 0.6947347852918837
05/28/2023 14:09:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:24 - INFO - __main__ -    dev: infer_time = 3.5085555555555556
05/28/2023 14:09:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:24 - INFO - __main__ -     acc = 0.48736462093862815
05/28/2023 14:09:24 - INFO - __main__ -     cls_loss = 0.6938596587432059
05/28/2023 14:09:24 - INFO - __main__ -     eval_loss = 0.6947347852918837
05/28/2023 14:09:24 - INFO - __main__ -     global_step = 19
05/28/2023 14:09:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:24 - INFO - __main__ -     infer_time = 3.5085555555555556
05/28/2023 14:09:24 - INFO - __main__ -     loss = 0.6938596587432059

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.25it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 13.28it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.18it/s][A05/28/2023 14:09:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:24 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:09:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.99it/s]
05/28/2023 14:09:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:24 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 14:09:24 - INFO - __main__ -    dev: eval_loss = 0.6926382448938158
05/28/2023 14:09:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:24 - INFO - __main__ -    dev: infer_time = 3.506777777777778
05/28/2023 14:09:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:24 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 14:09:24 - INFO - __main__ -     cls_loss = 0.6942801845484766
05/28/2023 14:09:24 - INFO - __main__ -     eval_loss = 0.6926382448938158
05/28/2023 14:09:24 - INFO - __main__ -     global_step = 29
05/28/2023 14:09:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:24 - INFO - __main__ -     infer_time = 3.506777777777778
05/28/2023 14:09:24 - INFO - __main__ -     loss = 0.6942801845484766

Iteration:  38%|###8      | 30/78 [00:02<00:03, 14.92it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 16.68it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 18.20it/s][A05/28/2023 14:09:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:25 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:09:25 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.16it/s]
05/28/2023 14:09:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:25 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:09:25 - INFO - __main__ -    dev: eval_loss = 0.6917863686879476
05/28/2023 14:09:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:25 - INFO - __main__ -    dev: infer_time = 3.5052222222222222
05/28/2023 14:09:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:25 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:09:25 - INFO - __main__ -     cls_loss = 0.6950262784957886
05/28/2023 14:09:25 - INFO - __main__ -     eval_loss = 0.6917863686879476
05/28/2023 14:09:25 - INFO - __main__ -     global_step = 39
05/28/2023 14:09:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:25 - INFO - __main__ -     infer_time = 3.5052222222222222
05/28/2023 14:09:25 - INFO - __main__ -     loss = 0.6950262784957886

Iteration:  50%|#####     | 39/78 [00:03<00:02, 16.95it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 18.37it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 19.56it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 20.53it/s][A05/28/2023 14:09:25 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:25 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:09:25 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:25 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.20it/s]
05/28/2023 14:09:25 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:25 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:09:25 - INFO - __main__ -    dev: eval_loss = 0.6952742139498392
05/28/2023 14:09:25 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:25 - INFO - __main__ -    dev: infer_time = 3.4975555555555555
05/28/2023 14:09:25 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:25 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:09:25 - INFO - __main__ -     cls_loss = 0.6947986872828736
05/28/2023 14:09:25 - INFO - __main__ -     eval_loss = 0.6952742139498392
05/28/2023 14:09:25 - INFO - __main__ -     global_step = 49
05/28/2023 14:09:25 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:25 - INFO - __main__ -     infer_time = 3.4975555555555555
05/28/2023 14:09:25 - INFO - __main__ -     loss = 0.6947986872828736

Iteration:  65%|######5   | 51/78 [00:03<00:01, 18.29it/s][A
Iteration:  69%|######9   | 54/78 [00:04<00:01, 19.63it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:01, 20.53it/s][A05/28/2023 14:09:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:26 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:09:26 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 133.09it/s]
05/28/2023 14:09:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:26 - INFO - __main__ -    dev: acc = 0.5018050541516246
05/28/2023 14:09:26 - INFO - __main__ -    dev: eval_loss = 0.6934679084353976
05/28/2023 14:09:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:26 - INFO - __main__ -    dev: infer_time = 3.4988888888888887
05/28/2023 14:09:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:26 - INFO - __main__ -     acc = 0.5018050541516246
05/28/2023 14:09:26 - INFO - __main__ -     cls_loss = 0.6948066929639396
05/28/2023 14:09:26 - INFO - __main__ -     eval_loss = 0.6934679084353976
05/28/2023 14:09:26 - INFO - __main__ -     global_step = 59
05/28/2023 14:09:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:26 - INFO - __main__ -     infer_time = 3.4988888888888887
05/28/2023 14:09:26 - INFO - __main__ -     loss = 0.6948066929639396

Iteration:  77%|#######6  | 60/78 [00:04<00:00, 18.13it/s][A
Iteration:  81%|########  | 63/78 [00:04<00:00, 19.36it/s][A
Iteration:  85%|########4 | 66/78 [00:04<00:00, 20.32it/s][A05/28/2023 14:09:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:26 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:09:26 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 132.96it/s]
05/28/2023 14:09:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:26 - INFO - __main__ -    dev: acc = 0.516245487364621
05/28/2023 14:09:26 - INFO - __main__ -    dev: eval_loss = 0.6929923892021179
05/28/2023 14:09:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:26 - INFO - __main__ -    dev: infer_time = 3.521444444444444
05/28/2023 14:09:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:26 - INFO - __main__ -     acc = 0.516245487364621
05/28/2023 14:09:26 - INFO - __main__ -     cls_loss = 0.6943318083666373
05/28/2023 14:09:26 - INFO - __main__ -     eval_loss = 0.6929923892021179
05/28/2023 14:09:26 - INFO - __main__ -     global_step = 69
05/28/2023 14:09:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:26 - INFO - __main__ -     infer_time = 3.521444444444444
05/28/2023 14:09:26 - INFO - __main__ -     loss = 0.6943318083666373

Iteration:  88%|########8 | 69/78 [00:04<00:00, 18.18it/s][A
Iteration:  92%|#########2| 72/78 [00:04<00:00, 19.14it/s][A
Iteration:  96%|#########6| 75/78 [00:05<00:00, 20.16it/s][AIteration: 100%|##########| 78/78 [00:05<00:00, 14.96it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.21s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.21s/it]
05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   w_emb: 9156600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   p_emb: 153600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   t_emb: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ln_emb: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 183008

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 182700

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   intermediate_numel: 183008

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   output_numel: 183300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 183008

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 182700

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   intermediate_numel: 183008

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   output_numel: 183300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 183008

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 182700

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   intermediate_numel: 183008

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   output_numel: 183300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   query_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   key_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   value_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   self_numel: 270900

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   output_numel: 90900

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 183008

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 182700

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ln_numel: 600

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   attention_numel: 361800

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   intermediate_numel: 183008

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   output_numel: 183300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   layer_numel: 2912432
05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   dense_numel: 90300
05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   emb_numel: 9311400

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   encoder_numel: 2912432

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   pooler_numel: 90300

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   all parameters: 12314132

05/28/2023 14:09:27 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:27 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 300, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_qkv_sizes': [300, 300, 300, 300]}
parameter size = 12314132
best_acc = 0.5270758122743683
time_per_batch_infer = 3.502 ms
infer_cnt = 63
**************E*************

05/28/2023 14:09:27 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [576, 576, 576], 'sample_qkv_sizes': [492, 492, 492]}
05/28/2023 14:09:27 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:09:27 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:09:27 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:27 - INFO - __main__ -   guid: train-0
05/28/2023 14:09:27 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:09:27 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:27 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:27 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:27 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:27 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:29 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:09:29 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:29 - INFO - __main__ -   guid: dev-0
05/28/2023 14:09:29 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:09:29 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:29 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:29 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:29 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:29 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:29 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:09:29 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:09:29 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:09:29 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:09:29 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:09:29 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:09:29 - INFO - __main__ -     Batch size = 32
05/28/2023 14:09:29 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 22.99it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 23.46it/s][A05/28/2023 14:09:30 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:30 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:09:30 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:30 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.04it/s]
05/28/2023 14:09:30 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:30 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:30 - INFO - __main__ -    dev: eval_loss = 0.6920904583401151
05/28/2023 14:09:30 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:30 - INFO - __main__ -    dev: infer_time = 2.8476666666666666
05/28/2023 14:09:30 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:30 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:30 - INFO - __main__ -     cls_loss = 0.6970772676997714
05/28/2023 14:09:30 - INFO - __main__ -     eval_loss = 0.6920904583401151
05/28/2023 14:09:30 - INFO - __main__ -     global_step = 9
05/28/2023 14:09:30 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:30 - INFO - __main__ -     infer_time = 2.8476666666666666
05/28/2023 14:09:30 - INFO - __main__ -     loss = 0.6970772676997714
05/28/2023 14:09:30 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.96it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.86it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  8.07it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.36it/s][A05/28/2023 14:09:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:32 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:09:32 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 122.84it/s]
05/28/2023 14:09:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:32 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:32 - INFO - __main__ -    dev: eval_loss = 0.6956009401215447
05/28/2023 14:09:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:32 - INFO - __main__ -    dev: infer_time = 2.865
05/28/2023 14:09:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:32 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:32 - INFO - __main__ -     cls_loss = 0.6918285802790993
05/28/2023 14:09:32 - INFO - __main__ -     eval_loss = 0.6956009401215447
05/28/2023 14:09:32 - INFO - __main__ -     global_step = 19
05/28/2023 14:09:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:32 - INFO - __main__ -     infer_time = 2.865
05/28/2023 14:09:32 - INFO - __main__ -     loss = 0.6918285802790993

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.29it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 13.56it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.59it/s][A05/28/2023 14:09:32 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:32 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:09:32 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:32 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 122.85it/s]
05/28/2023 14:09:32 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:32 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:32 - INFO - __main__ -    dev: eval_loss = 0.7136414779557122
05/28/2023 14:09:32 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:32 - INFO - __main__ -    dev: infer_time = 2.8717777777777775
05/28/2023 14:09:32 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:32 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:32 - INFO - __main__ -     cls_loss = 0.6931230631367914
05/28/2023 14:09:32 - INFO - __main__ -     eval_loss = 0.7136414779557122
05/28/2023 14:09:32 - INFO - __main__ -     global_step = 29
05/28/2023 14:09:32 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:32 - INFO - __main__ -     infer_time = 2.8717777777777775
05/28/2023 14:09:32 - INFO - __main__ -     loss = 0.6931230631367914

Iteration:  38%|###8      | 30/78 [00:02<00:03, 15.29it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 17.06it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 18.65it/s][A05/28/2023 14:09:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:33 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:09:33 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 123.02it/s]
05/28/2023 14:09:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:33 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:09:33 - INFO - __main__ -    dev: eval_loss = 0.6926388806766934
05/28/2023 14:09:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:33 - INFO - __main__ -    dev: infer_time = 2.873333333333333
05/28/2023 14:09:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:33 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:09:33 - INFO - __main__ -     cls_loss = 0.6976518676831172
05/28/2023 14:09:33 - INFO - __main__ -     eval_loss = 0.6926388806766934
05/28/2023 14:09:33 - INFO - __main__ -     global_step = 39
05/28/2023 14:09:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:33 - INFO - __main__ -     infer_time = 2.873333333333333
05/28/2023 14:09:33 - INFO - __main__ -     loss = 0.6976518676831172

Iteration:  50%|#####     | 39/78 [00:03<00:02, 17.16it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 18.66it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 20.03it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 20.93it/s][A05/28/2023 14:09:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:33 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:09:33 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 122.94it/s]
05/28/2023 14:09:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:33 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:33 - INFO - __main__ -    dev: eval_loss = 0.6904095941119723
05/28/2023 14:09:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:33 - INFO - __main__ -    dev: infer_time = 2.850444444444445
05/28/2023 14:09:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:33 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:33 - INFO - __main__ -     cls_loss = 0.6972236973898751
05/28/2023 14:09:33 - INFO - __main__ -     eval_loss = 0.6904095941119723
05/28/2023 14:09:33 - INFO - __main__ -     global_step = 49
05/28/2023 14:09:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:33 - INFO - __main__ -     infer_time = 2.850444444444445
05/28/2023 14:09:33 - INFO - __main__ -     loss = 0.6972236973898751

Iteration:  65%|######5   | 51/78 [00:03<00:01, 18.51it/s][A
Iteration:  69%|######9   | 54/78 [00:04<00:01, 19.71it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:01, 20.84it/s][A05/28/2023 14:09:34 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:34 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:09:34 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:34 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 122.93it/s]
05/28/2023 14:09:34 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:34 - INFO - __main__ -    dev: acc = 0.5667870036101083
05/28/2023 14:09:34 - INFO - __main__ -    dev: eval_loss = 0.6920821997854445
05/28/2023 14:09:34 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:34 - INFO - __main__ -    dev: infer_time = 2.871888888888889
05/28/2023 14:09:34 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:34 - INFO - __main__ -     acc = 0.5667870036101083
05/28/2023 14:09:34 - INFO - __main__ -     cls_loss = 0.6973125207222114
05/28/2023 14:09:34 - INFO - __main__ -     eval_loss = 0.6920821997854445
05/28/2023 14:09:34 - INFO - __main__ -     global_step = 59
05/28/2023 14:09:34 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:34 - INFO - __main__ -     infer_time = 2.871888888888889
05/28/2023 14:09:34 - INFO - __main__ -     loss = 0.6973125207222114
05/28/2023 14:09:34 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  77%|#######6  | 60/78 [00:05<00:03,  5.36it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:02,  6.96it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:01,  8.84it/s][A05/28/2023 14:09:35 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:35 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:09:35 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:35 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 122.73it/s]
05/28/2023 14:09:35 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:35 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:35 - INFO - __main__ -    dev: eval_loss = 0.6959407528241476
05/28/2023 14:09:35 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:35 - INFO - __main__ -    dev: infer_time = 2.866111111111111
05/28/2023 14:09:35 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:35 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:35 - INFO - __main__ -     cls_loss = 0.6965653403945591
05/28/2023 14:09:35 - INFO - __main__ -     eval_loss = 0.6959407528241476
05/28/2023 14:09:35 - INFO - __main__ -     global_step = 69
05/28/2023 14:09:35 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:35 - INFO - __main__ -     infer_time = 2.866111111111111
05/28/2023 14:09:35 - INFO - __main__ -     loss = 0.6965653403945591

Iteration:  88%|########8 | 69/78 [00:06<00:00,  9.97it/s][A
Iteration:  92%|#########2| 72/78 [00:06<00:00, 12.04it/s][A
Iteration:  96%|#########6| 75/78 [00:06<00:00, 14.17it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 12.06it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.47s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.47s/it]
05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   w_emb: 15016824

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   p_emb: 251904

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   t_emb: 984

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   ln_emb: 984

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 283968

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 283884

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 283968

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   output_numel: 284868

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 283968

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 283884

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 283968

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   output_numel: 284868

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 283968

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 283884

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   intermediate_numel: 283968

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   output_numel: 284868

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   layer_numel: 4620132
05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   dense_numel: 242556
05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   emb_numel: 15270696

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   encoder_numel: 4620132

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   pooler_numel: 242556

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   all parameters: 20133384

05/28/2023 14:09:36 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:36 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [576, 576, 576], 'sample_qkv_sizes': [492, 492, 492]}
parameter size = 20133384
best_acc = 0.5667870036101083
time_per_batch_infer = 2.864 ms
infer_cnt = 63
**************E*************

05/28/2023 14:09:36 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [256, 256, 256, 256, 256], 'sample_qkv_sizes': [264, 264, 264, 264, 264]}
05/28/2023 14:09:36 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:09:36 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:09:36 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:36 - INFO - __main__ -   guid: train-0
05/28/2023 14:09:36 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:09:36 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:36 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:36 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:36 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:36 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:38 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:09:38 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:38 - INFO - __main__ -   guid: dev-0
05/28/2023 14:09:38 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:09:38 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:38 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:38 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:38 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:38 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:09:38 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:09:38 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:09:38 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:09:38 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:09:38 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:09:38 - INFO - __main__ -     Batch size = 32
05/28/2023 14:09:38 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   3%|2         | 2/78 [00:00<00:03, 19.81it/s][A
Iteration:   6%|6         | 5/78 [00:00<00:03, 20.73it/s][A
Iteration:  10%|#         | 8/78 [00:00<00:03, 20.92it/s][A05/28/2023 14:09:39 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:39 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:09:39 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:39 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 145.79it/s]
05/28/2023 14:09:39 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:39 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:39 - INFO - __main__ -    dev: eval_loss = 0.716729031668769
05/28/2023 14:09:39 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:39 - INFO - __main__ -    dev: infer_time = 4.197111111111112
05/28/2023 14:09:39 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:39 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:39 - INFO - __main__ -     cls_loss = 0.6917140748765733
05/28/2023 14:09:39 - INFO - __main__ -     eval_loss = 0.716729031668769
05/28/2023 14:09:39 - INFO - __main__ -     global_step = 9
05/28/2023 14:09:39 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:39 - INFO - __main__ -     infer_time = 4.197111111111112
05/28/2023 14:09:39 - INFO - __main__ -     loss = 0.6917140748765733
05/28/2023 14:09:39 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  14%|#4        | 11/78 [00:01<00:15,  4.20it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:10,  5.99it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:07,  7.95it/s][A05/28/2023 14:09:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:41 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:09:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.18it/s]
05/28/2023 14:09:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:41 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:41 - INFO - __main__ -    dev: eval_loss = 0.6922161711586846
05/28/2023 14:09:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:41 - INFO - __main__ -    dev: infer_time = 4.177888888888889
05/28/2023 14:09:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:41 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:41 - INFO - __main__ -     cls_loss = 0.6996502499831351
05/28/2023 14:09:41 - INFO - __main__ -     eval_loss = 0.6922161711586846
05/28/2023 14:09:41 - INFO - __main__ -     global_step = 19
05/28/2023 14:09:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:41 - INFO - __main__ -     infer_time = 4.177888888888889
05/28/2023 14:09:41 - INFO - __main__ -     loss = 0.6996502499831351
05/28/2023 14:09:41 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  24%|##4       | 19/78 [00:03<00:16,  3.54it/s][A
Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.49it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:08,  6.23it/s][A
Iteration:  35%|###4      | 27/78 [00:04<00:06,  8.18it/s][A05/28/2023 14:09:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:43 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:09:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.33it/s]
05/28/2023 14:09:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:43 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:43 - INFO - __main__ -    dev: eval_loss = 0.6946143706639608
05/28/2023 14:09:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:43 - INFO - __main__ -    dev: infer_time = 4.2187777777777775
05/28/2023 14:09:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:43 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:43 - INFO - __main__ -     cls_loss = 0.6999681674200913
05/28/2023 14:09:43 - INFO - __main__ -     eval_loss = 0.6946143706639608
05/28/2023 14:09:43 - INFO - __main__ -     global_step = 29
05/28/2023 14:09:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:43 - INFO - __main__ -     infer_time = 4.2187777777777775
05/28/2023 14:09:43 - INFO - __main__ -     loss = 0.6999681674200913

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.89it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 11.05it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 13.11it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.94it/s][A05/28/2023 14:09:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:43 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:09:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.80it/s]
05/28/2023 14:09:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:43 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:43 - INFO - __main__ -    dev: eval_loss = 0.6977611051665412
05/28/2023 14:09:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:43 - INFO - __main__ -    dev: infer_time = 4.163444444444444
05/28/2023 14:09:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:43 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:43 - INFO - __main__ -     cls_loss = 0.6982751916616391
05/28/2023 14:09:43 - INFO - __main__ -     eval_loss = 0.6977611051665412
05/28/2023 14:09:43 - INFO - __main__ -     global_step = 39
05/28/2023 14:09:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:43 - INFO - __main__ -     infer_time = 4.163444444444444
05/28/2023 14:09:43 - INFO - __main__ -     loss = 0.6982751916616391

Iteration:  51%|#####1    | 40/78 [00:04<00:02, 14.01it/s][A
Iteration:  55%|#####5    | 43/78 [00:05<00:02, 15.81it/s][A
Iteration:  59%|#####8    | 46/78 [00:05<00:01, 17.24it/s][A05/28/2023 14:09:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:44 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:09:44 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.99it/s]
05/28/2023 14:09:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:44 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:44 - INFO - __main__ -    dev: eval_loss = 0.6943482160568237
05/28/2023 14:09:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:44 - INFO - __main__ -    dev: infer_time = 4.163666666666667
05/28/2023 14:09:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:44 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:44 - INFO - __main__ -     cls_loss = 0.6982046888799084
05/28/2023 14:09:44 - INFO - __main__ -     eval_loss = 0.6943482160568237
05/28/2023 14:09:44 - INFO - __main__ -     global_step = 49
05/28/2023 14:09:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:44 - INFO - __main__ -     infer_time = 4.163666666666667
05/28/2023 14:09:44 - INFO - __main__ -     loss = 0.6982046888799084

Iteration:  63%|######2   | 49/78 [00:05<00:01, 16.09it/s][A
Iteration:  67%|######6   | 52/78 [00:05<00:01, 17.24it/s][A
Iteration:  71%|#######   | 55/78 [00:05<00:01, 18.31it/s][A
Iteration:  74%|#######4  | 58/78 [00:05<00:01, 19.07it/s][A05/28/2023 14:09:44 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:44 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:09:44 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:44 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.94it/s]
05/28/2023 14:09:44 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:44 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:44 - INFO - __main__ -    dev: eval_loss = 0.6921344465679593
05/28/2023 14:09:44 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:44 - INFO - __main__ -    dev: infer_time = 4.155666666666666
05/28/2023 14:09:44 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:44 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:44 - INFO - __main__ -     cls_loss = 0.6973723658060623
05/28/2023 14:09:44 - INFO - __main__ -     eval_loss = 0.6921344465679593
05/28/2023 14:09:44 - INFO - __main__ -     global_step = 59
05/28/2023 14:09:44 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:44 - INFO - __main__ -     infer_time = 4.155666666666666
05/28/2023 14:09:44 - INFO - __main__ -     loss = 0.6973723658060623

Iteration:  77%|#######6  | 60/78 [00:05<00:01, 16.93it/s][A
Iteration:  81%|########  | 63/78 [00:06<00:00, 18.04it/s][A
Iteration:  85%|########4 | 66/78 [00:06<00:00, 18.87it/s][A05/28/2023 14:09:45 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:45 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:09:45 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:45 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.13it/s]
05/28/2023 14:09:45 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:45 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:45 - INFO - __main__ -    dev: eval_loss = 0.6920092635684543
05/28/2023 14:09:45 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:45 - INFO - __main__ -    dev: infer_time = 4.1579999999999995
05/28/2023 14:09:45 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:45 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:45 - INFO - __main__ -     cls_loss = 0.6966772157212963
05/28/2023 14:09:45 - INFO - __main__ -     eval_loss = 0.6920092635684543
05/28/2023 14:09:45 - INFO - __main__ -     global_step = 69
05/28/2023 14:09:45 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:45 - INFO - __main__ -     infer_time = 4.1579999999999995
05/28/2023 14:09:45 - INFO - __main__ -     loss = 0.6966772157212963

Iteration:  88%|########8 | 69/78 [00:06<00:00, 17.22it/s][A
Iteration:  91%|#########1| 71/78 [00:06<00:00, 17.59it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 18.73it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 19.49it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.40it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.84s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.84s/it]
05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   w_emb: 8057808

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   p_emb: 135168

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   t_emb: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_emb: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67848

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 68376

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67848

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 68376

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67848

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 68376

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67848

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 68376

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   query_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   key_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   value_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   self_numel: 209880

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 70488

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 67848

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ln_numel: 528

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   attention_numel: 280368

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   intermediate_numel: 67840

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   output_numel: 68376

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   layer_numel: 2082920
05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   dense_numel: 69960
05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   emb_numel: 8194032

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   encoder_numel: 2082920

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   pooler_numel: 69960

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   all parameters: 10346912

05/28/2023 14:09:45 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:45 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [256, 256, 256, 256, 256], 'sample_qkv_sizes': [264, 264, 264, 264, 264]}
parameter size = 10346912
best_acc = 0.5270758122743683
time_per_batch_infer = 4.176 ms
infer_cnt = 63
**************E*************

05/28/2023 14:09:45 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [800, 800, 800], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
05/28/2023 14:09:45 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:09:45 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:09:45 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:45 - INFO - __main__ -   guid: train-0
05/28/2023 14:09:45 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:09:45 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:45 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:45 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:45 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:45 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:47 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:09:47 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:47 - INFO - __main__ -   guid: dev-0
05/28/2023 14:09:47 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:09:47 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:47 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:47 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:47 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:47 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:47 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:09:47 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:09:48 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:09:48 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:09:48 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:09:48 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:09:48 - INFO - __main__ -     Batch size = 32
05/28/2023 14:09:48 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.58it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.96it/s][A05/28/2023 14:09:48 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:48 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:09:48 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:48 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 118.66it/s]
05/28/2023 14:09:48 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:48 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:09:48 - INFO - __main__ -    dev: eval_loss = 0.6897598306337992
05/28/2023 14:09:48 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:48 - INFO - __main__ -    dev: infer_time = 2.833666666666667
05/28/2023 14:09:48 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:48 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:09:48 - INFO - __main__ -     cls_loss = 0.696986112329695
05/28/2023 14:09:48 - INFO - __main__ -     eval_loss = 0.6897598306337992
05/28/2023 14:09:48 - INFO - __main__ -     global_step = 9
05/28/2023 14:09:48 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:48 - INFO - __main__ -     infer_time = 2.833666666666667
05/28/2023 14:09:48 - INFO - __main__ -     loss = 0.696986112329695
05/28/2023 14:09:48 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.93it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.83it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  7.98it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.26it/s][A05/28/2023 14:09:50 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:50 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:09:50 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:50 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 118.69it/s]
05/28/2023 14:09:50 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:50 - INFO - __main__ -    dev: acc = 0.49458483754512633
05/28/2023 14:09:50 - INFO - __main__ -    dev: eval_loss = 0.692736976676517
05/28/2023 14:09:50 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:50 - INFO - __main__ -    dev: infer_time = 2.8511111111111114
05/28/2023 14:09:50 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:50 - INFO - __main__ -     acc = 0.49458483754512633
05/28/2023 14:09:50 - INFO - __main__ -     cls_loss = 0.7001702973717138
05/28/2023 14:09:50 - INFO - __main__ -     eval_loss = 0.692736976676517
05/28/2023 14:09:50 - INFO - __main__ -     global_step = 19
05/28/2023 14:09:50 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:50 - INFO - __main__ -     infer_time = 2.8511111111111114
05/28/2023 14:09:50 - INFO - __main__ -     loss = 0.7001702973717138

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.22it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 13.43it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.46it/s][A05/28/2023 14:09:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:51 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:09:51 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 118.81it/s]
05/28/2023 14:09:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:51 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:51 - INFO - __main__ -    dev: eval_loss = 0.6939095060030619
05/28/2023 14:09:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:51 - INFO - __main__ -    dev: infer_time = 2.861
05/28/2023 14:09:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:51 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:51 - INFO - __main__ -     cls_loss = 0.7007487765673933
05/28/2023 14:09:51 - INFO - __main__ -     eval_loss = 0.6939095060030619
05/28/2023 14:09:51 - INFO - __main__ -     global_step = 29
05/28/2023 14:09:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:51 - INFO - __main__ -     infer_time = 2.861
05/28/2023 14:09:51 - INFO - __main__ -     loss = 0.7007487765673933

Iteration:  38%|###8      | 30/78 [00:02<00:03, 15.02it/s][A
Iteration:  42%|####2     | 33/78 [00:02<00:02, 16.80it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 18.28it/s][A05/28/2023 14:09:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:51 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:09:51 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 118.63it/s]
05/28/2023 14:09:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:51 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:09:51 - INFO - __main__ -    dev: eval_loss = 0.6996162931124369
05/28/2023 14:09:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:51 - INFO - __main__ -    dev: infer_time = 2.858333333333333
05/28/2023 14:09:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:51 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:09:51 - INFO - __main__ -     cls_loss = 0.6987006817108545
05/28/2023 14:09:51 - INFO - __main__ -     eval_loss = 0.6996162931124369
05/28/2023 14:09:51 - INFO - __main__ -     global_step = 39
05/28/2023 14:09:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:51 - INFO - __main__ -     infer_time = 2.858333333333333
05/28/2023 14:09:51 - INFO - __main__ -     loss = 0.6987006817108545

Iteration:  50%|#####     | 39/78 [00:03<00:02, 16.75it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:01, 18.21it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 19.44it/s][A
Iteration:  62%|######1   | 48/78 [00:03<00:01, 20.42it/s][A05/28/2023 14:09:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:52 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:09:52 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 118.92it/s]
05/28/2023 14:09:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:52 - INFO - __main__ -    dev: acc = 0.5018050541516246
05/28/2023 14:09:52 - INFO - __main__ -    dev: eval_loss = 0.6929865479469299
05/28/2023 14:09:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:52 - INFO - __main__ -    dev: infer_time = 2.8396666666666666
05/28/2023 14:09:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:52 - INFO - __main__ -     acc = 0.5018050541516246
05/28/2023 14:09:52 - INFO - __main__ -     cls_loss = 0.6977697318913986
05/28/2023 14:09:52 - INFO - __main__ -     eval_loss = 0.6929865479469299
05/28/2023 14:09:52 - INFO - __main__ -     global_step = 49
05/28/2023 14:09:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:52 - INFO - __main__ -     infer_time = 2.8396666666666666
05/28/2023 14:09:52 - INFO - __main__ -     loss = 0.6977697318913986

Iteration:  65%|######5   | 51/78 [00:03<00:01, 17.98it/s][A
Iteration:  69%|######9   | 54/78 [00:04<00:01, 19.27it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:01, 20.26it/s][A05/28/2023 14:09:52 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:52 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:09:52 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:52 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 118.78it/s]
05/28/2023 14:09:52 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:52 - INFO - __main__ -    dev: acc = 0.5703971119133574
05/28/2023 14:09:52 - INFO - __main__ -    dev: eval_loss = 0.6904098921351962
05/28/2023 14:09:52 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:52 - INFO - __main__ -    dev: infer_time = 2.8373333333333326
05/28/2023 14:09:52 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:52 - INFO - __main__ -     acc = 0.5703971119133574
05/28/2023 14:09:52 - INFO - __main__ -     cls_loss = 0.6966702119778778
05/28/2023 14:09:52 - INFO - __main__ -     eval_loss = 0.6904098921351962
05/28/2023 14:09:52 - INFO - __main__ -     global_step = 59
05/28/2023 14:09:52 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:52 - INFO - __main__ -     infer_time = 2.8373333333333326
05/28/2023 14:09:52 - INFO - __main__ -     loss = 0.6966702119778778
05/28/2023 14:09:52 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  77%|#######6  | 60/78 [00:05<00:03,  5.30it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:02,  6.87it/s][A
Iteration:  85%|########4 | 66/78 [00:06<00:01,  8.71it/s][A05/28/2023 14:09:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:54 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:09:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 118.80it/s]
05/28/2023 14:09:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:54 - INFO - __main__ -    dev: acc = 0.5740072202166066
05/28/2023 14:09:54 - INFO - __main__ -    dev: eval_loss = 0.6900997691684299
05/28/2023 14:09:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:54 - INFO - __main__ -    dev: infer_time = 2.8466666666666667
05/28/2023 14:09:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:54 - INFO - __main__ -     acc = 0.5740072202166066
05/28/2023 14:09:54 - INFO - __main__ -     cls_loss = 0.6959789386693982
05/28/2023 14:09:54 - INFO - __main__ -     eval_loss = 0.6900997691684299
05/28/2023 14:09:54 - INFO - __main__ -     global_step = 69
05/28/2023 14:09:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:54 - INFO - __main__ -     infer_time = 2.8466666666666667
05/28/2023 14:09:54 - INFO - __main__ -     loss = 0.6959789386693982
05/28/2023 14:09:54 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  88%|########8 | 69/78 [00:07<00:02,  4.33it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:01,  5.71it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00,  7.37it/s][AIteration: 100%|##########| 78/78 [00:07<00:00,  9.91it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.87s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.87s/it]
05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   w_emb: 13918032

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   p_emb: 233472

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   t_emb: 912

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   ln_emb: 912

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 365600

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 365256

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   intermediate_numel: 365600

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   output_numel: 366168

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 365600

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 365256

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   intermediate_numel: 365600

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   output_numel: 366168

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 365600

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 365256

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   intermediate_numel: 365600

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   output_numel: 366168

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   layer_numel: 4698744
05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   dense_numel: 208392
05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   emb_numel: 14153328

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   encoder_numel: 4698744

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   pooler_numel: 208392

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   all parameters: 19060464

05/28/2023 14:09:56 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:09:56 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [800, 800, 800], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
parameter size = 19060464
best_acc = 0.5740072202166066
time_per_batch_infer = 2.847 ms
infer_cnt = 63
**************E*************

05/28/2023 14:09:56 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [192, 192, 192], 'sample_qkv_sizes': [456, 456, 456]}
05/28/2023 14:09:56 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:09:56 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:09:56 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:56 - INFO - __main__ -   guid: train-0
05/28/2023 14:09:56 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:09:56 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:56 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:56 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:56 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:56 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:58 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:09:58 - INFO - __main__ -   *** Example ***
05/28/2023 14:09:58 - INFO - __main__ -   guid: dev-0
05/28/2023 14:09:58 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:09:58 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:58 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:58 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:09:58 - INFO - __main__ -   label: not_entailment
05/28/2023 14:09:58 - INFO - __main__ -   label_id: 1
05/28/2023 14:09:58 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:09:58 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:09:58 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:09:58 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:09:58 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:09:58 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:09:58 - INFO - __main__ -     Batch size = 32
05/28/2023 14:09:58 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 24.57it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 25.55it/s][A05/28/2023 14:09:59 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:09:59 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:09:59 - INFO - __main__ -     Num examples = 277
05/28/2023 14:09:59 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 148.05it/s]
05/28/2023 14:09:59 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:09:59 - INFO - __main__ -    dev: acc = 0.4693140794223827
05/28/2023 14:09:59 - INFO - __main__ -    dev: eval_loss = 0.6978586316108704
05/28/2023 14:09:59 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:09:59 - INFO - __main__ -    dev: infer_time = 2.833111111111111
05/28/2023 14:09:59 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:09:59 - INFO - __main__ -     acc = 0.4693140794223827
05/28/2023 14:09:59 - INFO - __main__ -     cls_loss = 0.6996439231766595
05/28/2023 14:09:59 - INFO - __main__ -     eval_loss = 0.6978586316108704
05/28/2023 14:09:59 - INFO - __main__ -     global_step = 9
05/28/2023 14:09:59 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:09:59 - INFO - __main__ -     infer_time = 2.833111111111111
05/28/2023 14:09:59 - INFO - __main__ -     loss = 0.6996439231766595
05/28/2023 14:09:59 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  4.05it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:10,  6.04it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.30it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.75it/s][A05/28/2023 14:10:01 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:01 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:10:01 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:01 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 146.93it/s]
05/28/2023 14:10:01 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:01 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:01 - INFO - __main__ -    dev: eval_loss = 0.6927503479851617
05/28/2023 14:10:01 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:01 - INFO - __main__ -    dev: infer_time = 2.8823333333333334
05/28/2023 14:10:01 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:01 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:01 - INFO - __main__ -     cls_loss = 0.702431791707089
05/28/2023 14:10:01 - INFO - __main__ -     eval_loss = 0.6927503479851617
05/28/2023 14:10:01 - INFO - __main__ -     global_step = 19
05/28/2023 14:10:01 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:01 - INFO - __main__ -     infer_time = 2.8823333333333334
05/28/2023 14:10:01 - INFO - __main__ -     loss = 0.702431791707089
05/28/2023 14:10:01 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:12,  4.48it/s][A
Iteration:  31%|###       | 24/78 [00:03<00:08,  6.06it/s][A
Iteration:  35%|###4      | 27/78 [00:03<00:06,  7.93it/s][A05/28/2023 14:10:02 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:02 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:10:02 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:02 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.48it/s]
05/28/2023 14:10:02 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:02 - INFO - __main__ -    dev: acc = 0.5018050541516246
05/28/2023 14:10:02 - INFO - __main__ -    dev: eval_loss = 0.6925362679693434
05/28/2023 14:10:02 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:02 - INFO - __main__ -    dev: infer_time = 2.8885555555555555
05/28/2023 14:10:02 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:02 - INFO - __main__ -     acc = 0.5018050541516246
05/28/2023 14:10:02 - INFO - __main__ -     cls_loss = 0.7016717339384144
05/28/2023 14:10:02 - INFO - __main__ -     eval_loss = 0.6925362679693434
05/28/2023 14:10:02 - INFO - __main__ -     global_step = 29
05/28/2023 14:10:02 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:02 - INFO - __main__ -     infer_time = 2.8885555555555555
05/28/2023 14:10:02 - INFO - __main__ -     loss = 0.7016717339384144

Iteration:  38%|###8      | 30/78 [00:03<00:05,  9.40it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:03, 11.69it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:03, 13.94it/s][A05/28/2023 14:10:03 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:03 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:10:03 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:03 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.53it/s]
05/28/2023 14:10:03 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:03 - INFO - __main__ -    dev: acc = 0.5703971119133574
05/28/2023 14:10:03 - INFO - __main__ -    dev: eval_loss = 0.6919190817409091
05/28/2023 14:10:03 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:03 - INFO - __main__ -    dev: infer_time = 2.865111111111111
05/28/2023 14:10:03 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:03 - INFO - __main__ -     acc = 0.5703971119133574
05/28/2023 14:10:03 - INFO - __main__ -     cls_loss = 0.7002018720675738
05/28/2023 14:10:03 - INFO - __main__ -     eval_loss = 0.6919190817409091
05/28/2023 14:10:03 - INFO - __main__ -     global_step = 39
05/28/2023 14:10:03 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:03 - INFO - __main__ -     infer_time = 2.865111111111111
05/28/2023 14:10:03 - INFO - __main__ -     loss = 0.7002018720675738
05/28/2023 14:10:03 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:05<00:07,  5.03it/s][A
Iteration:  54%|#####3    | 42/78 [00:05<00:05,  6.63it/s][A
Iteration:  58%|#####7    | 45/78 [00:05<00:03,  8.51it/s][A
Iteration:  62%|######1   | 48/78 [00:06<00:02, 10.61it/s][A05/28/2023 14:10:04 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:04 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:10:04 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:04 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.59it/s]
05/28/2023 14:10:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:05 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:05 - INFO - __main__ -    dev: eval_loss = 0.6971254083845351
05/28/2023 14:10:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:05 - INFO - __main__ -    dev: infer_time = 2.872444444444445
05/28/2023 14:10:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:05 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:05 - INFO - __main__ -     cls_loss = 0.6985939193745049
05/28/2023 14:10:05 - INFO - __main__ -     eval_loss = 0.6971254083845351
05/28/2023 14:10:05 - INFO - __main__ -     global_step = 49
05/28/2023 14:10:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:05 - INFO - __main__ -     infer_time = 2.872444444444445
05/28/2023 14:10:05 - INFO - __main__ -     loss = 0.6985939193745049

Iteration:  65%|######5   | 51/78 [00:06<00:02, 11.73it/s][A
Iteration:  69%|######9   | 54/78 [00:06<00:01, 13.96it/s][A
Iteration:  73%|#######3  | 57/78 [00:06<00:01, 16.09it/s][A05/28/2023 14:10:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:05 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:10:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.27it/s]
05/28/2023 14:10:05 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:05 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:05 - INFO - __main__ -    dev: eval_loss = 0.6948347489039103
05/28/2023 14:10:05 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:05 - INFO - __main__ -    dev: infer_time = 2.8901111111111115
05/28/2023 14:10:05 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:05 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:05 - INFO - __main__ -     cls_loss = 0.6981352183778408
05/28/2023 14:10:05 - INFO - __main__ -     eval_loss = 0.6948347489039103
05/28/2023 14:10:05 - INFO - __main__ -     global_step = 59
05/28/2023 14:10:05 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:05 - INFO - __main__ -     infer_time = 2.8901111111111115
05/28/2023 14:10:05 - INFO - __main__ -     loss = 0.6981352183778408

Iteration:  77%|#######6  | 60/78 [00:06<00:01, 16.16it/s][A
Iteration:  81%|########  | 63/78 [00:06<00:00, 18.05it/s][A
Iteration:  85%|########4 | 66/78 [00:06<00:00, 19.68it/s][A05/28/2023 14:10:05 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:05 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:10:05 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:05 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 147.64it/s]
05/28/2023 14:10:06 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:06 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:10:06 - INFO - __main__ -    dev: eval_loss = 0.6930831008487277
05/28/2023 14:10:06 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:06 - INFO - __main__ -    dev: infer_time = 2.856222222222222
05/28/2023 14:10:06 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:06 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:10:06 - INFO - __main__ -     cls_loss = 0.6972268370614536
05/28/2023 14:10:06 - INFO - __main__ -     eval_loss = 0.6930831008487277
05/28/2023 14:10:06 - INFO - __main__ -     global_step = 69
05/28/2023 14:10:06 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:06 - INFO - __main__ -     infer_time = 2.856222222222222
05/28/2023 14:10:06 - INFO - __main__ -     loss = 0.6972268370614536

Iteration:  88%|########8 | 69/78 [00:07<00:00, 18.32it/s][A
Iteration:  92%|#########2| 72/78 [00:07<00:00, 19.82it/s][A
Iteration:  96%|#########6| 75/78 [00:07<00:00, 21.10it/s][AIteration: 100%|##########| 78/78 [00:07<00:00, 10.50it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.43s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.43s/it]
05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   w_emb: 13918032

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   p_emb: 233472

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   t_emb: 912

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   ln_emb: 912

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 87744

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 88008

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   intermediate_numel: 87744

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   output_numel: 88920

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 87744

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 88008

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   intermediate_numel: 87744

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   output_numel: 88920

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 87744

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 88008

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   intermediate_numel: 87744

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   output_numel: 88920

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   layer_numel: 3033432
05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   dense_numel: 208392
05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   emb_numel: 14153328

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   encoder_numel: 3033432

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   pooler_numel: 208392

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   all parameters: 17395152

05/28/2023 14:10:06 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:06 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [192, 192, 192], 'sample_qkv_sizes': [456, 456, 456]}
parameter size = 17395152
best_acc = 0.5703971119133574
time_per_batch_infer = 2.870 ms
infer_cnt = 63
**************E*************

05/28/2023 14:10:06 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 492, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [492, 492, 492]}
05/28/2023 14:10:06 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:10:06 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:10:06 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:06 - INFO - __main__ -   guid: train-0
05/28/2023 14:10:06 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:10:06 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:06 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:06 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:06 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:06 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:08 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:10:08 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:08 - INFO - __main__ -   guid: dev-0
05/28/2023 14:10:08 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:10:08 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:08 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:08 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:08 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:08 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:08 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:10:08 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:10:08 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:10:08 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:10:08 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:10:08 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:10:08 - INFO - __main__ -     Batch size = 32
05/28/2023 14:10:08 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.77it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.79it/s][A05/28/2023 14:10:09 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:09 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:10:09 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:09 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.42it/s]
05/28/2023 14:10:09 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:09 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:09 - INFO - __main__ -    dev: eval_loss = 0.7383000916904874
05/28/2023 14:10:09 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:09 - INFO - __main__ -    dev: infer_time = 2.8485555555555555
05/28/2023 14:10:09 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:09 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:09 - INFO - __main__ -     cls_loss = 0.6962449616856046
05/28/2023 14:10:09 - INFO - __main__ -     eval_loss = 0.7383000916904874
05/28/2023 14:10:09 - INFO - __main__ -     global_step = 9
05/28/2023 14:10:09 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:09 - INFO - __main__ -     infer_time = 2.8485555555555555
05/28/2023 14:10:09 - INFO - __main__ -     loss = 0.6962449616856046
05/28/2023 14:10:09 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:18,  3.82it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.64it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.70it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.91it/s][A05/28/2023 14:10:11 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:11 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:10:11 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:11 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.13it/s]
05/28/2023 14:10:11 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:11 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:11 - INFO - __main__ -    dev: eval_loss = 0.6905501286188761
05/28/2023 14:10:11 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:11 - INFO - __main__ -    dev: infer_time = 2.8641111111111113
05/28/2023 14:10:11 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:11 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:11 - INFO - __main__ -     cls_loss = 0.6961898678227475
05/28/2023 14:10:11 - INFO - __main__ -     eval_loss = 0.6905501286188761
05/28/2023 14:10:11 - INFO - __main__ -     global_step = 19
05/28/2023 14:10:11 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:11 - INFO - __main__ -     infer_time = 2.8641111111111113
05/28/2023 14:10:11 - INFO - __main__ -     loss = 0.6961898678227475
05/28/2023 14:10:11 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  26%|##5       | 20/78 [00:03<00:14,  3.92it/s][A
Iteration:  29%|##9       | 23/78 [00:03<00:10,  5.45it/s][A
Iteration:  33%|###3      | 26/78 [00:03<00:07,  7.25it/s][A05/28/2023 14:10:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:13 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:10:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.40it/s]
05/28/2023 14:10:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:13 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:13 - INFO - __main__ -    dev: eval_loss = 0.69000819656584
05/28/2023 14:10:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:13 - INFO - __main__ -    dev: infer_time = 2.869888888888889
05/28/2023 14:10:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:13 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:13 - INFO - __main__ -     cls_loss = 0.6942106670346754
05/28/2023 14:10:13 - INFO - __main__ -     eval_loss = 0.69000819656584
05/28/2023 14:10:13 - INFO - __main__ -     global_step = 29
05/28/2023 14:10:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:13 - INFO - __main__ -     infer_time = 2.869888888888889
05/28/2023 14:10:13 - INFO - __main__ -     loss = 0.6942106670346754

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.50it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.53it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.54it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.49it/s][A05/28/2023 14:10:13 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:13 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:10:13 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:13 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.08it/s]
05/28/2023 14:10:13 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:13 - INFO - __main__ -    dev: acc = 0.48736462093862815
05/28/2023 14:10:13 - INFO - __main__ -    dev: eval_loss = 0.6933899720509847
05/28/2023 14:10:13 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:13 - INFO - __main__ -    dev: infer_time = 2.8773333333333335
05/28/2023 14:10:13 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:13 - INFO - __main__ -     acc = 0.48736462093862815
05/28/2023 14:10:13 - INFO - __main__ -     cls_loss = 0.6949131916730832
05/28/2023 14:10:13 - INFO - __main__ -     eval_loss = 0.6933899720509847
05/28/2023 14:10:13 - INFO - __main__ -     global_step = 39
05/28/2023 14:10:13 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:13 - INFO - __main__ -     infer_time = 2.8773333333333335
05/28/2023 14:10:13 - INFO - __main__ -     loss = 0.6949131916730832

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.11it/s][A
Iteration:  56%|#####6    | 44/78 [00:04<00:02, 15.82it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.32it/s][A05/28/2023 14:10:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:14 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:10:14 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.07it/s]
05/28/2023 14:10:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:14 - INFO - __main__ -    dev: acc = 0.5018050541516246
05/28/2023 14:10:14 - INFO - __main__ -    dev: eval_loss = 0.6925063331921896
05/28/2023 14:10:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:14 - INFO - __main__ -    dev: infer_time = 2.8966666666666665
05/28/2023 14:10:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:14 - INFO - __main__ -     acc = 0.5018050541516246
05/28/2023 14:10:14 - INFO - __main__ -     cls_loss = 0.6947073498550727
05/28/2023 14:10:14 - INFO - __main__ -     eval_loss = 0.6925063331921896
05/28/2023 14:10:14 - INFO - __main__ -     global_step = 49
05/28/2023 14:10:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:14 - INFO - __main__ -     infer_time = 2.8966666666666665
05/28/2023 14:10:14 - INFO - __main__ -     loss = 0.6947073498550727

Iteration:  64%|######4   | 50/78 [00:05<00:01, 15.81it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 17.26it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 18.47it/s][A05/28/2023 14:10:14 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:14 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:10:14 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:14 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.17it/s]
05/28/2023 14:10:14 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:14 - INFO - __main__ -    dev: acc = 0.5523465703971119
05/28/2023 14:10:14 - INFO - __main__ -    dev: eval_loss = 0.689166525999705
05/28/2023 14:10:14 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:14 - INFO - __main__ -    dev: infer_time = 2.9171111111111108
05/28/2023 14:10:14 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:14 - INFO - __main__ -     acc = 0.5523465703971119
05/28/2023 14:10:14 - INFO - __main__ -     cls_loss = 0.6943012829554283
05/28/2023 14:10:14 - INFO - __main__ -     eval_loss = 0.689166525999705
05/28/2023 14:10:14 - INFO - __main__ -     global_step = 59
05/28/2023 14:10:14 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:14 - INFO - __main__ -     infer_time = 2.9171111111111108
05/28/2023 14:10:14 - INFO - __main__ -     loss = 0.6943012829554283
05/28/2023 14:10:14 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  76%|#######5  | 59/78 [00:07<00:03,  5.27it/s][A
Iteration:  79%|#######9  | 62/78 [00:07<00:02,  6.80it/s][A
Iteration:  83%|########3 | 65/78 [00:07<00:01,  8.58it/s][A
Iteration:  87%|########7 | 68/78 [00:07<00:00, 10.49it/s][A05/28/2023 14:10:16 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:16 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:10:16 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:16 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 112.24it/s]
05/28/2023 14:10:16 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:16 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:16 - INFO - __main__ -    dev: eval_loss = 0.6959419647852579
05/28/2023 14:10:16 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:16 - INFO - __main__ -    dev: infer_time = 2.867666666666667
05/28/2023 14:10:16 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:16 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:16 - INFO - __main__ -     cls_loss = 0.6938686940980994
05/28/2023 14:10:16 - INFO - __main__ -     eval_loss = 0.6959419647852579
05/28/2023 14:10:16 - INFO - __main__ -     global_step = 69
05/28/2023 14:10:16 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:16 - INFO - __main__ -     infer_time = 2.867666666666667
05/28/2023 14:10:16 - INFO - __main__ -     loss = 0.6938686940980994

Iteration:  90%|########9 | 70/78 [00:07<00:00, 10.66it/s][A
Iteration:  94%|#########3| 73/78 [00:07<00:00, 12.77it/s][A
Iteration:  97%|#########7| 76/78 [00:07<00:00, 14.76it/s][AIteration: 100%|##########| 78/78 [00:08<00:00,  9.70it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.04s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.04s/it]
05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   w_emb: 15016824

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   p_emb: 251904

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   t_emb: 984

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   ln_emb: 984

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 425952

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 425580

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   intermediate_numel: 425952

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   output_numel: 426564

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 425952

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 425580

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   intermediate_numel: 425952

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   output_numel: 426564

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   query_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   key_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   value_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   self_numel: 727668

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   output_numel: 243540

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 425952

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 425580

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   ln_numel: 984

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   attention_numel: 971208

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   intermediate_numel: 425952

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   output_numel: 426564

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   layer_numel: 5471172
05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   dense_numel: 242556
05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   emb_numel: 15270696

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   encoder_numel: 5471172

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   pooler_numel: 242556

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   all parameters: 20984424

05/28/2023 14:10:16 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:16 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 492, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [492, 492, 492]}
parameter size = 20984424
best_acc = 0.5523465703971119
time_per_batch_infer = 2.877 ms
infer_cnt = 63
**************E*************

05/28/2023 14:10:16 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
05/28/2023 14:10:16 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:10:17 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:10:17 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:17 - INFO - __main__ -   guid: train-0
05/28/2023 14:10:17 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:10:17 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:17 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:17 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:17 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:17 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:18 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:10:18 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:18 - INFO - __main__ -   guid: dev-0
05/28/2023 14:10:18 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:10:18 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:18 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:18 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:18 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:18 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:19 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:10:19 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:10:19 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:10:19 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:10:19 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:10:19 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:10:19 - INFO - __main__ -     Batch size = 32
05/28/2023 14:10:19 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 20.63it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 21.05it/s][A05/28/2023 14:10:20 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:20 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:10:20 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:20 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 119.39it/s]
05/28/2023 14:10:20 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:20 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:20 - INFO - __main__ -    dev: eval_loss = 0.6955043011241488
05/28/2023 14:10:20 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:20 - INFO - __main__ -    dev: infer_time = 3.6358888888888887
05/28/2023 14:10:20 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:20 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:20 - INFO - __main__ -     cls_loss = 0.6962635583347745
05/28/2023 14:10:20 - INFO - __main__ -     eval_loss = 0.6955043011241488
05/28/2023 14:10:20 - INFO - __main__ -     global_step = 9
05/28/2023 14:10:20 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:20 - INFO - __main__ -     infer_time = 3.6358888888888887
05/28/2023 14:10:20 - INFO - __main__ -     loss = 0.6962635583347745
05/28/2023 14:10:20 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.89it/s][A
Iteration:  14%|#4        | 11/78 [00:01<00:13,  5.14it/s][A
Iteration:  18%|#7        | 14/78 [00:02<00:08,  7.29it/s][A
Iteration:  22%|##1       | 17/78 [00:02<00:06,  9.53it/s][A05/28/2023 14:10:21 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:21 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:10:21 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:21 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 120.63it/s]
05/28/2023 14:10:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:22 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:22 - INFO - __main__ -    dev: eval_loss = 0.6907176110479567
05/28/2023 14:10:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:22 - INFO - __main__ -    dev: infer_time = 3.626555555555556
05/28/2023 14:10:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:22 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:22 - INFO - __main__ -     cls_loss = 0.6941315029796801
05/28/2023 14:10:22 - INFO - __main__ -     eval_loss = 0.6907176110479567
05/28/2023 14:10:22 - INFO - __main__ -     global_step = 19
05/28/2023 14:10:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:22 - INFO - __main__ -     infer_time = 3.626555555555556
05/28/2023 14:10:22 - INFO - __main__ -     loss = 0.6941315029796801

Iteration:  24%|##4       | 19/78 [00:02<00:05,  9.86it/s][A
Iteration:  28%|##8       | 22/78 [00:02<00:04, 12.09it/s][A
Iteration:  32%|###2      | 25/78 [00:02<00:03, 14.08it/s][A
Iteration:  36%|###5      | 28/78 [00:02<00:03, 15.79it/s][A05/28/2023 14:10:22 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:22 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:10:22 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:22 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 120.50it/s]
05/28/2023 14:10:22 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:22 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:22 - INFO - __main__ -    dev: eval_loss = 0.7033542593320211
05/28/2023 14:10:22 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:22 - INFO - __main__ -    dev: infer_time = 3.634666666666667
05/28/2023 14:10:22 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:22 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:22 - INFO - __main__ -     cls_loss = 0.6939322722369227
05/28/2023 14:10:22 - INFO - __main__ -     eval_loss = 0.7033542593320211
05/28/2023 14:10:22 - INFO - __main__ -     global_step = 29
05/28/2023 14:10:22 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:22 - INFO - __main__ -     infer_time = 3.634666666666667
05/28/2023 14:10:22 - INFO - __main__ -     loss = 0.6939322722369227

Iteration:  38%|###8      | 30/78 [00:02<00:03, 14.39it/s][A
Iteration:  42%|####2     | 33/78 [00:03<00:02, 16.10it/s][A
Iteration:  46%|####6     | 36/78 [00:03<00:02, 17.45it/s][A05/28/2023 14:10:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:23 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:10:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 120.82it/s]
05/28/2023 14:10:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:23 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:23 - INFO - __main__ -    dev: eval_loss = 0.6933037241299947
05/28/2023 14:10:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:23 - INFO - __main__ -    dev: infer_time = 3.613666666666666
05/28/2023 14:10:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:23 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:23 - INFO - __main__ -     cls_loss = 0.6938902185513423
05/28/2023 14:10:23 - INFO - __main__ -     eval_loss = 0.6933037241299947
05/28/2023 14:10:23 - INFO - __main__ -     global_step = 39
05/28/2023 14:10:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:23 - INFO - __main__ -     infer_time = 3.613666666666666
05/28/2023 14:10:23 - INFO - __main__ -     loss = 0.6938902185513423

Iteration:  50%|#####     | 39/78 [00:03<00:02, 15.92it/s][A
Iteration:  54%|#####3    | 42/78 [00:03<00:02, 17.17it/s][A
Iteration:  58%|#####7    | 45/78 [00:03<00:01, 18.24it/s][A
Iteration:  60%|######    | 47/78 [00:03<00:01, 18.47it/s][A05/28/2023 14:10:23 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:23 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:10:23 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:23 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 120.93it/s]
05/28/2023 14:10:23 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:23 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:23 - INFO - __main__ -    dev: eval_loss = 0.690247290664249
05/28/2023 14:10:23 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:23 - INFO - __main__ -    dev: infer_time = 3.6243333333333334
05/28/2023 14:10:23 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:23 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:23 - INFO - __main__ -     cls_loss = 0.6938754247159374
05/28/2023 14:10:23 - INFO - __main__ -     eval_loss = 0.690247290664249
05/28/2023 14:10:23 - INFO - __main__ -     global_step = 49
05/28/2023 14:10:23 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:23 - INFO - __main__ -     infer_time = 3.6243333333333334
05/28/2023 14:10:23 - INFO - __main__ -     loss = 0.6938754247159374

Iteration:  63%|######2   | 49/78 [00:04<00:01, 16.08it/s][A
Iteration:  67%|######6   | 52/78 [00:04<00:01, 17.50it/s][A
Iteration:  71%|#######   | 55/78 [00:04<00:01, 18.52it/s][A
Iteration:  74%|#######4  | 58/78 [00:04<00:01, 19.15it/s][A05/28/2023 14:10:24 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:24 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:10:24 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:24 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 120.89it/s]
05/28/2023 14:10:24 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:24 - INFO - __main__ -    dev: acc = 0.5379061371841155
05/28/2023 14:10:24 - INFO - __main__ -    dev: eval_loss = 0.6907509432898628
05/28/2023 14:10:24 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:24 - INFO - __main__ -    dev: infer_time = 3.6198888888888896
05/28/2023 14:10:24 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:24 - INFO - __main__ -     acc = 0.5379061371841155
05/28/2023 14:10:24 - INFO - __main__ -     cls_loss = 0.6940733012506517
05/28/2023 14:10:24 - INFO - __main__ -     eval_loss = 0.6907509432898628
05/28/2023 14:10:24 - INFO - __main__ -     global_step = 59
05/28/2023 14:10:24 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:24 - INFO - __main__ -     infer_time = 3.6198888888888896
05/28/2023 14:10:24 - INFO - __main__ -     loss = 0.6940733012506517
05/28/2023 14:10:24 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  77%|#######6  | 60/78 [00:05<00:03,  4.69it/s][A
Iteration:  81%|########  | 63/78 [00:06<00:02,  6.33it/s][A
Iteration:  85%|########4 | 66/78 [00:06<00:01,  8.17it/s][A05/28/2023 14:10:26 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:26 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:10:26 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:26 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 120.85it/s]
05/28/2023 14:10:26 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:26 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:26 - INFO - __main__ -    dev: eval_loss = 0.696790443526374
05/28/2023 14:10:26 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:26 - INFO - __main__ -    dev: infer_time = 3.617777777777778
05/28/2023 14:10:26 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:26 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:26 - INFO - __main__ -     cls_loss = 0.6936744883440543
05/28/2023 14:10:26 - INFO - __main__ -     eval_loss = 0.696790443526374
05/28/2023 14:10:26 - INFO - __main__ -     global_step = 69
05/28/2023 14:10:26 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:26 - INFO - __main__ -     infer_time = 3.617777777777778
05/28/2023 14:10:26 - INFO - __main__ -     loss = 0.6936744883440543

Iteration:  88%|########8 | 69/78 [00:06<00:00,  9.29it/s][A
Iteration:  91%|#########1| 71/78 [00:06<00:00, 10.54it/s][A
Iteration:  95%|#########4| 74/78 [00:06<00:00, 12.64it/s][A
Iteration:  99%|#########8| 77/78 [00:06<00:00, 14.50it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.34it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.88s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.88s/it]
05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   w_emb: 13185504

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   p_emb: 221184

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   t_emb: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ln_emb: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 83136

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 83376

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83136

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   output_numel: 84240

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 83136

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 83376

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83136

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   output_numel: 84240

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 83136

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 83376

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83136

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   output_numel: 84240

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   query_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   key_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   value_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   self_numel: 561168

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   output_numel: 187920

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 83136

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 83376

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ln_numel: 864

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   attention_numel: 749088

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   intermediate_numel: 83136

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   output_numel: 84240

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   layer_numel: 3665856
05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   dense_numel: 187056
05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   emb_numel: 13408416

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   encoder_numel: 3665856

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   pooler_numel: 187056

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   all parameters: 17261328

05/28/2023 14:10:26 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:26 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
parameter size = 17261328
best_acc = 0.5379061371841155
time_per_batch_infer = 3.625 ms
infer_cnt = 63
**************E*************

05/28/2023 14:10:26 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [896, 896, 896], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
05/28/2023 14:10:26 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:10:26 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:10:26 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:26 - INFO - __main__ -   guid: train-0
05/28/2023 14:10:26 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:10:26 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:26 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:26 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:26 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:26 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:28 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:10:28 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:28 - INFO - __main__ -   guid: dev-0
05/28/2023 14:10:28 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:10:28 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:28 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:28 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:28 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:28 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:28 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:10:28 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:10:29 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:10:29 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:10:29 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:10:29 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:10:29 - INFO - __main__ -     Batch size = 32
05/28/2023 14:10:29 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.78it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.33it/s][A05/28/2023 14:10:29 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:29 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:10:29 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:29 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.21it/s]
05/28/2023 14:10:29 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:29 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:29 - INFO - __main__ -    dev: eval_loss = 0.6925456921259562
05/28/2023 14:10:29 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:29 - INFO - __main__ -    dev: infer_time = 2.873666666666667
05/28/2023 14:10:29 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:29 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:29 - INFO - __main__ -     cls_loss = 0.6929130752881368
05/28/2023 14:10:29 - INFO - __main__ -     eval_loss = 0.6925456921259562
05/28/2023 14:10:29 - INFO - __main__ -     global_step = 9
05/28/2023 14:10:29 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:29 - INFO - __main__ -     infer_time = 2.873666666666667
05/28/2023 14:10:29 - INFO - __main__ -     loss = 0.6929130752881368
05/28/2023 14:10:29 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:17,  3.94it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:11,  5.82it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:07,  7.95it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 10.21it/s][A05/28/2023 14:10:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:31 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:10:31 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.90it/s]
05/28/2023 14:10:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:31 - INFO - __main__ -    dev: acc = 0.4657039711191336
05/28/2023 14:10:31 - INFO - __main__ -    dev: eval_loss = 0.6939149697621664
05/28/2023 14:10:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:31 - INFO - __main__ -    dev: infer_time = 2.893333333333333
05/28/2023 14:10:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:31 - INFO - __main__ -     acc = 0.4657039711191336
05/28/2023 14:10:31 - INFO - __main__ -     cls_loss = 0.6998902653392992
05/28/2023 14:10:31 - INFO - __main__ -     eval_loss = 0.6939149697621664
05/28/2023 14:10:31 - INFO - __main__ -     global_step = 19
05/28/2023 14:10:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:31 - INFO - __main__ -     infer_time = 2.893333333333333
05/28/2023 14:10:31 - INFO - __main__ -     loss = 0.6998902653392992

Iteration:  27%|##6       | 21/78 [00:02<00:05, 11.15it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:04, 13.30it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:03, 15.25it/s][A05/28/2023 14:10:31 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:31 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:10:31 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:31 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.02it/s]
05/28/2023 14:10:31 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:31 - INFO - __main__ -    dev: acc = 0.5342960288808665
05/28/2023 14:10:31 - INFO - __main__ -    dev: eval_loss = 0.6910613046752082
05/28/2023 14:10:31 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:31 - INFO - __main__ -    dev: infer_time = 2.872777777777778
05/28/2023 14:10:31 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:31 - INFO - __main__ -     acc = 0.5342960288808665
05/28/2023 14:10:31 - INFO - __main__ -     cls_loss = 0.6976690600658285
05/28/2023 14:10:31 - INFO - __main__ -     eval_loss = 0.6910613046752082
05/28/2023 14:10:31 - INFO - __main__ -     global_step = 29
05/28/2023 14:10:31 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:31 - INFO - __main__ -     infer_time = 2.872777777777778
05/28/2023 14:10:31 - INFO - __main__ -     loss = 0.6976690600658285
05/28/2023 14:10:31 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:04<00:09,  5.00it/s][A
Iteration:  42%|####2     | 33/78 [00:04<00:06,  6.56it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:05,  8.37it/s][A05/28/2023 14:10:33 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:33 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:10:33 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:33 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 116.00it/s]
05/28/2023 14:10:33 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:33 - INFO - __main__ -    dev: acc = 0.5523465703971119
05/28/2023 14:10:33 - INFO - __main__ -    dev: eval_loss = 0.6910119189156426
05/28/2023 14:10:33 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:33 - INFO - __main__ -    dev: infer_time = 2.884555555555556
05/28/2023 14:10:33 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:33 - INFO - __main__ -     acc = 0.5523465703971119
05/28/2023 14:10:33 - INFO - __main__ -     cls_loss = 0.6965260872474084
05/28/2023 14:10:33 - INFO - __main__ -     eval_loss = 0.6910119189156426
05/28/2023 14:10:33 - INFO - __main__ -     global_step = 39
05/28/2023 14:10:33 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:33 - INFO - __main__ -     infer_time = 2.884555555555556
05/28/2023 14:10:33 - INFO - __main__ -     loss = 0.6965260872474084
05/28/2023 14:10:33 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  50%|#####     | 39/78 [00:05<00:09,  4.24it/s][A
Iteration:  54%|#####3    | 42/78 [00:06<00:06,  5.59it/s][A
Iteration:  58%|#####7    | 45/78 [00:06<00:04,  7.24it/s][A
Iteration:  62%|######1   | 48/78 [00:06<00:03,  9.11it/s][A05/28/2023 14:10:35 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:35 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:10:35 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:35 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.83it/s]
05/28/2023 14:10:35 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:35 - INFO - __main__ -    dev: acc = 0.5631768953068592
05/28/2023 14:10:35 - INFO - __main__ -    dev: eval_loss = 0.6914526952637566
05/28/2023 14:10:35 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:35 - INFO - __main__ -    dev: infer_time = 2.9111111111111114
05/28/2023 14:10:35 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:35 - INFO - __main__ -     acc = 0.5631768953068592
05/28/2023 14:10:35 - INFO - __main__ -     cls_loss = 0.6959759459203604
05/28/2023 14:10:35 - INFO - __main__ -     eval_loss = 0.6914526952637566
05/28/2023 14:10:35 - INFO - __main__ -     global_step = 49
05/28/2023 14:10:35 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:35 - INFO - __main__ -     infer_time = 2.9111111111111114
05/28/2023 14:10:35 - INFO - __main__ -     loss = 0.6959759459203604
05/28/2023 14:10:35 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  64%|######4   | 50/78 [00:07<00:06,  4.00it/s][A
Iteration:  68%|######7   | 53/78 [00:07<00:04,  5.45it/s][A
Iteration:  72%|#######1  | 56/78 [00:08<00:03,  7.16it/s][A05/28/2023 14:10:37 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:37 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:10:37 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:37 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.62it/s]
05/28/2023 14:10:37 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:37 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:37 - INFO - __main__ -    dev: eval_loss = 0.6971595684687296
05/28/2023 14:10:37 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:37 - INFO - __main__ -    dev: infer_time = 2.9099999999999997
05/28/2023 14:10:37 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:37 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:37 - INFO - __main__ -     cls_loss = 0.6948674088817531
05/28/2023 14:10:37 - INFO - __main__ -     eval_loss = 0.6971595684687296
05/28/2023 14:10:37 - INFO - __main__ -     global_step = 59
05/28/2023 14:10:37 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:37 - INFO - __main__ -     infer_time = 2.9099999999999997
05/28/2023 14:10:37 - INFO - __main__ -     loss = 0.6948674088817531

Iteration:  76%|#######5  | 59/78 [00:08<00:02,  8.41it/s][A
Iteration:  79%|#######9  | 62/78 [00:08<00:01, 10.38it/s][A
Iteration:  83%|########3 | 65/78 [00:08<00:01, 12.29it/s][A
Iteration:  87%|########7 | 68/78 [00:08<00:00, 14.26it/s][A05/28/2023 14:10:37 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:37 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:10:37 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:37 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 115.82it/s]
05/28/2023 14:10:37 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:37 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:37 - INFO - __main__ -    dev: eval_loss = 0.6977050436867608
05/28/2023 14:10:37 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:37 - INFO - __main__ -    dev: infer_time = 2.9232222222222224
05/28/2023 14:10:37 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:37 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:37 - INFO - __main__ -     cls_loss = 0.6956862824550574
05/28/2023 14:10:37 - INFO - __main__ -     eval_loss = 0.6977050436867608
05/28/2023 14:10:37 - INFO - __main__ -     global_step = 69
05/28/2023 14:10:37 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:37 - INFO - __main__ -     infer_time = 2.9232222222222224
05/28/2023 14:10:37 - INFO - __main__ -     loss = 0.6956862824550574

Iteration:  91%|#########1| 71/78 [00:08<00:00, 14.05it/s][A
Iteration:  95%|#########4| 74/78 [00:09<00:00, 15.90it/s][A
Iteration:  99%|#########8| 77/78 [00:09<00:00, 17.46it/s][AIteration: 100%|##########| 78/78 [00:09<00:00,  8.47it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.21s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.21s/it]
05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   w_emb: 13918032

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   p_emb: 233472

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   t_emb: 912

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   ln_emb: 912

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 409472

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 409032

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   intermediate_numel: 409472

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   output_numel: 409944

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 409472

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 409032

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   intermediate_numel: 409472

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   output_numel: 409944

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   query_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   key_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   value_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   self_numel: 625176

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   output_numel: 209304

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 409472

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 409032

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   ln_numel: 912

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   attention_numel: 834480

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   intermediate_numel: 409472

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   output_numel: 409944

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   layer_numel: 4961688
05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   dense_numel: 208392
05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   emb_numel: 14153328

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   encoder_numel: 4961688

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   pooler_numel: 208392

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   all parameters: 19323408

05/28/2023 14:10:38 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:38 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [896, 896, 896], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
parameter size = 19323408
best_acc = 0.5631768953068592
time_per_batch_infer = 2.896 ms
infer_cnt = 63
**************E*************

05/28/2023 14:10:38 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [992, 992, 992], 'sample_qkv_sizes': [288, 288, 288]}
05/28/2023 14:10:38 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:10:38 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:10:38 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:38 - INFO - __main__ -   guid: train-0
05/28/2023 14:10:38 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:10:38 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:38 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:38 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:38 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:38 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:40 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:10:40 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:40 - INFO - __main__ -   guid: dev-0
05/28/2023 14:10:40 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:10:40 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:40 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:40 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:40 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:40 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:40 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:10:40 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:10:40 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:10:40 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:10:40 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:10:40 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:10:40 - INFO - __main__ -     Batch size = 32
05/28/2023 14:10:40 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:02, 25.35it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:02, 26.74it/s][A05/28/2023 14:10:41 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:41 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:10:41 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:41 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 163.92it/s]
05/28/2023 14:10:41 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:41 - INFO - __main__ -    dev: acc = 0.51985559566787
05/28/2023 14:10:41 - INFO - __main__ -    dev: eval_loss = 0.6914574305216471
05/28/2023 14:10:41 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:41 - INFO - __main__ -    dev: infer_time = 2.794333333333333
05/28/2023 14:10:41 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:41 - INFO - __main__ -     acc = 0.51985559566787
05/28/2023 14:10:41 - INFO - __main__ -     cls_loss = 0.6969555483924018
05/28/2023 14:10:41 - INFO - __main__ -     eval_loss = 0.6914574305216471
05/28/2023 14:10:41 - INFO - __main__ -     global_step = 9
05/28/2023 14:10:41 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:41 - INFO - __main__ -     infer_time = 2.794333333333333
05/28/2023 14:10:41 - INFO - __main__ -     loss = 0.6969555483924018
05/28/2023 14:10:41 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:16,  4.12it/s][A
Iteration:  15%|#5        | 12/78 [00:01<00:10,  6.17it/s][A
Iteration:  19%|#9        | 15/78 [00:01<00:07,  8.58it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:05, 11.22it/s][A05/28/2023 14:10:42 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:42 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:10:42 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:42 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 163.82it/s]
05/28/2023 14:10:42 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:42 - INFO - __main__ -    dev: acc = 0.5126353790613718
05/28/2023 14:10:42 - INFO - __main__ -    dev: eval_loss = 0.6935472422175937
05/28/2023 14:10:42 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:42 - INFO - __main__ -    dev: infer_time = 2.8046666666666673
05/28/2023 14:10:42 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:42 - INFO - __main__ -     acc = 0.5126353790613718
05/28/2023 14:10:42 - INFO - __main__ -     cls_loss = 0.6979671716690063
05/28/2023 14:10:42 - INFO - __main__ -     eval_loss = 0.6935472422175937
05/28/2023 14:10:42 - INFO - __main__ -     global_step = 19
05/28/2023 14:10:42 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:42 - INFO - __main__ -     infer_time = 2.8046666666666673
05/28/2023 14:10:42 - INFO - __main__ -     loss = 0.6979671716690063

Iteration:  27%|##6       | 21/78 [00:02<00:04, 12.49it/s][A
Iteration:  31%|###       | 24/78 [00:02<00:03, 15.08it/s][A
Iteration:  35%|###4      | 27/78 [00:02<00:02, 17.58it/s][A05/28/2023 14:10:43 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:43 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:10:43 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:43 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 163.96it/s]
05/28/2023 14:10:43 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:43 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:43 - INFO - __main__ -    dev: eval_loss = 0.6920704576704237
05/28/2023 14:10:43 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:43 - INFO - __main__ -    dev: infer_time = 2.786666666666667
05/28/2023 14:10:43 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:43 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:43 - INFO - __main__ -     cls_loss = 0.6964359324553917
05/28/2023 14:10:43 - INFO - __main__ -     eval_loss = 0.6920704576704237
05/28/2023 14:10:43 - INFO - __main__ -     global_step = 29
05/28/2023 14:10:43 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:43 - INFO - __main__ -     infer_time = 2.786666666666667
05/28/2023 14:10:43 - INFO - __main__ -     loss = 0.6964359324553917
05/28/2023 14:10:43 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  38%|###8      | 30/78 [00:03<00:09,  5.26it/s][A
Iteration:  42%|####2     | 33/78 [00:03<00:06,  6.99it/s][A
Iteration:  46%|####6     | 36/78 [00:04<00:04,  9.04it/s][A05/28/2023 14:10:45 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:45 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:10:45 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:45 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.07it/s]
05/28/2023 14:10:45 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:45 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:10:45 - INFO - __main__ -    dev: eval_loss = 0.690573083029853
05/28/2023 14:10:45 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:45 - INFO - __main__ -    dev: infer_time = 2.7779999999999996
05/28/2023 14:10:45 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:45 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:10:45 - INFO - __main__ -     cls_loss = 0.6966612384869502
05/28/2023 14:10:45 - INFO - __main__ -     eval_loss = 0.690573083029853
05/28/2023 14:10:45 - INFO - __main__ -     global_step = 39
05/28/2023 14:10:45 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:45 - INFO - __main__ -     infer_time = 2.7779999999999996
05/28/2023 14:10:45 - INFO - __main__ -     loss = 0.6966612384869502

Iteration:  50%|#####     | 39/78 [00:04<00:03, 10.63it/s][A
Iteration:  54%|#####3    | 42/78 [00:04<00:02, 13.06it/s][A
Iteration:  58%|#####7    | 45/78 [00:04<00:02, 15.49it/s][A
Iteration:  62%|######1   | 48/78 [00:04<00:01, 17.86it/s][A05/28/2023 14:10:45 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:45 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:10:45 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:45 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.08it/s]
05/28/2023 14:10:45 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:45 - INFO - __main__ -    dev: acc = 0.4657039711191336
05/28/2023 14:10:45 - INFO - __main__ -    dev: eval_loss = 0.694242020448049
05/28/2023 14:10:45 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:45 - INFO - __main__ -    dev: infer_time = 2.778222222222222
05/28/2023 14:10:45 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:45 - INFO - __main__ -     acc = 0.4657039711191336
05/28/2023 14:10:45 - INFO - __main__ -     cls_loss = 0.6964207997127455
05/28/2023 14:10:45 - INFO - __main__ -     eval_loss = 0.694242020448049
05/28/2023 14:10:45 - INFO - __main__ -     global_step = 49
05/28/2023 14:10:45 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:45 - INFO - __main__ -     infer_time = 2.778222222222222
05/28/2023 14:10:45 - INFO - __main__ -     loss = 0.6964207997127455

Iteration:  65%|######5   | 51/78 [00:04<00:01, 17.78it/s][A
Iteration:  69%|######9   | 54/78 [00:04<00:01, 19.96it/s][A
Iteration:  73%|#######3  | 57/78 [00:04<00:00, 21.85it/s][A05/28/2023 14:10:45 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:45 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:10:45 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:45 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 163.85it/s]
05/28/2023 14:10:45 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:45 - INFO - __main__ -    dev: acc = 0.4981949458483754
05/28/2023 14:10:45 - INFO - __main__ -    dev: eval_loss = 0.6923105385568407
05/28/2023 14:10:45 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:45 - INFO - __main__ -    dev: infer_time = 2.805222222222222
05/28/2023 14:10:46 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:46 - INFO - __main__ -     acc = 0.4981949458483754
05/28/2023 14:10:46 - INFO - __main__ -     cls_loss = 0.6959310810444719
05/28/2023 14:10:46 - INFO - __main__ -     eval_loss = 0.6923105385568407
05/28/2023 14:10:46 - INFO - __main__ -     global_step = 59
05/28/2023 14:10:46 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:46 - INFO - __main__ -     infer_time = 2.805222222222222
05/28/2023 14:10:46 - INFO - __main__ -     loss = 0.6959310810444719

Iteration:  77%|#######6  | 60/78 [00:05<00:00, 20.27it/s][A
Iteration:  81%|########  | 63/78 [00:05<00:00, 22.01it/s][A
Iteration:  85%|########4 | 66/78 [00:05<00:00, 23.58it/s][A05/28/2023 14:10:46 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:46 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:10:46 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:46 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 164.03it/s]
05/28/2023 14:10:46 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:46 - INFO - __main__ -    dev: acc = 0.49097472924187724
05/28/2023 14:10:46 - INFO - __main__ -    dev: eval_loss = 0.6928750541475084
05/28/2023 14:10:46 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:46 - INFO - __main__ -    dev: infer_time = 2.7754444444444446
05/28/2023 14:10:46 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:46 - INFO - __main__ -     acc = 0.49097472924187724
05/28/2023 14:10:46 - INFO - __main__ -     cls_loss = 0.6952708881834279
05/28/2023 14:10:46 - INFO - __main__ -     eval_loss = 0.6928750541475084
05/28/2023 14:10:46 - INFO - __main__ -     global_step = 69
05/28/2023 14:10:46 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:46 - INFO - __main__ -     infer_time = 2.7754444444444446
05/28/2023 14:10:46 - INFO - __main__ -     loss = 0.6952708881834279

Iteration:  88%|########8 | 69/78 [00:05<00:00, 21.39it/s][A
Iteration:  92%|#########2| 72/78 [00:05<00:00, 22.94it/s][A
Iteration:  96%|#########6| 75/78 [00:05<00:00, 24.14it/s][AIteration: 100%|##########| 78/78 [00:05<00:00, 13.37it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.83s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:05<00:00,  5.83s/it]
05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   w_emb: 8790336

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   p_emb: 147456

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   t_emb: 576

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   ln_emb: 576

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 286688

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 285984

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   intermediate_numel: 286688

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   output_numel: 286560

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 286688

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 285984

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   intermediate_numel: 286688

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   output_numel: 286560

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   query_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   key_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   value_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   self_numel: 249696

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   output_numel: 83808

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 286688

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 285984

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   ln_numel: 576

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   attention_numel: 333504

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   intermediate_numel: 286688

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   output_numel: 286560

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   layer_numel: 2720256
05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   dense_numel: 83232
05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   emb_numel: 8938944

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   encoder_numel: 2720256

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   pooler_numel: 83232

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   all parameters: 11742432

05/28/2023 14:10:46 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:46 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [992, 992, 992], 'sample_qkv_sizes': [288, 288, 288]}
parameter size = 11742432
best_acc = 0.5270758122743683
time_per_batch_infer = 2.789 ms
infer_cnt = 63
**************E*************

05/28/2023 14:10:46 - INFO - __main__ -   The current subbert_config is: {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 372, 'sample_intermediate_sizes': [320, 320, 320, 320], 'sample_qkv_sizes': [372, 372, 372, 372]}
05/28/2023 14:10:46 - INFO - __main__ -   The args: Namespace(arches_file='../cands/1st_generation_kd_ptq_10.evo.cands', cache_dir='', data_dir='/n/home00/lbailey/bigger_and_faster/tmp_glue_datasets/rte/', data_url='', do_lower_case=True, doc_stride=128, eval_batch_size=32, eval_step=10, gradient_accumulation_steps=1, init_method='', learning_rate=2e-05, max_answer_length=30, max_query_length=64, max_seq_length=128, model='/n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3', n_best_size=20, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=1.0, output_dir='../output/kd_ptq_10/', predict_file='SQuAD2.0/dev-v2.0.json', save_model_flag=0, seed='42', super_model='MLM', task_name='rte', train_batch_size=32, train_file='SQuAD2.0/train-v2.0.json', train_url='', verbose_logging=False, version_2_with_negative=0, warmup_proportion=0.1, weight_decay=0.0001)
05/28/2023 14:10:46 - INFO - __main__ -   Writing example 0 of 2490
05/28/2023 14:10:46 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:46 - INFO - __main__ -   guid: train-0
05/28/2023 14:10:46 - INFO - __main__ -   tokens: [CLS] no weapons of mass destruction found in iraq yet . [SEP] weapons of mass destruction found in iraq . [SEP]
05/28/2023 14:10:46 - INFO - __main__ -   input_ids: 101 2053 4255 1997 3742 6215 2179 1999 5712 2664 1012 102 4255 1997 3742 6215 2179 1999 5712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:46 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:46 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:46 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:46 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:48 - INFO - __main__ -   Writing example 0 of 277
05/28/2023 14:10:48 - INFO - __main__ -   *** Example ***
05/28/2023 14:10:48 - INFO - __main__ -   guid: dev-0
05/28/2023 14:10:48 - INFO - __main__ -   tokens: [CLS] dana reeve , the widow of the actor christopher reeve , has died of lung cancer at age 44 , according to the christopher reeve foundation . [SEP] christopher reeve had an accident . [SEP]
05/28/2023 14:10:48 - INFO - __main__ -   input_ids: 101 11271 20726 1010 1996 7794 1997 1996 3364 5696 20726 1010 2038 2351 1997 11192 4456 2012 2287 4008 1010 2429 2000 1996 5696 20726 3192 1012 102 5696 20726 2018 2019 4926 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:48 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:48 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
05/28/2023 14:10:48 - INFO - __main__ -   label: not_entailment
05/28/2023 14:10:48 - INFO - __main__ -   label_id: 1
05/28/2023 14:10:48 - INFO - transformer.modeling_super_kd -   loading archive file /n/home00/lbailey/bigger_and_faster/model/SUPER-KD-S1/output/superbert/checkpoints/superbert_epoch_4_lr_0.0001_bsz_12_grad_accu_1_512_gpu_1/epoch_3
05/28/2023 14:10:48 - INFO - transformer.modeling_super_kd -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "emb_size": 564,
  "fix_config": {
    "sample_hidden_size": 564,
    "sample_intermediate_sizes": [
      1024,
      1024,
      1024,
      1024,
      1024
    ],
    "sample_layer_num": 5,
    "sample_num_attention_heads": [
      12,
      12,
      12,
      12,
      12
    ],
    "sample_qkv_sizes": [
      528,
      528,
      528,
      528,
      528
    ]
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 564,
  "initializer_range": 0.02,
  "intermediate_size": 1024,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 5,
  "pre_trained": "",
  "qkv_size": 528,
  "structure": [],
  "training": "",
  "type_vocab_size": 2,
  "vocab_size": 30522
}

05/28/2023 14:10:49 - INFO - transformer.modeling_super_kd -   Weights of SuperBertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
05/28/2023 14:10:49 - INFO - __main__ -   Total parameters: 30020302
05/28/2023 14:10:49 - INFO - __main__ -   ***** Running training *****
05/28/2023 14:10:49 - INFO - __main__ -     Num examples = 2490
05/28/2023 14:10:49 - INFO - __main__ -     Batch size = 32
05/28/2023 14:10:49 - INFO - __main__ -     Num steps = 77
Epoch:   0%|          | 0/1 [00:00<?, ?it/s]
Iteration:   0%|          | 0/78 [00:00<?, ?it/s][A
Iteration:   4%|3         | 3/78 [00:00<00:03, 21.95it/s][A
Iteration:   8%|7         | 6/78 [00:00<00:03, 22.39it/s][A05/28/2023 14:10:49 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:49 - INFO - __main__ -     Epoch = 0 iter 9 step
05/28/2023 14:10:49 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:49 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 129.61it/s]
05/28/2023 14:10:49 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:49 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:49 - INFO - __main__ -    dev: eval_loss = 0.7080245680279202
05/28/2023 14:10:49 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:49 - INFO - __main__ -    dev: infer_time = 3.6697777777777785
05/28/2023 14:10:49 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:49 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:49 - INFO - __main__ -     cls_loss = 0.6939278311199613
05/28/2023 14:10:49 - INFO - __main__ -     eval_loss = 0.7080245680279202
05/28/2023 14:10:49 - INFO - __main__ -     global_step = 9
05/28/2023 14:10:49 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:49 - INFO - __main__ -     infer_time = 3.6697777777777785
05/28/2023 14:10:49 - INFO - __main__ -     loss = 0.6939278311199613
05/28/2023 14:10:49 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  12%|#1        | 9/78 [00:01<00:19,  3.51it/s][A
Iteration:  15%|#5        | 12/78 [00:02<00:12,  5.24it/s][A
Iteration:  19%|#9        | 15/78 [00:02<00:08,  7.25it/s][A
Iteration:  23%|##3       | 18/78 [00:02<00:06,  9.43it/s][A05/28/2023 14:10:51 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:51 - INFO - __main__ -     Epoch = 0 iter 19 step
05/28/2023 14:10:51 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:51 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.60it/s]
05/28/2023 14:10:51 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:51 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:51 - INFO - __main__ -    dev: eval_loss = 0.6915260685814751
05/28/2023 14:10:51 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:51 - INFO - __main__ -    dev: infer_time = 3.656777777777778
05/28/2023 14:10:51 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:51 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:51 - INFO - __main__ -     cls_loss = 0.7006191705402575
05/28/2023 14:10:51 - INFO - __main__ -     eval_loss = 0.6915260685814751
05/28/2023 14:10:51 - INFO - __main__ -     global_step = 19
05/28/2023 14:10:51 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:51 - INFO - __main__ -     infer_time = 3.656777777777778
05/28/2023 14:10:51 - INFO - __main__ -     loss = 0.7006191705402575
05/28/2023 14:10:51 - INFO - root -   ** ** * Saving fine-tuned model ** ** * 

Iteration:  27%|##6       | 21/78 [00:03<00:13,  4.25it/s][A
Iteration:  31%|###       | 24/78 [00:04<00:09,  5.73it/s][A
Iteration:  35%|###4      | 27/78 [00:04<00:06,  7.48it/s][A05/28/2023 14:10:53 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:53 - INFO - __main__ -     Epoch = 0 iter 29 step
05/28/2023 14:10:53 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:53 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.80it/s]
05/28/2023 14:10:53 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:53 - INFO - __main__ -    dev: acc = 0.4729241877256318
05/28/2023 14:10:53 - INFO - __main__ -    dev: eval_loss = 0.704198784298367
05/28/2023 14:10:53 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:53 - INFO - __main__ -    dev: infer_time = 3.654888888888889
05/28/2023 14:10:53 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:53 - INFO - __main__ -     acc = 0.4729241877256318
05/28/2023 14:10:53 - INFO - __main__ -     cls_loss = 0.6991362736142915
05/28/2023 14:10:53 - INFO - __main__ -     eval_loss = 0.704198784298367
05/28/2023 14:10:53 - INFO - __main__ -     global_step = 29
05/28/2023 14:10:53 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:53 - INFO - __main__ -     infer_time = 3.654888888888889
05/28/2023 14:10:53 - INFO - __main__ -     loss = 0.6991362736142915

Iteration:  37%|###7      | 29/78 [00:04<00:05,  8.20it/s][A
Iteration:  41%|####1     | 32/78 [00:04<00:04, 10.39it/s][A
Iteration:  45%|####4     | 35/78 [00:04<00:03, 12.59it/s][A
Iteration:  49%|####8     | 38/78 [00:04<00:02, 14.63it/s][A05/28/2023 14:10:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:54 - INFO - __main__ -     Epoch = 0 iter 39 step
05/28/2023 14:10:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 131.00it/s]
05/28/2023 14:10:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:54 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:54 - INFO - __main__ -    dev: eval_loss = 0.6940809885660807
05/28/2023 14:10:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:54 - INFO - __main__ -    dev: infer_time = 3.6655555555555557
05/28/2023 14:10:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:54 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:54 - INFO - __main__ -     cls_loss = 0.6995385549007318
05/28/2023 14:10:54 - INFO - __main__ -     eval_loss = 0.6940809885660807
05/28/2023 14:10:54 - INFO - __main__ -     global_step = 39
05/28/2023 14:10:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:54 - INFO - __main__ -     infer_time = 3.6655555555555557
05/28/2023 14:10:54 - INFO - __main__ -     loss = 0.6995385549007318

Iteration:  53%|#####2    | 41/78 [00:04<00:02, 14.57it/s][A
Iteration:  56%|#####6    | 44/78 [00:05<00:02, 16.34it/s][A
Iteration:  60%|######    | 47/78 [00:05<00:01, 17.83it/s][A05/28/2023 14:10:54 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:54 - INFO - __main__ -     Epoch = 0 iter 49 step
05/28/2023 14:10:54 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:54 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.91it/s]
05/28/2023 14:10:54 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:54 - INFO - __main__ -    dev: acc = 0.5270758122743683
05/28/2023 14:10:54 - INFO - __main__ -    dev: eval_loss = 0.6916754113303291
05/28/2023 14:10:54 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:54 - INFO - __main__ -    dev: infer_time = 3.666222222222222
05/28/2023 14:10:54 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:54 - INFO - __main__ -     acc = 0.5270758122743683
05/28/2023 14:10:54 - INFO - __main__ -     cls_loss = 0.697951535789334
05/28/2023 14:10:54 - INFO - __main__ -     eval_loss = 0.6916754113303291
05/28/2023 14:10:54 - INFO - __main__ -     global_step = 49
05/28/2023 14:10:54 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:54 - INFO - __main__ -     infer_time = 3.666222222222222
05/28/2023 14:10:54 - INFO - __main__ -     loss = 0.697951535789334

Iteration:  64%|######4   | 50/78 [00:05<00:01, 16.51it/s][A
Iteration:  68%|######7   | 53/78 [00:05<00:01, 17.95it/s][A
Iteration:  72%|#######1  | 56/78 [00:05<00:01, 19.14it/s][A05/28/2023 14:10:55 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:55 - INFO - __main__ -     Epoch = 0 iter 59 step
05/28/2023 14:10:55 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:55 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.92it/s]
05/28/2023 14:10:55 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:55 - INFO - __main__ -    dev: acc = 0.5234657039711191
05/28/2023 14:10:55 - INFO - __main__ -    dev: eval_loss = 0.6913319561216567
05/28/2023 14:10:55 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:55 - INFO - __main__ -    dev: infer_time = 3.657666666666667
05/28/2023 14:10:55 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:55 - INFO - __main__ -     acc = 0.5234657039711191
05/28/2023 14:10:55 - INFO - __main__ -     cls_loss = 0.6978439749297449
05/28/2023 14:10:55 - INFO - __main__ -     eval_loss = 0.6913319561216567
05/28/2023 14:10:55 - INFO - __main__ -     global_step = 59
05/28/2023 14:10:55 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:55 - INFO - __main__ -     infer_time = 3.657666666666667
05/28/2023 14:10:55 - INFO - __main__ -     loss = 0.6978439749297449

Iteration:  76%|#######5  | 59/78 [00:05<00:01, 17.28it/s][A
Iteration:  78%|#######8  | 61/78 [00:05<00:00, 17.58it/s][A
Iteration:  82%|########2 | 64/78 [00:06<00:00, 18.97it/s][A
Iteration:  86%|########5 | 67/78 [00:06<00:00, 19.87it/s][A05/28/2023 14:10:55 - INFO - __main__ -   ***** Running evaluation *****
05/28/2023 14:10:55 - INFO - __main__ -     Epoch = 0 iter 69 step
05/28/2023 14:10:55 - INFO - __main__ -     Num examples = 277
05/28/2023 14:10:55 - INFO - __main__ -     Batch size = 32


Evaluating:   0%|          | 0/9 [00:00<?, ?it/s][A[AEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 130.87it/s]
05/28/2023 14:10:55 - INFO - __main__ -   ******** Eval results ********
05/28/2023 14:10:55 - INFO - __main__ -    dev: acc = 0.47653429602888087
05/28/2023 14:10:55 - INFO - __main__ -    dev: eval_loss = 0.6946312321556939
05/28/2023 14:10:55 - INFO - __main__ -    dev: infer_cnt = 9
05/28/2023 14:10:55 - INFO - __main__ -    dev: infer_time = 3.6543333333333328
05/28/2023 14:10:55 - INFO - __main__ -   ***** Eval results *****
05/28/2023 14:10:55 - INFO - __main__ -     acc = 0.47653429602888087
05/28/2023 14:10:55 - INFO - __main__ -     cls_loss = 0.6973631157391313
05/28/2023 14:10:55 - INFO - __main__ -     eval_loss = 0.6946312321556939
05/28/2023 14:10:55 - INFO - __main__ -     global_step = 69
05/28/2023 14:10:55 - INFO - __main__ -     infer_cnt = 9
05/28/2023 14:10:55 - INFO - __main__ -     infer_time = 3.6543333333333328
05/28/2023 14:10:55 - INFO - __main__ -     loss = 0.6973631157391313

Iteration:  90%|########9 | 70/78 [00:06<00:00, 17.53it/s][A
Iteration:  94%|#########3| 73/78 [00:06<00:00, 18.81it/s][A
Iteration:  97%|#########7| 76/78 [00:06<00:00, 19.82it/s][AIteration: 100%|##########| 78/78 [00:06<00:00, 11.50it/s]
Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.78s/it]Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:06<00:00,  6.78s/it]
05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   w_emb: 11354184

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   p_emb: 190464

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   t_emb: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ln_emb: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   query_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   key_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   value_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   self_numel: 416268

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   output_numel: 139500

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 119360

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 119412

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   attention_numel: 555768

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   intermediate_numel: 119360

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   output_numel: 120156

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   query_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   key_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   value_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   self_numel: 416268

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   output_numel: 139500

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 119360

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 119412

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   attention_numel: 555768

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   intermediate_numel: 119360

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   output_numel: 120156

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   query_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   key_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   value_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   self_numel: 416268

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   output_numel: 139500

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 119360

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 119412

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   attention_numel: 555768

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   intermediate_numel: 119360

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   output_numel: 120156

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   query_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   key_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   value_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   self_numel: 416268

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   output_numel: 139500

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 119360

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 119412

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ln_numel: 744

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   attention_numel: 555768

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   intermediate_numel: 119360

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   output_numel: 120156

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   layer_numel: 3181136
05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   dense_numel: 138756
05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   emb_numel: 11546136

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   encoder_numel: 3181136

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   pooler_numel: 138756

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   all parameters: 14866028

05/28/2023 14:10:56 - INFO - transformer.modeling_super_kd -   ===========================
05/28/2023 14:10:56 - INFO - __main__ -   **************S*************
task_name = rte
architecture = {'sample_layer_num': 4, 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_hidden_size': 372, 'sample_intermediate_sizes': [320, 320, 320, 320], 'sample_qkv_sizes': [372, 372, 372, 372]}
parameter size = 14866028
best_acc = 0.5270758122743683
time_per_batch_infer = 3.661 ms
infer_cnt = 63
**************E*************

>>> Starting Search of EE Iteration 3 ...
Namespace(arch_perfs_file='../output/kd_ptq_10/subbert.results', candidate_file='/n/home00/lbailey/bigger_and_faster/cands/kd_ptq_10.cands', ckpt_path='/n/home00/lbailey/bigger_and_faster/conf_datasets/lat_predictor_quant.pt', feature_dim=4, feature_norm=[564, 5, 1024, 564], gen_size=10, head_num_space=[1, 12], hidden_dim=2000, hidden_layer_num=3, hidden_size_space=[144, 528], intermediate_size_space=[128, 1024], lat_norm=200, latency_constraint=10.0, layer_num_space=[1, 5], method='Evolved', model='KD', output_file='../cands/1st_generation_kd_ptq_10.evo.cands', qkv_size_space=[144, 528])
Size of candidates: 999
Size of fast candidates: 98
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 480, 'sample_intermediate_sizes': [832, 832, 832], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [480, 480, 480]}
new_arch_latency: 10.490909218788147
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 288, 'sample_intermediate_sizes': [992, 992, 992], 'sample_qkv_sizes': [288, 288, 288]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 288, 'sample_intermediate_sizes': [992, 992, 992], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [288, 288, 288]}
new_arch_latency: 8.580376207828522
old arch: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.365137457847595
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [192, 192, 192], 'sample_qkv_sizes': [456, 456, 456]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [192, 192, 192], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
new_arch_latency: 8.74694436788559
old arch: {'sample_layer_num': 5, 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_hidden_size': 264, 'sample_intermediate_sizes': [256, 256, 256, 256, 256], 'sample_qkv_sizes': [264, 264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 5, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [256, 256, 256, 256, 256], 'sample_num_attention_heads': [12, 12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252, 252]}
new_arch_latency: 10.520273447036743
old arch: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 2, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928], 'sample_num_attention_heads': [12, 12], 'sample_qkv_sizes': [468, 468]}
new_arch_latency: 7.4688926339149475
old arch: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 456, 'sample_intermediate_sizes': [896, 896, 896], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [456, 456, 456]}
new_arch_latency: 10.258431732654572
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 2, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864], 'sample_num_attention_heads': [12, 12], 'sample_qkv_sizes': [468, 468]}
new_arch_latency: 7.335959374904633
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [864, 864, 864], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.365137457847595
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [192, 192, 192], 'sample_qkv_sizes': [456, 456, 456]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 444, 'sample_intermediate_sizes': [192, 192, 192], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [444, 444, 444]}
new_arch_latency: 8.631326258182526
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 420, 'sample_intermediate_sizes': [768, 768, 768], 'sample_qkv_sizes': [420, 420, 420]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 420, 'sample_intermediate_sizes': [768, 768, 768], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [420, 420, 420]}
new_arch_latency: 9.597121179103851
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 420, 'sample_intermediate_sizes': [768, 768, 768], 'sample_qkv_sizes': [420, 420, 420]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 420, 'sample_intermediate_sizes': [832, 832, 832], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [420, 420, 420]}
new_arch_latency: 9.708458185195923
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
new_arch_latency: 9.021583199501038
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 252, 'sample_intermediate_sizes': [448, 448, 448, 448], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [252, 252, 252, 252]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 240, 'sample_intermediate_sizes': [384, 384, 384, 384], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [240, 240, 240, 240]}
new_arch_latency: 8.626770973205566
old arch: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.503265261650085
old arch: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.503265261650085
old arch: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 10.503265261650085
old arch: {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 480, 'sample_intermediate_sizes': [928, 928, 928], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [480, 480, 480]}
new_arch_latency: 10.701093077659607
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 276, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [276, 276, 276, 276]}
new_arch_latency: 9.643405675888062
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [512, 512, 512, 512], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 264, 'sample_intermediate_sizes': [576, 576, 576, 576], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [264, 264, 264, 264]}
new_arch_latency: 9.65738445520401
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [608, 608, 608], 'sample_qkv_sizes': [492, 492, 492]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 492, 'sample_intermediate_sizes': [608, 608, 608, 608], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [492, 492, 492, 492]}
new_arch_latency: 13.277183473110199
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 492, 'sample_intermediate_sizes': [608, 608, 608], 'sample_qkv_sizes': [492, 492, 492]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 480, 'sample_intermediate_sizes': [608, 608, 608], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [480, 480, 480]}
new_arch_latency: 10.009399056434631
old arch: {'sample_layer_num': 3, 'sample_num_attention_heads': [12, 12, 12], 'sample_hidden_size': 456, 'sample_intermediate_sizes': [192, 192, 192], 'sample_qkv_sizes': [456, 456, 456]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 468, 'sample_intermediate_sizes': [192, 192, 192], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [468, 468, 468]}
new_arch_latency: 8.873483538627625
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
after mutation, the new arch is : {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [256, 256, 256, 256], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
new_arch_latency: 11.086896061897278
old arch: {'sample_layer_num': 4, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192, 192], 'sample_num_attention_heads': [12, 12, 12, 12], 'sample_qkv_sizes': [432, 432, 432, 432]}
after mutation, the new arch is : {'sample_layer_num': 3, 'sample_hidden_size': 432, 'sample_intermediate_sizes': [192, 192, 192], 'sample_num_attention_heads': [12, 12, 12], 'sample_qkv_sizes': [432, 432, 432]}
new_arch_latency: 8.515678346157074
